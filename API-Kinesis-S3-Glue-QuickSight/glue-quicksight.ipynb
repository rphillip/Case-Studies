{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import mycredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# AWS Glue\n",
    "\n",
    "We have already used AWS Glue to generate metadata and extract a schema from our S3 data lakes. However, AWS Glue can be even more powerful by enabling us to create large scale Extraction-Transformation-Load (ETL) pipelines that grab, parse, and store data in an efficient manner. Glue is simply a serverless ETL tool that helps us do the following:\n",
    "\n",
    "* **Extract**: Data extraction from several sources, such as business systems, sensors, databanks, or applications.\n",
    "* **Transform**: This extracted data is converted into a format that various applications can use. \n",
    "* **Load**: The now transformed data then gets loaded into a target data storage system.\n",
    "\n",
    "There are a some component terms associated with AWS Glue we should consider before we cover its implementation.\n",
    "\n",
    "* **Tables**: These are not your typical relational database tables, but are instead metadata table definitions of data sources - not the data itself. It essentially provides the details for where the data is located.\n",
    "* **Database**: A database in the AWS Glue Catalog is a container that holds tables. We use this to organize our tables into respective categories.\n",
    "* **Data Catalog**: The data catalog holds the metadata which summarizes basic information about the processed data. This is where the tables and database live.\n",
    "* **Crawler**: The crawler is used to retrieve data from the source using custom or built-in classifiers. This is what *crawls* data sources to generate a schema through a certain set of rules. \n",
    "* **Classifier**: A classifier is used to match data for the crawler. This is where data specifications can be made. \n",
    "* **Job**: the business logic that executes the ETL tasks.\n",
    "* **Trigger**: Starts the ETL job at a specific time or event.\n",
    "\n",
    "Ultimately, what AWS Glue allows us to do is create efficient ETL pipelines allowing us to store/access data in a variety of places (Redshift, S3, DynamoDB, etc.). \n",
    "\n",
    "![Mini Glue Architecture](https://dataincubator-course.s3.amazonaws.com/miniprojects/images/dwa_mini_glue_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Glue is also very comfortable handling streaming data through the usage of Spark Streaming. These jobs created with Glue will run continuously as they consume data from streaming sources like Apache Kafka or Kinesis Data Streams. After this job has extracted and transformed the data is can then be loaded into a data lake or database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Kinesis\n",
    "\n",
    "[AWS Kinesis](https://aws.amazon.com/kinesis/) is a tool that makes it easy to collect, process, or analyze real-time streaming data. This tool behaves similar to Kafka. These data handling capabilities are provided by three modalities of AWS Kinesis:\n",
    "\n",
    "* **Kinesis Data Streams**: A scalable streaming service that captures real-time data.\n",
    "* **Kinesis Data Firehose**: Built off of data streams, Firehose can capture, transform, and load data streams in AWS data stores.\n",
    "* **Kinesis Data Analytics**: Built off Firehose, this tool enables real-time data analytic processing with SQL or Apache Flink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "This data will be requested and handled through a Kinesis Data Stream producer that you will create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### The Real-Time Data\n",
    "\n",
    "We are going to use a Kinesis Stream in this example and get information about subreddits from the [Reddit API](https://www.reddit.com/dev/api/). You will need to generate an API key and create an application which can be done on this [Reddit page](https://www.reddit.com/login/?dest=https%3A%2F%2Fwww.reddit.com%2Fprefs%2Fapps). In order to get access you will need to specify an OAuth token through the `requests` library.\n",
    "\n",
    "The goal of this project is to utilize the [AWS Glue `DynamicFrameReader`](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame-reader.html) to read the streaming data being generated by Kinesis, format it, then upload it to an S3 data lake and a DynamoDB [Sink](https://en.wikipedia.org/wiki/Sink_(computing)#:~:text=In%20computing%2C%20a%20sink%2C%20event,from%20another%20object%20or%20function.&text=The%20word%20sink%20has%20been,and%20output%20in%20the%20industry.). There will be two separate jobs to handle both of these tasks but they will utilize the same Glue Schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We will primarily be dealing with new posts to the python subreddit. This will provide us with a stream of posts as they're made in real-time. This stream will need to be handled by a [Kinesis Producer](https://aws.amazon.com/blogs/big-data/snakes-in-the-stream-feeding-and-eating-amazon-kinesis-streams-with-python/) - similar to a Kafka Producer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "When handling the information we are primarily concerned with the following data:\n",
    "* `subreddit`\n",
    "* `title`\n",
    "* `selftext`\n",
    "* `upvote_ratio`\n",
    "* `ups`\n",
    "* `downs`\n",
    "* `score`\n",
    "\n",
    "We will generate the following columns with Glue Job:\n",
    "* `python_mentions` **INTEGER**: Number of times 'python' mentioned in `title` and `selftext`\n",
    "* `title_length` **INTEGER**: Length of `title`\n",
    "* `selftext_length` **INTEGER**: Length of `selftext`\n",
    "\n",
    "These are the items we will persist to the S3 data lake and DynamoDB after being handled by a Glue job. It is important to realize with this stream we do not want to continuously push data to AWS Kinesis that it has been seen before. This filtering should be handled early on in the API request, here is an article that describes a bit of the [process](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c).\n",
    "\n",
    "The Kinesis data stream should only be handling the raw format of these. It should not handle any of the transformations because we want to reserve that for the Glue ETL process - take a second to think about why that may be the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we create the kinesis stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session( \n",
    "    region_name=mycredentials.amazonregion,\n",
    "    aws_access_key_id=mycredentials.amazonid,\n",
    "    aws_secret_access_key=mycredentials.amazonkey\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kinesis(session, kinesisname):\n",
    "    try:\n",
    "        response = session.client('kinesis').create_stream(\n",
    "            StreamName='kinesisname',\n",
    "            ShardCount=1,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "kinesisname=\"redditstream\"\n",
    "response = create_kinesis(session,kinesisname)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  redditstream.py\n",
    "This is the producer for the Kinesis stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mycredentials\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from kinesis_helper import KinesisStream\n",
    "import uuid\n",
    "\n",
    "# we use this function to convert responses to dataframes\n",
    "def df_from_response(res):\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        df = df.append({\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'title': post['data']['title'],\n",
    "            'selftext': post['data']['selftext'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'score': post['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'id': post['data']['id'],\n",
    "            'kind': post['kind']\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# authenticate API\n",
    "# authorization after creating reddit application\n",
    "auth = requests.auth.HTTPBasicAuth(mycredentials.redditclient, mycredentials.redditsecret)\n",
    "\n",
    "# provide information for access token\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': mycredentials.reddituser,\n",
    "    'password': mycredentials.redditpass\n",
    "}\n",
    "headers = {\n",
    "    'User-Agent': 'streamtest/0.0.1',\n",
    "}\n",
    "\n",
    "# send our request for access token\n",
    "res = (requests.post('https://www.reddit.com/api/v1/access_token', auth=auth,\n",
    "    headers=headers, data=data))\n",
    "# convert response to JSON and pull 'access_token' value\n",
    "TOKEN = res.json()['access_token']\n",
    "# add authorization to header\n",
    "headers = {**headers, **{'Authorization': f'bearer {TOKEN}'}}\n",
    "\n",
    "# Token is valid for two hours - keep it that way\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n",
    "\n",
    "# initialize dataframe and parameters for pulling data in loop\n",
    "new_df = pd.DataFrame()\n",
    "params = {'limit': 100}\n",
    "stream = KinesisStream('redditstream')\n",
    "\n",
    "\n",
    "# we stream the newests posts\n",
    "while True:\n",
    "\n",
    "    # make request\n",
    "    res = requests.get(\"https://oauth.reddit.com/r/python/new\",\n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "\n",
    "    # get dataframe from response\n",
    "    new_df = df_from_response(res)\n",
    "    if not new_df.empty:\n",
    "        # take the final row (newest entry)\n",
    "        row = new_df.iloc[0]\n",
    "        # create fullname\n",
    "        fullname = row['kind'] + '_' + row['id']\n",
    "        # add/update fullname in params\n",
    "        #we use before to get newest submissions\n",
    "        params['before'] = fullname\n",
    "        new_df['Data'] = new_df.apply(lambda x: json.dumps(x.to_dict(), separators= (',',':')), axis=1)\n",
    "        new_df['PartitionKey'] = [str(uuid.uuid4()) for _ in range(len(new_df.index))]\n",
    "        datadf = new_df[['Data','PartitionKey']].to_dict(orient=\"records\")\n",
    "        result = stream.send_stream(data=datadf)\n",
    "        print(datadf, result)\n",
    "        #add sleep time to limit api calls and prevent timeouts\n",
    "        time.sleep(1)\n",
    "    #wait for new posts.\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "        print(\"waiting for new posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kinesis_helper.py\n",
    "This has the producer class for kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, uuid, boto3\n",
    "import mycredentials\n",
    "\n",
    "class KinesisStream(object):\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        self.stream = stream\n",
    "\n",
    "    def _connected_client(self):\n",
    "        \"\"\" Connect to Kinesis Streams \"\"\"\n",
    "        return boto3.client('kinesis',\n",
    "                            region_name=mycredentials.amazonregion,\n",
    "                            aws_access_key_id=mycredentials.amazonid,\n",
    "                            aws_secret_access_key=mycredentials.amazonkey)\n",
    "\n",
    "    def send_stream(self, data, partition_key=None):\n",
    "\n",
    "        #we send list of records to kinesis\n",
    "\n",
    "        client = self._connected_client()\n",
    "        response = client.put_records(\n",
    "            Records=data,\n",
    "           StreamName=self.stream\n",
    "        )\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Create Data Catalog\n",
    "\n",
    "Instead of using a Glue Crawler, create a Glue Data Catalog, Database, and Table that contains the metadata of the data being provided from the Kinesis stream you created. It is important to keep the types consistent and ensure that the types defined in Glue are the format we want those data columns to take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (AlreadyExistsException) when calling the CreateTable operation: Table already exists.\n"
     ]
    }
   ],
   "source": [
    "def create_glue_db(session, glue_db_name):\n",
    "    try:\n",
    "        glue_client = session.client('glue')\n",
    "        response = glue_client.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': glue_db_name\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)   \n",
    "kin = session.client('kinesis')\n",
    "kin_arn = kin.describe_stream(StreamName=kinesisname)['StreamDescription']['StreamARN']\n",
    "kin_arn = create_glue_kinesis_table(session, kin_arn)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (AlreadyExistsException) when calling the CreateDatabase operation: Database already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create Glue Data Catalog and Database\n",
    "dbname=\"reddit\"\n",
    "response = create_glue_db(session,\"reddit\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "code_folding": [],
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (AlreadyExistsException) when calling the CreateTable operation: Table already exists.\n"
     ]
    }
   ],
   "source": [
    "# Attach your Reddit Kinesis Data Stream to AWS Glue\n",
    "def create_glue_kinesis_table(session,kinesisarn):\n",
    "    try:\n",
    "        response = session.client('glue').create_table(\n",
    "            DatabaseName='reddit',\n",
    "            TableInput={\n",
    "                'Name': 'redditstream',\n",
    "                'Owner': 'string',\n",
    "                'Retention': 123,\n",
    "                \"StorageDescriptor\": {\n",
    "                    \"Columns\": [\n",
    "                            {\n",
    "                                \"Name\": \"subreddit\",\n",
    "                                \"Type\": \"string\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"title\",\n",
    "                                \"Type\": \"string\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"selftext\",\n",
    "                                \"Type\": \"string\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"upvote_ratio\",\n",
    "                                \"Type\": \"double\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"ups\",\n",
    "                                \"Type\": \"double\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"downs\",\n",
    "                                \"Type\": \"double\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"score\",\n",
    "                                \"Type\": \"double\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"created_utc\",\n",
    "                                \"Type\": \"string\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"id\",\n",
    "                                \"Type\": \"string\",\n",
    "                                \"Comment\": \"\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"Name\": \"kind\",\n",
    "                                \"Type\": \"string\",\n",
    "                                \"Comment\": \"\"\n",
    "                            }\n",
    "                        ],\n",
    "                    \"Location\": \"redditstream\",\n",
    "                    \"InputFormat\": \"org.apache.hadoop.mapred.TextInputFormat\",\n",
    "                    \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\",\n",
    "                    \"Compressed\": False,\n",
    "                    \"NumberOfBuckets\": 0,\n",
    "                    \"SerdeInfo\": {\n",
    "                        \"Name\": \"json\",\n",
    "                        \"SerializationLibrary\": \"org.openx.data.jsonserde.JsonSerDe\",\n",
    "                        \"Parameters\": {\n",
    "                            \"paths\": \"created_utc,downs,id,kind,score,selftext,subreddit,title,ups,upvote_ratio\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"BucketColumns\": [],\n",
    "                    \"SortColumns\": [],\n",
    "                    \"Parameters\": {\n",
    "                        \"streamARN\": \"arn:aws:kinesis:us-east-1:496180498631:stream/redditstream\",\n",
    "                        \"typeOfData\": \"kinesis\"\n",
    "                    },\n",
    "                    \"SkewedInfo\": {},\n",
    "                    \"StoredAsSubDirectories\": False\n",
    "                },\n",
    "                \"Parameters\": {\n",
    "                    \"classification\": \"json\"\n",
    "                }\n",
    "            }\n",
    "                    \n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "response = create_glue_kinesis_table(session,kin_arn)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### S3 Glue Job \n",
    "\n",
    "Now that Glue knows the format the streaming data is going to take we can begin creating a job that will transform the data and eventually load it into an S3 data lake and Dynamo database.\n",
    "\n",
    "Remember, we are going to need a IAM role that has access to the AWS resources that are being interacted with - follow the principle of least privileges.\n",
    "\n",
    "An S3 bucket will be used to store Glue scripts and logs - create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Create s3 bucket\n",
    "def create_bucket(session, bucket_name):\n",
    "    \"\"\"Creates an AWS S3 bucket in the default region\"\"\"\n",
    "    s3_resource = session.resource('s3')\n",
    "    try:\n",
    "        response = s3_resource.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            #CreateBucketConfiguration={'LocationConstraint': 'us-east-1'}\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    else:\n",
    "        bucket = s3_resource.Bucket(bucket_name)\n",
    "        return bucket\n",
    "bucket_name = 'datalakeminirs'\n",
    "bucket = create_bucket(session, bucket_name)\n",
    "s3_resource = session.resource('s3')\n",
    "bucket = s3_resource.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name myGlueS3 already exists.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create Role\n",
    "def create_glue_role(session, role_name, policies):\n",
    "    try:\n",
    "        iam_client = session.client('iam')\n",
    "        policy_document = json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                  \"Effect\": \"Allow\",\n",
    "                  \"Principal\": {\n",
    "                    \"Service\": \"glue.amazonaws.com\"\n",
    "                  },\n",
    "                  \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=policy_document,\n",
    "        )\n",
    "        for policy in policies:\n",
    "            response = iam_client.attach_role_policy(\n",
    "                RoleName=role_name, \n",
    "                PolicyArn=policy\n",
    "            ) \n",
    "        return role    \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "role_name = 'myGlueS3'\n",
    "policies = [\n",
    "    'arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "    'arn:aws:iam::aws:policy/AWSLakeFormationDataAdmin',\n",
    "    'arn:aws:iam::aws:policy/CloudWatchFullAccess',\n",
    "    'arn:aws:iam::aws:policy/AmazonKinesisFullAccess',\n",
    "    'arn:aws:iam::aws:policy/GlueConsoleFullAccess',\n",
    "    'arn:aws:iam::aws:policy/AwsGlueDataBrewFullAccessPolicy',\n",
    "]\n",
    "\n",
    "role = create_glue_role(session, role_name, policies)\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "##### Create S3 Glue Job Script\n",
    "\n",
    "We are going to create a AWS Glue pipeline that will hydrate a data lake that is stored on S3. This Glue Script should accomplish the following:\n",
    "\n",
    "* Process data through a [`GlueContext`](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-glue-context.html), [`DynamicFrame`](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html), and Spark Streaming.\n",
    "* Utilize `ApplyMapping` to correctly type each data column.\n",
    "* Count total number of times python was mentioned in the `title` and `selftext`.\n",
    "* Count total length of `title` and `selftext`.\n",
    "* Create an S3 path with the ingestion year, month, day, and hour for the S3 Data Lake.\n",
    "\n",
    "***Bonus***: Figure out how to batch the data together for this S3 data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# here is the gluejob script\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, size, split, lower\n",
    "from pyspark.sql import DataFrame, Row\n",
    "import datetime\n",
    "from awsglue import DynamicFrame\n",
    "\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args[\"JOB_NAME\"], args)\n",
    "\n",
    "\n",
    "\n",
    "# read kinesis stream from data catalog\n",
    "dataframe_KinesisStream_node1 = glueContext.create_data_frame.from_catalog(\n",
    "    database=\"reddit\",\n",
    "    table_name=\"redditstream\",\n",
    "    additional_options={\"startingPosition\": \"earliest\", \"inferSchema\": \"false\"},\n",
    "    transformation_ctx=\"dataframe_KinesisStream_node1\",\n",
    ")\n",
    "\n",
    "def processBatch(data_frame, batchId):\n",
    "    if data_frame.count() > 0:\n",
    "        #add additional columns\n",
    "        df1 = (data_frame.withColumn(\"length_title\", F.length(\"title\"))\n",
    "            .withColumn(\"length_selftext\", F.length(\"selftext\"))\n",
    "            .withColumn('python_mentions', F.size(F.split(F.lower(F.col(\"title\")), r\"python\")) - 1 +\n",
    "                F.size(F.split(F.lower(F.col(\"selftext\")), r\"python\")) - 1)\n",
    "            )\n",
    "        KinesisStream_node1 = DynamicFrame.fromDF(\n",
    "            df1, glueContext, \"from_data_frame\"\n",
    "        )\n",
    "        #apply corerct data types\n",
    "        apply_mapping = ApplyMapping.apply(frame = KinesisStream_node1, mappings = [ \\\n",
    "            (\"subreddit\", \"string\", \"subreddit\", \"string\"), \\\n",
    "            (\"title\", \"string\", \"title\", \"string\"), \\\n",
    "            (\"selftext\", \"string\", \"selftext\", \"string\"), \\\n",
    "            (\"upvote_ratio\", \"double\", \"upvote_ratio\", \"double\"), \\\n",
    "            (\"ups\", \"double\", \"ups\", \"integer\"), \\\n",
    "            (\"downs\", \"double\", \"downs\", \"integer\"), \\\n",
    "            (\"score\", \"double\", \"score\", \"integer\"), \\\n",
    "            (\"created_utc\", \"string\", \"screated_utc\", \"string\"), \\\n",
    "            (\"id\", \"string\", \"id\", \"string\"), \\\n",
    "            (\"kind\", \"string\", \"kind\", \"string\"), \\\n",
    "            (\"length_title\", \"integer\", \"length_title\", \"integer\"), \\\n",
    "            (\"length_selftext\", \"integer\", \"length_selftext\", \"integer\"), \\\n",
    "            (\"python_mentions\", \"integer\", \"python_mentions\", \"integer\")],\\\n",
    "            transformation_ctx = \"apply_mapping\")\n",
    "        df2 = apply_mapping.toDF()\n",
    "        #add ingestion time to separate into s3 ingestion folders\n",
    "        KinesisStream_node2 =(DynamicFrame.fromDF(\n",
    "            glueContext.add_ingestion_time_columns(df2, \"hour\"),\n",
    "            glueContext,\n",
    "            \"from_data_frame\",)\n",
    "            )\n",
    "\n",
    "        S3bucket_node3_path = \"s3://datalakeminirs/\"\n",
    "        #create Data sink\n",
    "        S3bucket_node3 = glueContext.getSink(\n",
    "            path=S3bucket_node3_path,\n",
    "            connection_type=\"s3\",\n",
    "            updateBehavior=\"LOG\",\n",
    "            partitionKeys=[\"ingest_year\", \"ingest_month\", \"ingest_day\", \"ingest_hour\"],\n",
    "            enableUpdateCatalog=True,\n",
    "            transformation_ctx=\"S3bucket_node3\",\n",
    "        )\n",
    "        \n",
    "        S3bucket_node3.setCatalogInfo(\n",
    "            catalogDatabase=\"reddit\", catalogTableName=\"output\"\n",
    "        )\n",
    "        S3bucket_node3.setFormat(\"json\")\n",
    "        S3bucket_node3.writeFrame(KinesisStream_node2)\n",
    "\n",
    "\n",
    "glueContext.forEachBatch(\n",
    "    frame=dataframe_KinesisStream_node1,\n",
    "    batch_function=processBatch,\n",
    "    options={\n",
    "        \"windowSize\": \"100 seconds\",\n",
    "        \"checkpointLocation\": args[\"TempDir\"] + \"/\" + args[\"JOB_NAME\"] + \"/checkpoint/\",\n",
    "    },\n",
    ")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create the glue job\n",
    "glue = session.client('glue')\n",
    "glue_job_name = 'r5'\n",
    "script_path = 's3://datalakeminirs/gluejob.py'\n",
    "\n",
    "response = glue.create_job(\n",
    "    Name=glue_job_name,\n",
    "    Description='kinesis stream',\n",
    "    Role=role,\n",
    "    ExecutionProperty={\n",
    "    'MaxConcurrentRuns': 2\n",
    "        },\n",
    "    Command={\n",
    "    'Name': 'gluestreaming',\n",
    "    'ScriptLocation': script_path,\n",
    "    'PythonVersion': '3'\n",
    "        },\n",
    "    MaxRetries=0,\n",
    "    Timeout=1440,\n",
    "    GlueVersion='3.0',\n",
    "    NumberOfWorkers=2,\n",
    "    WorkerType='G.1X'\n",
    "    )\n",
    "response = glue.start_job_run(\n",
    "    JobName=glue_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The Kinesis data will be handled using Spark, so make sure to specify the use of Spark Streaming [boto3 command](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_job) when creating the Glue job.\n",
    "\n",
    "The function will also need allow the specification of an `outputPath` - the S3 path where the final aggregations are persisted. Think about how you can pass these arguments to our S3 script via boto3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Athena & QuickSight\n",
    "\n",
    "Athena, as you have seen before, is a fast and easy query engine that utilizes the AWS Glue data Catalog to store and retrieve metadata. We can also use QuickSight on top of Athena as our [consumption layer](https://blog.starburst.io/technical-blog/consumption-layer-101).\n",
    "\n",
    "QuickSight will use the information gathered from Athena and allow us to create a layered dashboard that can provide various visualization tools. This consumption layer can act as the intermediary between data engineers and analyst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query state: QUEUED\n",
      "Query state: QUEUED\n",
      "Query state: QUEUED\n",
      "Query state: QUEUED\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: RUNNING\n",
      "Query state: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# Connect to Athena\n",
    "import re\n",
    "def athena_query(client, params):\n",
    "    response = client.start_query_execution(\n",
    "        QueryString=params['query'],\n",
    "        QueryExecutionContext={\n",
    "            'Database': params['database']\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': 's3://' + params['bucket'] + '/output'\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "    \n",
    "    \n",
    "def athena_fetch_query(session, params):\n",
    "    athena = session.client('athena')\n",
    "    execution = athena_query(athena, params)\n",
    "    execution_id = execution['QueryExecutionId']\n",
    "    completed = False\n",
    "    \n",
    "    while not completed:\n",
    "        response = athena.get_query_execution(\n",
    "            QueryExecutionId=execution_id\n",
    "        )\n",
    "\n",
    "        state = response['QueryExecution']['Status']['State']\n",
    "        print('Query state:', state)\n",
    "        if state == 'FAILED':\n",
    "            print('Request Failed')\n",
    "            return None\n",
    "        elif state == 'SUCCEEDED':\n",
    "            s3_path = response['QueryExecution']['ResultConfiguration']['OutputLocation']\n",
    "            filename = re.findall('.*\\/(.*)', s3_path)[0]\n",
    "            return filename\n",
    "\n",
    "def get_s3_file(session, params, file_path):\n",
    "    try:\n",
    "        s3 = session.resource('s3')\n",
    "        file = s3.Bucket(params['bucket']).Object(file_path).get()\n",
    "        return file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "params = {\n",
    "    'database': \"reddit\",\n",
    "    'bucket': \"datalakeminirs\",\n",
    "    'query': 'SELECT * FROM output;'\n",
    "}\n",
    "\n",
    "file_name = athena_fetch_query(session, params)\n",
    "file_path = 'output/' + str(file_name)\n",
    "results_file = get_s3_file(session, params, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Develop a realtime QuickSight Dashboard\n",
    "https://us-east-1.quicksight.aws.amazon.com/sn/dashboards/f9cfc4e6-0bce-4aaa-b179-736cc387ed84/views/e4331ea7-2564-45f4-be57-2cbbdde9d9b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Sharing the Results\n",
    "\n",
    "After meaningful insights have been generated from the Reddit data you have interacted with, share a QuickSight Dashboard link in the Slack Channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Understanding Glue ETL vs EMR\n",
    "\n",
    "These are some of the best ETL tools on the market but they are interchangeable. AWS Glue has gained more popularity over the last year due to its ease of use and additional features. However, this doesn't mean it should be your first choice when developing an ETL pipeline. AWS Glue is serverless while EMR requires a bit more infrastructure to begin handling big data operations. Since Glue is serverless this leads to increased costs, opposed to EMR, which is cheaper when performing comparable costs. \n",
    "\n",
    "EMR is less scalable than Glue since it works on your onsite platform. If you have more flexible requirements, Glue is probably a better option. But if there are fixed requirements for your setup then EMR is the better choice because of costs.\n",
    "\n",
    "One last comparison is the EMR temp file storage through HDFS or S3. This can not be done on Glue due to its inherent serverless infrastructure. This, in turn, affects the performance of the Glue. This allows EMR to run the database faster and enhances the overall system performance.\n",
    "\n",
    "For more information regarding the comparison of these ETL tools check out [this link](https://medium.com/swlh/aws-glue-vs-emr-433b53872b30)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2022 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

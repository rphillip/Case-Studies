{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652cd3d5",
   "metadata": {},
   "source": [
    "# Revelio Labs - Data Engineering Assignment\n",
    "This is a simplified version of a data engineering project we had at Revelio Labs. The objective is to set up scalable pipelines using the existing stack that can ingest, update and process job posting data from our provider, GHR. You’ll be evaluated on your thought process, your SQL coding skills, how comfortable you are with basic terminal commands and your ability to explain your work. You don’t need to run any code for this exercise. Make sure you’ve read it entirely before you start so that you can see the full picture. Make the best use of online resources to catch up on technologies you are not familiar with. Choose the format that you think is best to gather your answers.\n",
    "Email received from GHR:\n",
    "We're in the process of re-creating an entire image of the dataset for you. It will be placed in a new folder 20200406; the files will be:\n",
    "- master_[index]_[date].csv, (job_id, company, post_date, salary, city) - titles_[index]_[date].csv, (job_id, title)\n",
    "- timelog_[index]_[date].csv, (job_id, remove_date)\n",
    "Each of these files represents a table for our database, and for this run, each table is split across several files (identified by [index]) due to size limitations in AWS.\n",
    "Going forward, new and revised job listing data will be published weekly (typically by Tuesday) in a new folder, dated for the Monday of the week (e.g. next one will be 20200413). These files will be much smaller, since they'll include only a week's worth of new and updated data.\n",
    "About the data:\n",
    "GHR data contains data on job postings starting in 2014. Each job_id is a unique identifier for a job posting (one row per job_id in each table) but sometimes the data is revised by including the same job_id in multiple deliveries. You can find a small sample of each file here (the sets of job_id are not the same among the files): https://info0.s3.us-east-2.amazonaws.com/assignment/engineering/data/master.csv https://info0.s3.us-east-2.amazonaws.com/assignment/engineering/data/title.csv https://info0.s3.us-east-2.amazonaws.com/assignment/engineering/data/timelog.csv\n",
    "\n",
    "Size of the entire image (20200406): master 30Gb, title 6Gb, timelog 9.4Gb\n",
    "Size of a weekly update (average): master 1Gb, title 100Mb, timelog 150 Mb\n",
    "\n",
    "GHR data (SFTP) Access Details:\n",
    "\n",
    "● Server: ghr-server\n",
    "● UID: client-revelio\n",
    "● private key: stored on EC2 at ~/.ssh/id-rsa-revelio-new\n",
    "● Port: 22\n",
    "\n",
    "About Revelio Labs Stack (AWS)\n",
    "\n",
    "● EC2 r5.4xlarge (remote Ubuntu machine, 16 CPU, 128Gb memory, 9.5Tb disk space).\n",
    "We access it through a terminal on our local machine using the ssh protocol, everything has to be done through the command line once connected to this EC2 machine. You can connect to the GHR bucket using the SFTP protocol on this EC2 machine.\n",
    "https://aws.amazon.com/ec2/\n",
    "\n",
    "● Redshift storage for structured data via PostgreSQL tables (7-node cluster, ~18Tb of disk overall)\n",
    "https://aws.amazon.com/redshift/\n",
    "\n",
    "For the following tasks, please provide commented scripts that perform all the necessary actions. SQL commands should be valid PostgreSQL syntax. Please make the best use of types, keys and indexes. Specify any credentials needed which have not already been specified as “<credential_name>”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effb6d4",
   "metadata": {},
   "source": [
    "# Task 1) SFTP ingestion\n",
    "\n",
    "Please produce a bash script to transfer the folder YYYYMMDD on the GHR server to the Revelio EC2 instance over SFTP and save the data at “/work/ghr/YYYYMMDD”. This script should work for the entire image of the original dataset and for all following weekly updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc08993",
   "metadata": {},
   "source": [
    "Lets login to the EC2 instance. Hopefully the credentials and iam roles/policies are already set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfee7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh -i <credential.pem> ec2-user@revelio.bunchofawschars.compute-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa8e78",
   "metadata": {},
   "source": [
    "Ok lets make it easy on myself. I'm just gonna write a python code that writes a bash. Then we will write a bash that calls that code and the bash to run on the EC2 server.\n",
    "\n",
    "We want to watch out for what we have and what we dont have so we only request gets that are needed. We don't want to redownload things we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import datetime as dt\n",
    "#get current files in directory\n",
    "existing = set([ name for name in os.listdir('./work/ghr/') if os.path.isdir(os.path.join('./', name)) ])\n",
    "\n",
    "#get span of dates for folders\n",
    "folders = set()\n",
    "start = dt.datetime.strptime(\"20200403\",\"%Y%m%d\")\n",
    "end = dt.datetime.strptime(\"20200604\",\"%Y%m%d\")\n",
    "#We can set it to the current time\n",
    "#end = datetime.today()\n",
    "week = start\n",
    "#get all the folder names\n",
    "while week < end:\n",
    "    folders.add(week.strftime(\"%Y%m%d\"))\n",
    "    d = dt.timedelta(days=7)\n",
    "    week += d\n",
    "#get the folders we are missing    \n",
    "getfolders = folders-existing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9e851cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we write the get commands to download the missing folders\n",
    "with open ('download_files.sh', 'w') as rsh:\n",
    "    rsh.write('''\\\n",
    "#! /bin/bash\\n''')\n",
    "    for folder in getfolders:\n",
    "        #note if -i doesnt work, -oIdentityFile=~/.ssh/id-rsa-revelio-new could also work\n",
    "        rsh.writelines(f'echo \"get -r {folder}/ work/ghr/\" | sftp -P 22 -i ~/.ssh/id-rsa-revelio-new client-revelio@ghr-server\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be1de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the bash to download the files\n",
    "#! /bin/bash\n",
    "python get_ghr_files.py\n",
    "bash download_files.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or we can call the bash commands in python instead of writing a whole new bash\n",
    "import subprocess\n",
    "for folder in getfolders:\n",
    "    #note if -i doesnt work, -oIdentityFile=~/.ssh/id-rsa-revelio-new could also work\n",
    "    command = f'echo \"get -r {folder}/ work/ghr/\" | sftp -P 22 -i ~/.ssh/id-rsa-revelio-new client-revelio@ghr-server'\n",
    "    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "    output, error = process.communicate()\n",
    "    print(output, error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558eca71",
   "metadata": {},
   "source": [
    "# Task 2) SQL Ingestion\n",
    "Create a single table in PostgreSQL called posting_20200406 containing all the data from each of the three files (job_id, company, post_date, salary, city, title, remove_date) for the initial GHR image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db65c81",
   "metadata": {},
   "source": [
    "## set up staging\n",
    "These are the staging tables we upload the csv files to. The last staging table is the final table we copy the joined staging tables to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e0e92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use this code to see the sizes of the string to help determine the datatypes of the table\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('title.csv')\n",
    "measurer = np.vectorize(len)\n",
    "restitle = measurer(df.values.astype(str)).max(axis=0)\n",
    "\n",
    "df1 = pd.read_csv('timelog.csv')\n",
    "restime = measurer(df.values.astype(str)).max(axis=0)\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('master.csv', encoding='latin-1')\n",
    "resmaster = measurer(df2.values.astype(str)).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00636c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE public.stage_master\n",
    "(\n",
    "    job_id BIGINT,\n",
    "    company VARCHAR(200),\n",
    "    post_date VARCHAR(19),\n",
    "    salary  INT,\n",
    "    city VARCHAR(60),\n",
    ")\n",
    "DISTSTYLE KEY DISTKEY (job_id) SORTKEY (post_date);\n",
    "\n",
    "CREATE TABLE public.stage_title\n",
    "(\n",
    "    job_id BIGINT,\n",
    "    title VARCHAR(70),\n",
    ")\n",
    "DISTSTYLE KEY DISTKEY (job_id) SORTKEY (title);\n",
    "\n",
    "CREATE TABLE public.stage_timelog\n",
    "(\n",
    "    job_id BIGINT,\n",
    "    remove_date VARCHAR(19),\n",
    ")\n",
    "DISTSTYLE KEY DISTKEY (job_id) SORTKEY (remove_date);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef604649",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE public.posting_20200406_stage\n",
    "(\n",
    "    job_id BIGINT,\n",
    "    company VARCHAR(200),\n",
    "    post_date TIMESTAMP,\n",
    "    salary  INT,\n",
    "    city VARCHAR(60),\n",
    "    title VARCHAR(70),\n",
    "    remove_date TIMESTAMP,\n",
    ")\n",
    "DISTSTYLE KEY DISTKEY (job_id) SORTKEY (remove_date);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c474291",
   "metadata": {},
   "source": [
    "## Upload from EC2 to S3 bucket\n",
    "\n",
    "We could try to upload straight from the EC2 (using a manifest with endpoints and all that jazz), but Redshift is just easier to use with S3 buckets. So lets just copy it over there. Also the cost of the bucket would be like $1 a month so might as well use it. So here is how we can do it with python.\n",
    "We need to make sure the IAM roles and policies are set so the EC2 has s3 access. (Side note: Probably whoever is writing this should have access to EC2 and S3. If not then the user arns should be added so they can modify the EC2 and S3 roles.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "#if bucket doesn't not exist we make one\n",
    "def create_bucket(session, bucket_name):\n",
    "    \"\"\"Creates an AWS S3 bucket in the 'us-east-1' region\"\"\"\n",
    "    try:\n",
    "        s3_resource = session.resource('s3')\n",
    "        response = s3_resource.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            #below is sometimes not needed when the default profile is the same region\n",
    "            CreateBucketConfiguration={'LocationConstraint': 'us-east-1'}\n",
    "        )\n",
    "        \n",
    "        bucket = s3_resource.Bucket(bucket_name)\n",
    "        return bucket\n",
    "        \n",
    "    except s3_resource.meta.client.exceptions.BucketAlreadyExists:\n",
    "        bucket = s3_resource.Bucket(bucket_name)\n",
    "        return bucket\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)       \n",
    "        \n",
    "def create_EC2_role(session, role_name, policies):\n",
    "    \"\"\"Create role with S3 full access policy\"\"\"\n",
    "    try:\n",
    "        iam_client = session.client('iam')\n",
    "        policy_document = json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                  \"Effect\": \"Allow\",\n",
    "                  \"Principal\": {\n",
    "                    \"Service\": \"ec2.amazonaws.com\"\n",
    "                  },\n",
    "                  \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=policy_document,\n",
    "        )\n",
    "        for policy in policies:\n",
    "            response = iam_client.attach_role_policy(\n",
    "                RoleName=role_name, \n",
    "                PolicyArn=policy\n",
    "            ) \n",
    "        return role    \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "session = boto3.session.Session( \n",
    "    aws_access_key_id=<credentials>\n",
    "    aws_secret_access_key=<credentials>,\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "bucket = (session, 'ghr_data')\n",
    "role_name = 'EC2toS3'\n",
    "policies = [\n",
    "    'arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    "]\n",
    "role = create_EC2_role(session, role_name, policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a711b2",
   "metadata": {},
   "source": [
    " Now if the EC2 has AWS CLI installed with it, we can just sync the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b4d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 sync /work/ghr/ s3://ghr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57252c44",
   "metadata": {},
   "source": [
    "Otherwise we could just upload it with python & boto3. And while we're here we can use the code to build a manifest for Redshift to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [f for f in os.listdir('./work/ghr/20200406') if os.path.isfile(f)]\n",
    "master_entries = []\n",
    "title_entries = []\n",
    "timelog_entries = []\n",
    "for f in files:\n",
    "    if \"csv\" in f:\n",
    "        bucket.upload_file(Filename=f, Key=f'ghr_data/20200406/{f}')\n",
    "        if \"master\" in f:\n",
    "            manifest_master.append({\"url\":f\"s3://ghr_data/20200406/{f}\", \"mandatory\":true})\n",
    "        elif \"title\" in f:\n",
    "            manifest_title.append({\"url\":f\"s3://ghr_data/20200406/{f}\", \"mandatory\":true})\n",
    "        elif \"timelog\" in f:\n",
    "            manifest_timelog.append({\"url\":f\"s3://ghr_data/20200406/{f}\", \"mandatory\":true})\n",
    "master_manifest = { \"entries\": master_entries }\n",
    "title_manifest = { \"entries\": title_entries }\n",
    "timelog_manifest = { \"entries\": timelog_entries }\n",
    "\n",
    "s3_client = session.client('s3')\n",
    "putresponse = s3_client.put_object(\n",
    "     Body=json.dumps(master_manifest),\n",
    "     Bucket='ghr_data',\n",
    "     Key='20200406/master.manifest'\n",
    ")\n",
    "putresponse = s3_client.put_object(\n",
    "     Body=json.dumps(title_manifest),\n",
    "     Bucket='ghr_data',\n",
    "     Key='20200406/title.manifest'\n",
    ") \n",
    "putresponse = s3_client.put_object(\n",
    "     Body=json.dumps(timelog_manifest),\n",
    "     Bucket='ghr_data',\n",
    "     Key='20200406/timelog.manifest'\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac30fa4",
   "metadata": {},
   "source": [
    "## copy data to staging table\n",
    "Here we can run the copy commands using the manifests from S3. First lets add the policies so redshift can access S3. (Side note: Probably whoever is writing this should have access to the Redshift cluster. If not then the user arns should be added so they can modify the red shift roles.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_redshift_role(session, role_name, policies):\n",
    "    \"\"\"Create role with S3 full access policy\"\"\"\n",
    "    try:\n",
    "        iam_client = session.client('iam')\n",
    "        policy_document = json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                  \"Effect\": \"Allow\",\n",
    "                  \"Principal\": {\n",
    "                    \"Service\": \"redshift.amazonaws.com\"\n",
    "                  },\n",
    "                  \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        role = iam_client.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=policy_document,\n",
    "        )\n",
    "        for policy in policies:\n",
    "            response = iam_client.attach_role_policy(\n",
    "                RoleName=role_name, \n",
    "                PolicyArn=policy\n",
    "            ) \n",
    "        return role    \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "role_name = 'RedshifttoS3'\n",
    "policies = [\n",
    "    'arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    "] \n",
    "role = create_redshift_role(session, role_name, policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becef3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we upload the files to the tables in Spectrum.\n",
    "#Hopefully the role has the permissions it needs to use S3\n",
    "copy public.stage_master\n",
    "from 's3://ghr_data/20200406/master.manifest' \n",
    "iam_role 'arn:aws:iam::10912381340:role/RedshifttoS3'\n",
    "manifest;\n",
    "copy public.stage_title\n",
    "from 's3://ghr_data/20200406/title.manifest' \n",
    "iam_role 'arn:aws:iam::10912381340:role/RedshifttoS3'\n",
    "manifest;\n",
    "copy public.stage_timelog\n",
    "from 's3://ghr_data/20200406/timelog.manifest' \n",
    "iam_role 'arn:aws:iam::10912381340:role/RedshifttoS3'\n",
    "manifest;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c64a6a",
   "metadata": {},
   "source": [
    "## copy data from staging to posting_20200406\n",
    "Note we change the date strings to TIMESTAMP and join all the staging tables together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSERT into public.posting_20200406_stage\n",
    "SELECT m.job_id, m.company, TO_TIMESTAMP(m.post_date, 'YYYY-MM-DD HH24:MI:SS') as post_date,\n",
    "    m.salary, m.city, t.title, TO_TIMESTAMP(tl.remove_date, 'YYYY-MM-DD HH24:MI:SS') as remove_date,\n",
    "FROM public.stage_master m \n",
    "LEFT JOIN public.stage_title t USING(job_id)\n",
    "LEFT JOIN public.stage_timelog tl USING (job_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e9bb5",
   "metadata": {},
   "source": [
    "Now we need to make sure there are no job duplicates. lets take the job_id with the latest remove_date. Because Redshift uploads in parallel, who knows what file was uploaded when for both the GHR database and this Redshift ingestion? It'd be best if there was a timestamp with the master file of when the records was entered in the GHR database. So lets best guess for duplicates with the lastest removal date as the most recent entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ba440",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE table public.posting_20200406 DISTSTYLE KEY DISTKEY (job_id) SORTKEY (remove_date) As \n",
    "SELECT job_id, company, post_date, salary, city, title, remove_date\n",
    "FROM\n",
    "( \n",
    " SELECT *, \n",
    " row_number() over (partition by job_id order by remove_date desc) rno \n",
    " From public.posting_20200406_stage \n",
    ") \n",
    "where rno = 1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e00f0e",
   "metadata": {},
   "source": [
    "# Task 3) Data update\n",
    "Now assume we have a history of posting data received from GHR stored in the tables posting_YYYYMMDD, where YYYYMMDD represents the date the data was received, and a full history of the posting data stored as positng_current. Please explain how you would update the posting_current table with new data stored at posting_20210601."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed538074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets assume that we did well on our part that job_id are distinct on posting_20210601 and posting_current\n",
    "\n",
    "#We make a copy because deleting is scary\n",
    "BEGIN;\n",
    "\n",
    "DROP TABLE IF EXISTS public.posting_current_old\n",
    "CREATE TABLE public.posting_current_old AS SELECT * FROM public.posting_current;\n",
    "#for the future we could just truncate the table and insert posting_current\n",
    "\n",
    "#We delete the jobs from posting current that exist in posting_20210601\n",
    "DELETE FROM public.posting_current\n",
    "WHERE job_id IN(\n",
    "     SELECT job_id\n",
    "     FROM public.posting_20210601\n",
    ")\n",
    "#We insert the posting_20210601 table into public.post_current\n",
    "INSERT INTO public.posting_current\n",
    "SELECT *\n",
    "FROM public.posting_20210601;\n",
    "\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909bb6a",
   "metadata": {},
   "source": [
    "# Task 4) Data processing\n",
    "Starting from the posting_current table, build a table in PostgreSQL that contains the counts of the new, active and removed job postings, as well as the average salaries of the new, active and removed job postings for each month (from 2014 to now). Each row should correspond to a single month and should have the columns (month, count_new, count_active, count_removed, salary_new, salary_active, salary_removed).\n",
    "For a given month, a posting is considered new if it was posted during that month. A posting is considered removed if it was removed during that month. A posting is considered active if it was posted in the current or a previous month and stays active through the month in which it is removed. For example, if posting_current had only one posting in it, posted in 2020-01 and removed in 2020-05, with a salary of 100K, the final aggregated table would look like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BEGIN;\n",
    "\n",
    "# Here we create a new post view for the following fields:\n",
    "# count_new = posting date month count of job ids\n",
    "# salary_new = posting date month SUM of salaries\n",
    "CREATE MATERIALIZED VIEW public.new_post_mv\n",
    "AS\n",
    "SELECT TOCHAR(DATE_TRUNC('month', posting_date),'YYYY-MM') AS month,\n",
    "    COUNT(job_id) AS count_new,\n",
    "    SUM(salary) AS salary_new\n",
    "FROM public.posting_current\n",
    "GROUP BY month;\n",
    "\n",
    "# Here we create a removed post view for the following fields:\n",
    "# count_remove = posting removal date count of job ids\n",
    "# salary_remove = posting removal date SUM of salaries\n",
    "CREATE MATERIALIZED VIEW public.remove_post_mv\n",
    "AS\n",
    "SELECT TO_CHAR(DATE_TRUNC('month', remove_date),'YYYY-MM') AS month,\n",
    "    COUNT(job_id) AS count_removed,\n",
    "    SUM(salary) AS salary_removed\n",
    "FROM public.posting_current\n",
    "GROUP BY month;\n",
    "\n",
    "# Here we create a view for the following fields:\n",
    "# count_active = all dates from posting date to (including) posting removal month count of job ids\n",
    "# salary_active = all dates from posting date to (including) posting removal month SUM of salaries\n",
    "# we use generate_series to create dates between the times span of post_date and remove_date\\\n",
    "# Note that we may get nulls in the remove_date because it could still be a current post.\n",
    "# We could fill the value with NOW(), for this exercise we will used the last date \n",
    "CREATE MATERIALIZED VIEW public.active_post_mv\n",
    "AS\n",
    "SELECT TO_CHAR(generate_series(DATE_TRUNC('month',post_date),\n",
    "                               DATE_TRUNC('month',\n",
    "                                  COALESCE(remove_date,'2020-06-01T00:00:00Z'::TIMESTAMP),\n",
    "                       '1 month'::interval),\n",
    "               'YYYY-MM') AS month,\n",
    "    COUNT(job_id) AS count_active,\n",
    "    SUM(salary) AS salary_active\n",
    "FROM public.posting_current\n",
    "GROUP BY month;\n",
    "\n",
    "# Here we create a view for the following fields:\n",
    "# month = all the dates from 2014 to 2020 (NOW() could be used to make it current)\n",
    "CREATE MATERIALIZED VIEW public.post_dates_mv\n",
    "AS\n",
    "SELECT TO_CHAR(generate_series,'YYYY-MM') as month \n",
    "FROM generate_series('2014-01-01T00:00:00Z'::TIMESTAMP,\n",
    "                              '2020-06-01T00:00:00Z'::TIMESTAMP,\n",
    "                              '1 month'::interval);\n",
    "\n",
    "\n",
    "# Here we join all the tables on the months from public.post_dates_mv\n",
    "# We watch out for null for months where there is no data\n",
    "CREATE TABLE public.post_counts AS \n",
    "SELECT pd.month, \n",
    "    COALESCE(np.count_new,0) AS count_new,\n",
    "    COALESCE(ap.count_active,0) AS count_active,\n",
    "    COALESCE(rp.count_remove,0) AS count_remove,\n",
    "    COALESCE(np.salary_new,0) AS salary_new,\n",
    "    COALESCE(ap.salary_active,0) AS salary_active,\n",
    "    COALESCE(rp.salary_remove,0) AS salary_remove,\n",
    "FROM public.post_dates_mv pd\n",
    "LEFT JOIN public.new_post_mv np USING(month)\n",
    "LEFT JOIN public.remove_post_mv rp USING(month)\n",
    "LEFT JOIN public.active_post_mv ap USING(month)\n",
    "ORDER BY month;\n",
    "\n",
    "COMMIT;\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b567f",
   "metadata": {},
   "source": [
    "# A different approach to the problem\n",
    "\n",
    "Just a few thoughts on a different pipeline we could use to achieve the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d4ce8",
   "metadata": {},
   "source": [
    "1. We could use a lambda to download the files to S3 directly by including an SFTP package. Then we could trigger it with an Eventbridge chron for once a week. Or we could mount an S3 bucket with FUSE on the EC2. \n",
    "```python\n",
    "import paramiko\n",
    "def lambda_handler(event, context):\n",
    "    k = paramiko.RSAKey.from_private_key_file(\"~/.ssh/id-rsa-revelio-new\")\n",
    "    c = paramiko.SSHClient()\n",
    "    c.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    print(\"connecting\")\n",
    "    c.connect( hostname = \"ghr-server\", username = \"client-revelio\", pkey = k )\n",
    "    print(\"connected\")\n",
    "    #we can iterate through the files\n",
    "    commands = [ \"cp file1 s3://bucket\", \"cp file1 s3://bucket\" ]\n",
    "    for command in commands:\n",
    "        print \"Executing {}\".format( command )\n",
    "        stdin , stdout, stderr = c.exec_command(command)\n",
    "        print stdout.read()\n",
    "        print( \"Errors\")\n",
    "        print stderr.read()\n",
    "    c.close()\n",
    "```\n",
    "2. Since its all in the S3, we can use Glue to help clean/verify data as an in between before SQL ingestion. We can also use Glue to change the data to parquet files so its faster to read into Athena and Redshift.\n",
    "\n",
    "3. Once we dump everything into an S3 bucket, we could use Athena instead of Redshift. Redshift seems like overkill for this amount of data so unless there is already a Redshift cluster running and you want to squeeze this data in there, its more cost effective to use Athena. We could use a lambda like below to be triggered by the previous lambda.\n",
    "```python\n",
    "import time\n",
    "import boto3\n",
    "\n",
    " #this only creates the table... we can add onto the query or create a ddl file and read it.\n",
    "query = \"\"\"CREATE TABLE public.posting_20200406\n",
    "(\n",
    "    job_id BIGINT,\n",
    "    company VARCHAR(200),\n",
    "    post_date TIMESTAMP,\n",
    "    salary  INT,\n",
    "    city VARCHAR(60),\n",
    "    title VARCHAR(70),\n",
    "    remove_date TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "DATABASE = 'GHR'\n",
    "output='s3://ghr_data/queries'\n",
    "def lambda_handler(event, context):\n",
    "    client = boto3.client('athena')\n",
    "    # Execution\n",
    "    response = client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': DATABASE\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output,\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983ad23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

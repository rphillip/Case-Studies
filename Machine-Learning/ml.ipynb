{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# ML: Predicting Star Ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Our objective is to predict a new venue's popularity from information available when the venue opens.  We will do this by machine learning from a data set of venue popularities provided by Yelp.  The data set contains meta data about the venue (where it is located, the type of food served, etc.).  It also contains a star rating. Note that the venues are not limited to restaurants. This tutorial will walk you through one way to build a machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Metrics and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "For most questions, you are asked to submit your models `predict` method to the grader. The grader uses a test set to evaluate your model's performance against our reference solution, using the $R^2$ score. It **is** possible to receive a score greater than one, indicating that you've beaten our reference model. We compare our model's score on a test set to your score on the same test set. See how high you can go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Download and parse the incoming data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We start by downloading the data set from Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataincubator-course/mldata/yelp_train_academic_dataset_business.json.gz to ./yelp_train_academic_dataset_business.json.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_business.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The training data are a series of JSON objects, in a Gzipped file. Python supports Gzipped files natively: [`gzip.open`](https://docs.python.org/3/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in `json` package has a `loads` function that converts a JSON string into a Python dictionary. We could call that once for each row of the file. [`ujson`](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` package, but is *substantially* faster (at the cost of non-robust handling of malformed JSON). We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import gzip\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In scikit-learn, the labels to be predicted, in this case, the stars, are always kept in a separate data structure than the features. Let's get in this habit now, by creating a separate list of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA',\n",
       " 'full_address': '4840 E Indian School Rd\\nSte 101\\nPhoenix, AZ 85018',\n",
       " 'hours': {'Tuesday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Friday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Monday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Wednesday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Thursday': {'close': '17:00', 'open': '08:00'}},\n",
       " 'open': True,\n",
       " 'categories': ['Doctors', 'Health & Medical'],\n",
       " 'city': 'Phoenix',\n",
       " 'review_count': 7,\n",
       " 'name': 'Eric Goldberg, MD',\n",
       " 'neighborhoods': [],\n",
       " 'longitude': -111.983758,\n",
       " 'state': 'AZ',\n",
       " 'stars': 3.5,\n",
       " 'latitude': 33.499313,\n",
       " 'attributes': {'By Appointment Only': True},\n",
       " 'type': 'business'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_ratings = [row['stars'] for row in data]\n",
    "row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A few things to consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "1. The test set used by the grader will be in the same form as `data`. For this miniproject, it will be a list of dictionaries. The models you will build will need to handle data of this type; we'll discuss this more further in the questions.\n",
    "1. You may find it useful to serialize your trained model using either [`dill`](https://pypi.python.org/pypi/dill) or [`joblib`](http://scikit-learn.org/stable/modules/model_persistence.html). That way, you can reload your model after restarting the Jupyter notebook without needing to retrain it.\n",
    "1. There are obvious mistakes in the data; there is no need to try to correct them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 1: city_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The venues belong to different cities.  You can imagine that the ratings in some cities are probably higher than others.  We wish to build an estimator to make a prediction based on this, but first we need to work out the average rating for each city.  For this problem, create a list of tuples (city name, star rating), one for each city in the data set. There are many ways to do this; please feel free to experiment on your own.  If you get stuck, the steps below attempt to guide you through the process.\n",
    "\n",
    "A simple approach is to go through all of the dictionaries in our array, calculating the sum of the star ratings and the number of venues for each city. At the end, we can just divide the stars by the count to get the average. We could create a separate sum and count variable for each city, but that will get tedious quickly. A better approach is to create a dictionary for each. The key will be the city name, and the value the running sum or running count.\n",
    "\n",
    "One slight annoyance of this approach is that we will have to test whether a key exists in the dictionary before adding to the running tally.  The `collections` module's `defaultdict` class works around this by providing default values for keys that haven't been used. Thus, if we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "star_sum = defaultdict(int)\n",
    "count = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "we can increment any key of `star_sum` or `count` without first worrying whether the key exists. We need to go through the `data` and `star_ratings` list together, which we can do with the `zip` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Phoenix': 24646.0,\n",
       "             'De Forest': 22.5,\n",
       "             'Mc Farland': 31.0,\n",
       "             'Middleton': 487.5,\n",
       "             'Madison': 5341.0,\n",
       "             'Sun Prairie': 231.5,\n",
       "             'Windsor': 21.0,\n",
       "             'Monona': 191.0,\n",
       "             'Chandler': 5384.0,\n",
       "             'Scottsdale': 12325.5,\n",
       "             'Tempe': 6640.5,\n",
       "             'Florence': 61.5,\n",
       "             'Peoria': 1939.5,\n",
       "             'Glendale': 3946.5,\n",
       "             'Cave Creek': 512.5,\n",
       "             'Paradise Valley': 260.5,\n",
       "             'Mesa': 6631.0,\n",
       "             'Ahwatukee': 29.5,\n",
       "             'Pheonix': 6.0,\n",
       "             'Anthem': 208.0,\n",
       "             'Gilbert': 3523.5,\n",
       "             'Gold Canyon': 98.0,\n",
       "             'Apache Junction': 291.0,\n",
       "             'Goldfield': 3.5,\n",
       "             'Casa Grande': 306.0,\n",
       "             'Coolidge': 27.5,\n",
       "             'Higley': 35.0,\n",
       "             'Queen Creek': 663.5,\n",
       "             'Sun Lakes': 29.0,\n",
       "             'Goodyear': 957.0,\n",
       "             'Fort Mcdowell': 8.0,\n",
       "             'Fountain Hills': 398.0,\n",
       "             'Fountain Hls': 3.0,\n",
       "             'Maricopa': 264.0,\n",
       "             'chandler': 5.0,\n",
       "             'Buckeye': 242.0,\n",
       "             'Litchfield Park': 222.0,\n",
       "             'Tonopah': 15.0,\n",
       "             'Surprise': 1273.0,\n",
       "             'Wickenburg': 164.5,\n",
       "             'Sedona': 3.5,\n",
       "             'Henderson': 6417.0,\n",
       "             'Las Vegas': 39621.0,\n",
       "             'North Las Vegas': 1595.5,\n",
       "             'N Las Vegas': 28.5,\n",
       "             'N. Las Vegas': 6.5,\n",
       "             'C Las Vegas': 3.0,\n",
       "             'Sunrise': 3.0,\n",
       "             'Las Vegas ': 18.0,\n",
       "             'Cottage Grove': 29.0,\n",
       "             'Dane': 8.5,\n",
       "             'Fitchburg': 284.0,\n",
       "             'Stoughton': 7.5,\n",
       "             'Verona': 119.5,\n",
       "             'Waunakee': 70.0,\n",
       "             'Trempealeau': 4.0,\n",
       "             'New River': 21.5,\n",
       "             'Avondale': 824.5,\n",
       "             'Rio Verde': 12.0,\n",
       "             'Guadalupe': 24.0,\n",
       "             'El Mirage': 111.0,\n",
       "             'Sun City': 248.5,\n",
       "             'Tolleson': 155.0,\n",
       "             'Carefree': 140.0,\n",
       "             'Gila Bend': 54.0,\n",
       "             'Laveen': 187.0,\n",
       "             'Youngtown': 54.0,\n",
       "             'Sun City West': 58.0,\n",
       "             'Boulder City': 91.0,\n",
       "             'Nellis Air Force Base': 3.0,\n",
       "             'Nellis AFB': 38.0,\n",
       "             'Green Valley': 4.0,\n",
       "             'North Scottsdale': 4.0,\n",
       "             'Litchfield Park ': 4.5,\n",
       "             'Nellis Afb': 6.5,\n",
       "             'Summerlin': 4.0,\n",
       "             'San Tan Valley': 120.0,\n",
       "             'Tortilla Flat': 12.5,\n",
       "             'Mesa ': 3.0,\n",
       "             'Fort McDowell': 23.5,\n",
       "             'Morristown': 10.5,\n",
       "             'Waddell': 11.5,\n",
       "             'W Henderson': 3.5,\n",
       "             'Las Vegas, NV 89147': 2.5,\n",
       "             'Las Vegas East': 4.5,\n",
       "             'DeForest': 20.5,\n",
       "             'Paradise': 55.5,\n",
       "             'Stanfield': 3.5,\n",
       "             'Atlanta': 3.5,\n",
       "             'Newberry Springs': 3.5,\n",
       "             'St Clements': 4.5,\n",
       "             'Waterloo': 530.0,\n",
       "             'Kitchener': 429.0,\n",
       "             'Cambridge': 21.0,\n",
       "             'Edinburgh': 9851.5,\n",
       "             'Musselburgh': 35.5,\n",
       "             'South Queensferry': 39.0,\n",
       "             'Roslin': 4.5,\n",
       "             'Dalkeith': 13.0,\n",
       "             'Loanhead': 19.0,\n",
       "             'Lasswade': 10.0,\n",
       "             'Penicuik': 4.0,\n",
       "             'Bonnyrigg': 7.5,\n",
       "             'Ratho': 3.0,\n",
       "             'St Jacobs': 7.0,\n",
       "             'Arcadia': 5.0,\n",
       "             'Tonto Basin': 11.5,\n",
       "             'Queensferry': 11.5,\n",
       "             'Spring Valley': 24.5,\n",
       "             'Enterprise': 29.0,\n",
       "             'McFarland': 22.0,\n",
       "             'Rochester': 5.0,\n",
       "             'North Las Vegas ': 3.5,\n",
       "             'N E Las Vegas': 5.0,\n",
       "             'N W Las Vegas': 3.5,\n",
       "             'Heidelberg': 2.5,\n",
       "             'Central Henderson': 3.5,\n",
       "             'Saguaro Lake': 3.0,\n",
       "             'Cramond': 8.5,\n",
       "             'Leith': 4.0,\n",
       "             'W Spring Valley': 3.0,\n",
       "             'North Queensferry': 3.5,\n",
       "             'Inverkeithing': 3.0,\n",
       "             'Portobello': 7.5,\n",
       "             'Water of Leith': 4.5,\n",
       "             'Centennial Hills': 3.0,\n",
       "             'Woolwich': 8.5,\n",
       "             'Victoria Park': 4.5,\n",
       "             'Juniper Green': 11.5,\n",
       "             'W Summerlin': 3.5,\n",
       "             'Lake Las Vegas': 4.5,\n",
       "             'Henderson ': 4.0,\n",
       "             'Straiton': 2.5,\n",
       "             'Deforest': 5.0,\n",
       "             'Summerlin South': 6.5,\n",
       "             'Sun City Anthem': 3.0,\n",
       "             'Clark County': 12.0,\n",
       "             'Phoenix ': 1.0,\n",
       "             'Glendale Az': 4.0,\n",
       "             'Mcfarland': 3.5,\n",
       "             'Newbridge': 4.5,\n",
       "             'Sunrise Manor': 3.0,\n",
       "             'Midlothian': 13.5,\n",
       "             'Henderson and Las vegas': 4.0,\n",
       "             'NELLIS AFB': 4.5,\n",
       "             'Surprise Crossing': 4.5,\n",
       "             'South Las Vegas': 4.5,\n",
       "             'Black Canyon City': 3.0,\n",
       "             'Fort Kinnaird': 4.5,\n",
       "             'Chandler-Gilbert': 5.0,\n",
       "             'Phoenix Sky Harbor Center': 3.5,\n",
       "             'Eagan': 5.0,\n",
       "             'Stockbridge': 9.0,\n",
       "             'New York': 5.0,\n",
       "             'City of Edinburgh': 3.0,\n",
       "             'South Gyle': 2.5,\n",
       "             'Central City Village': 12.5,\n",
       "             'New Town': 11.0,\n",
       "             'Scotland': 3.5,\n",
       "             'London': 3.0,\n",
       "             'Whitney': 4.5,\n",
       "             'St. Jacobs': 4.5,\n",
       "             'Old Town': 4.0,\n",
       "             'Newington': 3.0,\n",
       "             'Scottsdale, Phoenix, Chandler, Gilbert': 5.0,\n",
       "             'Henderson (Stephanie)': 4.0,\n",
       "             'Henderson (Green  Valley)': 4.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for row, stars in zip(data, star_ratings):\n",
    "    # increment the running sum in star_sum\n",
    "    star_sum[row['city']] += stars\n",
    "    # increment the running count in count\n",
    "    count[row['city']] += 1\n",
    "star_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we can calculate the average ratings.  Again, a dictionary makes a good container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_stars = dict()\n",
    "for city in star_sum:\n",
    "    avg_stars[city]=(star_sum[city]/count[city])\n",
    "len(avg_stars)\n",
    "    \n",
    "    # calculate average star rating and store in avg_stars\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There should be 167 different cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Check to see that we have 167 entries in the dictionary.\n",
    "grader.check(len(avg_stars) == 167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can get that list of tuples by converting the returned view object from the `items` method into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('ml__city_avg', list(avg_stars.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 2: city_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, let's build a custom estimator that will make a prediction based solely on the city of a venue.  It is tempting to hard-code the answers from the previous section into this model, but we're going to resist and do things properly.\n",
    "\n",
    "This custom estimator will have a `fit` method.  It will receive `data` as its argument `X` and `star_ratings` as `y`, and should repeat the calculation of the previous problem there.  Then the `predict` method can look up the average rating for the city of each record it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class CityRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.avg_stars = dict()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Store the average rating per city in self.avg_stars\n",
    "        \n",
    "        star_sum = defaultdict(int)\n",
    "        count = defaultdict(int)\n",
    "        for city, stars in zip(X,y):\n",
    "            star_sum[city] += stars\n",
    "            count[city] +=1\n",
    "        for city in star_sum:\n",
    "            \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we can create an instance of our regressor and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "city_model = CityRegressor()\n",
    "city_model.fit(data, star_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "city_model.predict(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There is a problem, however.  What happens if we're asked to estimate the rating of a venue in a city that's not in our training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "city_model.predict([{'city': 'Phoenix'}, {'city': 'Timbuktu'}, {'city': 'Madison'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Your model should always return a number, even if the city was not in the training data. Make sure it does before submitting your model's predict method to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('ml__city_model', city_model.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: lat_long_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You can imagine that a city-based model might not be sufficiently fine-grained. For example, we know that some neighborhoods are trendier than others.  Use the latitude and longitude of a venue as features that help you understand neighborhood dynamics.\n",
    "\n",
    "Since we need to select the appropriate columns from our dictionaries to build our latitude-longitude model, we will have to use scikit-learn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). However, the `ColumnTransformer` works with either NumPy arrays or pandas data frames. While we can convert our training data into a data frame easily, the test set the grader uses is a list of dictionaries. Thus, our first estimator in our workflow should be a transformer that converts a list of dictionaries into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class ToDataFrame(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return a pandas data frame from X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's test out the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "to_data_frame = ToDataFrame()\n",
    "X_t = to_data_frame.fit_transform(data[:5])\n",
    "\n",
    "# Check that our transformer properly transform the input data into a data frame\n",
    "grader.check((X_t == pd.DataFrame(data[:5])).all(axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we are ready to use `ColumnTransformer` and test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "selector = ...\n",
    "expected = np.array([data[0]['latitude'], data[0]['longitude']])\n",
    "\n",
    "# Check that our selector returns just two columns, the latitude and longitude\n",
    "grader.check((selector.fit_transform(X_t)[0] == expected).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, let's feed the output of the transformer in to a `KNeighborsRegressor`. As a sanity check, we'll test it with the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Training the model\n",
    "data_transform = to_data_frame.transform(data)\n",
    "data_transform = selector.fit_transform(data_transform)\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(data_transform, star_ratings)\n",
    "\n",
    "# Making predictions\n",
    "test_data = data[:5]\n",
    "test_data_transform = to_data_frame.transform(test_data)\n",
    "test_data_transform = selector.transform(test_data_transform)\n",
    "knn.predict(test_data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We are not ready to submit to the grader; there are two things we still need to do:\n",
    "1. Wrap all the steps necessary to go from our data (list of dictionaries) to predicted ratings\n",
    "1. Determine the optimal value for our predictor's hyperparameter\n",
    "\n",
    "For the first point, we will use a pipeline, ensuring that our model applies all the required transformations given the form of the input data. Remember that a pipeline is made with a list of `(step_name, estimator)` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now let's fit and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "pipe.fit(data, star_ratings)\n",
    "pipe.predict(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's now focus on the second point. The `KNeighborsRegressor` takes the `n_neighbors` hyperparameter, which tells it how many nearest neighbors to average together when making a prediction. There is no reason to believe that 5 is the optimum value. We will need to determine a better value for this hyperparameter. A common approach is to use a hyperparameter searching tool such as [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). You may need to refer back to the notebooks about the ways to interface searching tools and pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You should consider whether the data needs to be shuffled as it might not have been randomized. For example, the data could be ordered by a certain feature or by the labels. If you perform a train/test split with [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split), the data is shuffled by default. However, when using `GridSearchCV`, the folds are not shuffled when you use the default K-folds cross-validation.\n",
    "\n",
    "The code below will plot a rolling mean of the star ratings. Do you need to shuffle the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Series.rolling(Series(star_ratings), window=1000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you've found a good value of `n_neighbors`, submit the model to the grader. Note, \"good\" is a relative measure here. The reference solution has an $R^2$ score of only 0.02. There is just rather little signal available for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('ml__lat_long_model', lat_long_model.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 4: category_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "While location is important, we could also try seeing how predictive the\n",
    "venue's category is. Build an estimator that considers only the `'categories'` field of the data.\n",
    "\n",
    "The categories come as a list of strings, but the scikit-learn's predictors all need numeric input. We ultimately want to create a column in our feature matrix to represent every category. For a given row, only the columns that represent the categories it contains will be filled with a one, otherwise, it will be filled with a zero. The described method is similar to **one-hot encoding**, however, an observation/row can contain more than one \"hot\", non-zero, column.\n",
    "\n",
    "To achieve our encoding plan, we need to use scikit-learn's provides [`DictVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer). This transformer takes a 1-D array of dictionaries and creates a column in the output matrix for each key in the dictionary and fills it with the value associated with it. Missing keys are filled with zeros. However, we need to build a transformer that takes an array of strings and returns an array of dictionaries with keys given by those strings and values of one. For example, it should transform `X_in` into `X_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "X_in = pd.Series([['a'], ['b', 'c']])\n",
    "X_out = pd.Series([{'a': 1}, {'b': 1, 'c': 1}])\n",
    "\n",
    "print(X_in)\n",
    "print(X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "class DictEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X will be a pandas series. Return a pandas series of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now let's test out that our `DictEncoder` works out as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Check that DictEncoder transforms a series of list of strings into the expected series of dictionaries\n",
    "grader.check((DictEncoder().fit_transform(X_in) == X_out).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, create a pipeline object of the two step transformation for the categories data. Afterwards, create a `ColumnTransformer` object that will use the aforementioned pipeline object to transform the `'categories'` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Finally, create a pipeline object that will\n",
    "1. Convert our list of dictionaries into a data frame\n",
    "1. Select the `'categories'` column and encode the data\n",
    "1. Train a regularized linear model such as `Ridge`\n",
    "\n",
    "There will be a large number of features, one for each category, so there is a significant danger of overfitting. Use cross validation to choose the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('ml__category_model', category_model.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Extension:** Some categories (e.g., Restaurants) are not very specific. Others (Japanese sushi) are much more so.  One way to deal with this is with an measure call term frequency-inverse document frequency (tf-idf). Add in a [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) between the `DictVectorizer` and the linear model, and see if that improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 5: attribute_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There is even more information in the attributes for each venue.  Let's build an estimator based on these.\n",
    "\n",
    "Venues attributes may be nested:\n",
    "```python\n",
    "{\n",
    "  'Attire': 'casual',\n",
    "  'Accepts Credit Cards': True,\n",
    "  'Ambiance': {'casual': False, 'classy': False}\n",
    "}\n",
    "```\n",
    "We wish to encode them in the same manner as our categories data using the `DictVectorizer`. Before we do so, we need to flatten the dictionary to a single level:\n",
    "```python\n",
    "{\n",
    "  'Attire_casual' : 1,\n",
    "  'Accepts Credit Cards': 1,\n",
    "  'Ambiance_casual': 0,\n",
    "  'Ambiance_classy': 0\n",
    "}\n",
    "```\n",
    "Build a custom transformer that flattens the dictionary for the `'attributes'` field. Similar to what was done before, create a model that properly encodes the attribute data and learns to predict the ratings.\n",
    "\n",
    "You may find it difficult to find a single regressor that does well enough. A common solution is to use a linear model to fit the linear part of some data, and use a non-linear model to fit the residual that the linear model can't fit. Build a custom predictor that takes as an argument two other predictors. It should use the first to fit the raw data and the second to fit the residuals of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Create the transformer to handle the attributes data\n",
    "\n",
    "# Create the linear + non-linear ensemble predictor\n",
    "\n",
    "# Create the attribute model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('ml__attribute_model', attribute_model.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 6: full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "So far we have only built models based on individual features.  Now we will build an ensemble regressor that averages together the estimates of the four previous regressors.\n",
    "\n",
    "In order to use the existing models as input to a predictor, we will have to turn them into transformers; a predictor can only be in the final step of a pipeline. Build a custom `ModelTransformer` class that takes a predictor as an argument. When `fit` is called, the predictor should be fit. When `transform` is called, the predictor's `predict` method should be called, and its results returned as the transformation.\n",
    "\n",
    "Note that the output of the `transform` method should be a 2-D array with a single column in order for it to work well with the scikit-learn pipeline. If you're using NumPy arrays, you can use `.reshape(-1, 1)` to create a column vector. If you are just using Python lists, you will want a list of lists of single elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        # What needs to be done here?\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit the stored predictor.\n",
    "        # Question: what should be returned?\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Use predict on the stored predictor as a \"transformation\".\n",
    "        # Be sure to return a 2-D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's now test it out on our `city_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "city_trans = ModelTransformer(city_model)\n",
    "city_trans.fit(data, star_ratings)\n",
    "X_t = city_trans.transform(data[:5])\n",
    "\n",
    "# Check that the transformation output is a 2-D array with one column\n",
    "grader.check(np.array(X_t).shape[-1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "y_pred = np.array(city_model.predict(data[:5]))\n",
    "\n",
    "# Check that the transformation output is the same as the model's predictions\n",
    "grader.check((y_pred.reshape(-1, 1) == X_t).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Create an instance of `ModelTransformer` for each of the previous four models. Combine these together in a single feature matrix with a\n",
    "[`FeatureUnion`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "union = FeatureUnion([\n",
    "        # FeatureUnion uses the same syntax as Pipeline\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Our `FeatureUnion` object should return a feature matrix with four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "union.fit(data, star_ratings)\n",
    "X_t = union.transform(data[:5])\n",
    "\n",
    "# Transformed data should have 5 rows and 4 columns\n",
    "grader.check(X_t.shape == (5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Finally, use a pipeline to combine the feature union with a linear regression (or another model) to weight the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('ml__full_model', full_model.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Extension:** Try a non-linear model such as [`RandomForestRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) to blend the predictions of the four models. Are you able to get better results? If so, what do you think it's learning how to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2021 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

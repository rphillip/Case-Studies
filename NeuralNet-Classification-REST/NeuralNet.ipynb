{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b99935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "03e5503d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>220000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>88004</td>\n",
       "      <td>31237</td>\n",
       "      <td>15980</td>\n",
       "      <td>8500</td>\n",
       "      <td>20000</td>\n",
       "      <td>5003</td>\n",
       "      <td>3047</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8979</td>\n",
       "      <td>5190</td>\n",
       "      <td>0</td>\n",
       "      <td>1837</td>\n",
       "      <td>3526</td>\n",
       "      <td>8998</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20878</td>\n",
       "      <td>20582</td>\n",
       "      <td>19357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22000</td>\n",
       "      <td>4200</td>\n",
       "      <td>2000</td>\n",
       "      <td>3100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>80000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52774</td>\n",
       "      <td>11855</td>\n",
       "      <td>48944</td>\n",
       "      <td>85900</td>\n",
       "      <td>3409</td>\n",
       "      <td>1178</td>\n",
       "      <td>1926</td>\n",
       "      <td>52964</td>\n",
       "      <td>1804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36535</td>\n",
       "      <td>32428</td>\n",
       "      <td>15313</td>\n",
       "      <td>2078</td>\n",
       "      <td>1800</td>\n",
       "      <td>1430</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                            \n",
       "1          20000    2          2         1   24      2      2     -1     -1   \n",
       "2         120000    2          2         2   26     -1      2      0      0   \n",
       "3          90000    2          2         2   34      0      0      0      0   \n",
       "4          50000    2          2         1   37      0      0      0      0   \n",
       "5          50000    1          2         1   57     -1      0     -1      0   \n",
       "...          ...  ...        ...       ...  ...    ...    ...    ...    ...   \n",
       "29996     220000    1          3         1   39      0      0      0      0   \n",
       "29997     150000    1          3         2   43     -1     -1     -1     -1   \n",
       "29998      30000    1          2         2   37      4      3      2     -1   \n",
       "29999      80000    1          3         1   41      1     -1      0      0   \n",
       "30000      50000    1          2         1   46      0      0      0      0   \n",
       "\n",
       "       PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "ID            ...                                                        \n",
       "1         -2  ...          0          0          0         0       689   \n",
       "2          0  ...       3272       3455       3261         0      1000   \n",
       "3          0  ...      14331      14948      15549      1518      1500   \n",
       "4          0  ...      28314      28959      29547      2000      2019   \n",
       "5          0  ...      20940      19146      19131      2000     36681   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29996      0  ...      88004      31237      15980      8500     20000   \n",
       "29997      0  ...       8979       5190          0      1837      3526   \n",
       "29998      0  ...      20878      20582      19357         0         0   \n",
       "29999      0  ...      52774      11855      48944     85900      3409   \n",
       "30000      0  ...      36535      32428      15313      2078      1800   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "ID                                                                         \n",
       "1             0         0         0         0                           1  \n",
       "2          1000      1000         0      2000                           1  \n",
       "3          1000      1000      1000      5000                           0  \n",
       "4          1200      1100      1069      1000                           0  \n",
       "5         10000      9000       689       679                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29996      5003      3047      5000      1000                           0  \n",
       "29997      8998       129         0         0                           0  \n",
       "29998     22000      4200      2000      3100                           1  \n",
       "29999      1178      1926     52964      1804                           1  \n",
       "30000      1430      1000      1000      1000                           1  \n",
       "\n",
       "[30000 rows x 24 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"default of credit card clients.xls\", header=1, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da0793df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIMIT_BAL                      10000\n",
       "SEX                                1\n",
       "EDUCATION                          0\n",
       "MARRIAGE                           0\n",
       "AGE                               21\n",
       "PAY_0                             -2\n",
       "PAY_2                             -2\n",
       "PAY_3                             -2\n",
       "PAY_4                             -2\n",
       "PAY_5                             -2\n",
       "PAY_6                             -2\n",
       "BILL_AMT1                    -165580\n",
       "BILL_AMT2                     -69777\n",
       "BILL_AMT3                    -157264\n",
       "BILL_AMT4                    -170000\n",
       "BILL_AMT5                     -81334\n",
       "BILL_AMT6                    -339603\n",
       "PAY_AMT1                           0\n",
       "PAY_AMT2                           0\n",
       "PAY_AMT3                           0\n",
       "PAY_AMT4                           0\n",
       "PAY_AMT5                           0\n",
       "PAY_AMT6                           0\n",
       "default payment next month         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mindf = df.min(skipna = False)\n",
    "mindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ad3820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIMIT_BAL                     1000000\n",
       "SEX                                 2\n",
       "EDUCATION                           6\n",
       "MARRIAGE                            3\n",
       "AGE                                79\n",
       "PAY_0                               8\n",
       "PAY_2                               8\n",
       "PAY_3                               8\n",
       "PAY_4                               8\n",
       "PAY_5                               8\n",
       "PAY_6                               8\n",
       "BILL_AMT1                      964511\n",
       "BILL_AMT2                      983931\n",
       "BILL_AMT3                     1664089\n",
       "BILL_AMT4                      891586\n",
       "BILL_AMT5                      927171\n",
       "BILL_AMT6                      961664\n",
       "PAY_AMT1                       873552\n",
       "PAY_AMT2                      1684259\n",
       "PAY_AMT3                       896040\n",
       "PAY_AMT4                       621000\n",
       "PAY_AMT5                       426529\n",
       "PAY_AMT6                       528666\n",
       "default payment next month          1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxdf = df.max(skipna = False)\n",
    "maxdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "73dc9fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='default payment next month', ylabel='count'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'default payment next month', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aab35c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2682</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13559</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49291</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35835</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>220000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>208365</td>\n",
       "      <td>88004</td>\n",
       "      <td>31237</td>\n",
       "      <td>15980</td>\n",
       "      <td>8500</td>\n",
       "      <td>20000</td>\n",
       "      <td>5003</td>\n",
       "      <td>3047</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>150000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3502</td>\n",
       "      <td>8979</td>\n",
       "      <td>5190</td>\n",
       "      <td>0</td>\n",
       "      <td>1837</td>\n",
       "      <td>3526</td>\n",
       "      <td>8998</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2758</td>\n",
       "      <td>20878</td>\n",
       "      <td>20582</td>\n",
       "      <td>19357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22000</td>\n",
       "      <td>4200</td>\n",
       "      <td>2000</td>\n",
       "      <td>3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>80000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>76304</td>\n",
       "      <td>52774</td>\n",
       "      <td>11855</td>\n",
       "      <td>48944</td>\n",
       "      <td>85900</td>\n",
       "      <td>3409</td>\n",
       "      <td>1178</td>\n",
       "      <td>1926</td>\n",
       "      <td>52964</td>\n",
       "      <td>1804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49764</td>\n",
       "      <td>36535</td>\n",
       "      <td>32428</td>\n",
       "      <td>15313</td>\n",
       "      <td>2078</td>\n",
       "      <td>1800</td>\n",
       "      <td>1430</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                            \n",
       "1          20000    2          2         1   24      2      2     -1     -1   \n",
       "2         120000    2          2         2   26     -1      2      0      0   \n",
       "3          90000    2          2         2   34      0      0      0      0   \n",
       "4          50000    2          2         1   37      0      0      0      0   \n",
       "5          50000    1          2         1   57     -1      0     -1      0   \n",
       "...          ...  ...        ...       ...  ...    ...    ...    ...    ...   \n",
       "29996     220000    1          3         1   39      0      0      0      0   \n",
       "29997     150000    1          3         2   43     -1     -1     -1     -1   \n",
       "29998      30000    1          2         2   37      4      3      2     -1   \n",
       "29999      80000    1          3         1   41      1     -1      0      0   \n",
       "30000      50000    1          2         1   46      0      0      0      0   \n",
       "\n",
       "       PAY_5  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  \\\n",
       "ID            ...                                                         \n",
       "1         -2  ...        689          0          0          0         0   \n",
       "2          0  ...       2682       3272       3455       3261         0   \n",
       "3          0  ...      13559      14331      14948      15549      1518   \n",
       "4          0  ...      49291      28314      28959      29547      2000   \n",
       "5          0  ...      35835      20940      19146      19131      2000   \n",
       "...      ...  ...        ...        ...        ...        ...       ...   \n",
       "29996      0  ...     208365      88004      31237      15980      8500   \n",
       "29997      0  ...       3502       8979       5190          0      1837   \n",
       "29998      0  ...       2758      20878      20582      19357         0   \n",
       "29999      0  ...      76304      52774      11855      48944     85900   \n",
       "30000      0  ...      49764      36535      32428      15313      2078   \n",
       "\n",
       "       PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "ID                                                       \n",
       "1           689         0         0         0         0  \n",
       "2          1000      1000      1000         0      2000  \n",
       "3          1500      1000      1000      1000      5000  \n",
       "4          2019      1200      1100      1069      1000  \n",
       "5         36681     10000      9000       689       679  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "29996     20000      5003      3047      5000      1000  \n",
       "29997      3526      8998       129         0         0  \n",
       "29998         0     22000      4200      2000      3100  \n",
       "29999      3409      1178      1926     52964      1804  \n",
       "30000      1800      1430      1000      1000      1000  \n",
       "\n",
       "[30000 rows x 23 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.iloc[:,:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "386b944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(df.iloc[:,:23], df['default payment next month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f4c5153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=[0,1],y=np.bincount(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = smote_enn.fit_resample(df.iloc[:,:23], df['default payment next month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2a221a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd807726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "sm = SMOTENC(random_state=42, categorical_features=[1,2,3,5,6,7,8,9,10])\n",
    "X_res, y_res = sm.fit_resample(df.iloc[:,:23], df['default payment next month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bdc40da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=[0,1],y=np.bincount(y_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69791f3a",
   "metadata": {},
   "source": [
    "# Scale everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d1c4fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "54abada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale_torch = torch.FloatTensor(X_scale)\n",
    "y_scale_torch = torch.FloatTensor(y_res)\n",
    "y_scale_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "3436dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetBinaryClassifier\n",
    "from classes import MyModule\n",
    "class toTensor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return torch.FloatTensor(X)\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=128, dropoutrate = 0.5):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.dropoutrate = dropoutrate\n",
    "        self.layer1 = nn.Linear(23, num_units)\n",
    "        self.nonlin = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropoutrate)\n",
    "        self.dropout2 = nn.Dropout(self.dropoutrate)\n",
    "        self.layer2 = nn.Linear(num_units, num_units)\n",
    "        self.output = nn.Linear(num_units,1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.layer1(X))\n",
    "        X = self.batchnorm1(X)\n",
    "        X = self.dropout1(X)\n",
    "        X = self.nonlin(self.layer2(X))\n",
    "        X = self.batchnorm2(X)\n",
    "        X = self.dropout2(X)\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "model = NeuralNetBinaryClassifier(\n",
    "    MyModule(dropoutrate = 0.2),\n",
    "    max_epochs=40,\n",
    "    lr=0.01,\n",
    "    batch_size=128,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "15e5cb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.5481\u001b[0m       \u001b[32m0.6897\u001b[0m        \u001b[35m0.5846\u001b[0m  1.6928\n",
      "      2        0.5493       0.6884        0.5853  1.5468\n",
      "      3        \u001b[36m0.5474\u001b[0m       0.6857        0.5868  1.5793\n",
      "      4        0.5481       \u001b[32m0.6899\u001b[0m        \u001b[35m0.5827\u001b[0m  1.8644\n",
      "      5        0.5481       \u001b[32m0.6930\u001b[0m        \u001b[35m0.5826\u001b[0m  1.8235\n",
      "      6        0.5475       0.6849        0.5842  1.8484\n",
      "      7        \u001b[36m0.5448\u001b[0m       0.6930        \u001b[35m0.5825\u001b[0m  1.7220\n",
      "      8        0.5470       0.6921        0.5852  1.7885\n",
      "      9        \u001b[36m0.5430\u001b[0m       0.6868        0.5868  1.6701\n",
      "     10        0.5454       0.6913        0.5839  1.7286\n",
      "     11        0.5441       0.6882        0.5863  1.7842\n",
      "     12        0.5450       0.6854        0.5829  1.8867\n",
      "     13        0.5448       0.6895        0.5841  1.7885\n",
      "     14        0.5441       \u001b[32m0.6940\u001b[0m        0.5830  1.7386\n",
      "     15        \u001b[36m0.5429\u001b[0m       0.6892        0.5869  1.5763\n",
      "     16        0.5431       0.6915        0.5865  2.2282\n",
      "     17        \u001b[36m0.5428\u001b[0m       0.6876        0.5879  2.1400\n",
      "     18        \u001b[36m0.5422\u001b[0m       0.6885        0.5847  2.0586\n",
      "     19        0.5437       0.6834        0.5859  2.2294\n",
      "     20        \u001b[36m0.5418\u001b[0m       0.6886        0.5852  1.7133\n",
      "     21        0.5419       0.6898        0.5830  2.1115\n",
      "     22        0.5436       0.6910        0.5828  1.9653\n",
      "     23        \u001b[36m0.5411\u001b[0m       0.6895        0.5866  1.9435\n",
      "     24        \u001b[36m0.5409\u001b[0m       0.6872        0.5861  1.8671\n",
      "     25        \u001b[36m0.5398\u001b[0m       0.6866        0.5864  1.7620\n",
      "     26        0.5402       0.6872        0.5867  1.9456\n",
      "     27        \u001b[36m0.5390\u001b[0m       0.6899        0.5843  1.7428\n",
      "     28        0.5392       0.6874        0.5856  1.5178\n",
      "     29        0.5398       0.6903        0.5866  1.6236\n",
      "     30        0.5403       0.6860        0.5879  1.5265\n",
      "     31        \u001b[36m0.5389\u001b[0m       0.6929        0.5840  1.5700\n",
      "     32        0.5390       0.6876        0.5862  1.5404\n",
      "     33        0.5399       0.6861        0.5883  1.4063\n",
      "     34        0.5390       \u001b[32m0.6975\u001b[0m        \u001b[35m0.5770\u001b[0m  1.4633\n",
      "     35        0.5402       0.6899        0.5867  1.5340\n",
      "     36        \u001b[36m0.5376\u001b[0m       0.6816        0.5931  1.3643\n",
      "     37        \u001b[36m0.5375\u001b[0m       0.6957        0.5815  1.5661\n",
      "     38        \u001b[36m0.5365\u001b[0m       0.6853        0.5851  1.5214\n",
      "     39        0.5365       0.6817        0.5869  1.4918\n",
      "     40        \u001b[36m0.5360\u001b[0m       0.6850        0.5900  1.4744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetBinaryClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (layer1): Linear(in_features=23, out_features=128, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (output): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (batchnorm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_scale_torch, y_scale_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d275d578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='epoch'>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_loss = []\n",
    "train_loss = []\n",
    "epochs = range(1,41)\n",
    "for i in range(40):\n",
    "    val_loss.append(model.history[i]['valid_loss'])\n",
    "    train_loss.append(model.history[i]['train_loss'])\n",
    "dfloss = (pd.DataFrame({'epoch': epochs, 'val_loss': val_loss, 'train_loss': train_loss}, \n",
    "                       columns=['epoch', 'val_loss', 'train_loss']).set_index('epoch'))\n",
    "sns.lineplot(data=dfloss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "305a11a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7422\u001b[0m       \u001b[32m0.5981\u001b[0m        \u001b[35m0.6681\u001b[0m  0.8009\n",
      "      2        \u001b[36m0.7018\u001b[0m       \u001b[32m0.6262\u001b[0m        \u001b[35m0.6462\u001b[0m  0.8059\n",
      "      3        \u001b[36m0.6801\u001b[0m       \u001b[32m0.6437\u001b[0m        \u001b[35m0.6344\u001b[0m  0.7908\n",
      "      4        \u001b[36m0.6680\u001b[0m       0.6437        \u001b[35m0.6290\u001b[0m  0.7710\n",
      "      5        \u001b[36m0.6595\u001b[0m       \u001b[32m0.6555\u001b[0m        \u001b[35m0.6214\u001b[0m  0.7588\n",
      "      6        \u001b[36m0.6514\u001b[0m       0.6536        \u001b[35m0.6189\u001b[0m  0.7778\n",
      "      7        \u001b[36m0.6447\u001b[0m       \u001b[32m0.6563\u001b[0m        \u001b[35m0.6153\u001b[0m  0.7794\n",
      "      8        \u001b[36m0.6411\u001b[0m       \u001b[32m0.6585\u001b[0m        \u001b[35m0.6121\u001b[0m  0.7884\n",
      "      9        \u001b[36m0.6363\u001b[0m       \u001b[32m0.6602\u001b[0m        \u001b[35m0.6110\u001b[0m  0.7841\n",
      "     10        \u001b[36m0.6346\u001b[0m       \u001b[32m0.6656\u001b[0m        \u001b[35m0.6090\u001b[0m  0.7597\n",
      "     11        \u001b[36m0.6308\u001b[0m       0.6628        \u001b[35m0.6084\u001b[0m  0.7602\n",
      "     12        \u001b[36m0.6289\u001b[0m       0.6626        0.6086  0.7605\n",
      "     13        \u001b[36m0.6263\u001b[0m       0.6646        \u001b[35m0.6060\u001b[0m  0.7634\n",
      "     14        \u001b[36m0.6229\u001b[0m       0.6643        0.6061  0.7471\n",
      "     15        \u001b[36m0.6227\u001b[0m       0.6622        0.6068  0.7732\n",
      "     16        0.6227       \u001b[32m0.6665\u001b[0m        \u001b[35m0.6045\u001b[0m  0.7986\n",
      "     17        \u001b[36m0.6201\u001b[0m       0.6625        0.6066  0.7884\n",
      "     18        \u001b[36m0.6191\u001b[0m       0.6602        0.6078  0.7631\n",
      "     19        \u001b[36m0.6184\u001b[0m       0.6655        \u001b[35m0.6019\u001b[0m  0.8011\n",
      "     20        \u001b[36m0.6175\u001b[0m       0.6665        \u001b[35m0.6017\u001b[0m  0.8340\n",
      "     21        \u001b[36m0.6159\u001b[0m       \u001b[32m0.6705\u001b[0m        0.6017  0.8640\n",
      "     22        \u001b[36m0.6156\u001b[0m       0.6645        \u001b[35m0.6012\u001b[0m  0.8956\n",
      "     23        \u001b[36m0.6142\u001b[0m       0.6626        0.6047  0.8541\n",
      "     24        \u001b[36m0.6127\u001b[0m       0.6692        \u001b[35m0.5995\u001b[0m  0.7387\n",
      "     25        \u001b[36m0.6112\u001b[0m       0.6662        0.6013  0.8709\n",
      "     26        \u001b[36m0.6085\u001b[0m       0.6676        0.6005  0.9085\n",
      "     27        0.6109       0.6652        \u001b[35m0.5993\u001b[0m  0.8738\n",
      "     28        0.6095       0.6672        \u001b[35m0.5987\u001b[0m  0.8432\n",
      "     29        \u001b[36m0.6079\u001b[0m       0.6685        0.6003  0.9241\n",
      "     30        \u001b[36m0.6072\u001b[0m       0.6673        \u001b[35m0.5978\u001b[0m  0.9492\n",
      "     31        0.6095       0.6693        0.6028  0.9233\n",
      "     32        0.6075       0.6685        \u001b[35m0.5968\u001b[0m  0.8054\n",
      "     33        \u001b[36m0.6057\u001b[0m       0.6670        0.5970  0.8271\n",
      "     34        0.6080       0.6693        \u001b[35m0.5962\u001b[0m  0.8135\n",
      "     35        \u001b[36m0.6038\u001b[0m       0.6665        0.5994  0.8080\n",
      "     36        0.6046       \u001b[32m0.6745\u001b[0m        \u001b[35m0.5954\u001b[0m  0.7959\n",
      "     37        0.6042       \u001b[32m0.6748\u001b[0m        0.5995  0.7966\n",
      "     38        0.6046       0.6736        0.5970  0.7870\n",
      "     39        \u001b[36m0.6024\u001b[0m       0.6648        0.5960  0.8368\n",
      "     40        0.6035       0.6736        \u001b[35m0.5949\u001b[0m  0.7449\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7240\u001b[0m       \u001b[32m0.6098\u001b[0m        \u001b[35m0.6616\u001b[0m  0.7022\n",
      "      2        \u001b[36m0.6868\u001b[0m       \u001b[32m0.6231\u001b[0m        \u001b[35m0.6447\u001b[0m  0.7312\n",
      "      3        \u001b[36m0.6668\u001b[0m       \u001b[32m0.6391\u001b[0m        \u001b[35m0.6362\u001b[0m  0.7035\n",
      "      4        \u001b[36m0.6536\u001b[0m       \u001b[32m0.6482\u001b[0m        \u001b[35m0.6298\u001b[0m  0.7796\n",
      "      5        \u001b[36m0.6464\u001b[0m       \u001b[32m0.6561\u001b[0m        \u001b[35m0.6272\u001b[0m  0.8602\n",
      "      6        \u001b[36m0.6427\u001b[0m       \u001b[32m0.6585\u001b[0m        \u001b[35m0.6228\u001b[0m  0.9531\n",
      "      7        \u001b[36m0.6377\u001b[0m       \u001b[32m0.6623\u001b[0m        \u001b[35m0.6211\u001b[0m  0.9218\n",
      "      8        \u001b[36m0.6353\u001b[0m       \u001b[32m0.6642\u001b[0m        \u001b[35m0.6185\u001b[0m  0.8628\n",
      "      9        \u001b[36m0.6335\u001b[0m       0.6642        \u001b[35m0.6181\u001b[0m  0.8745\n",
      "     10        \u001b[36m0.6279\u001b[0m       0.6632        \u001b[35m0.6171\u001b[0m  0.9107\n",
      "     11        \u001b[36m0.6237\u001b[0m       \u001b[32m0.6743\u001b[0m        \u001b[35m0.6151\u001b[0m  0.8516\n",
      "     12        \u001b[36m0.6236\u001b[0m       0.6742        \u001b[35m0.6145\u001b[0m  0.8030\n",
      "     13        \u001b[36m0.6219\u001b[0m       0.6632        0.6151  0.8129\n",
      "     14        \u001b[36m0.6184\u001b[0m       \u001b[32m0.6805\u001b[0m        \u001b[35m0.6130\u001b[0m  0.7993\n",
      "     15        0.6192       0.6752        \u001b[35m0.6121\u001b[0m  0.8758\n",
      "     16        \u001b[36m0.6160\u001b[0m       0.6738        0.6130  0.8165\n",
      "     17        \u001b[36m0.6145\u001b[0m       \u001b[32m0.6807\u001b[0m        \u001b[35m0.6107\u001b[0m  0.8668\n",
      "     18        \u001b[36m0.6142\u001b[0m       0.6797        0.6111  0.9158\n",
      "     19        0.6157       0.6780        0.6109  0.8874\n",
      "     20        0.6147       \u001b[32m0.6817\u001b[0m        \u001b[35m0.6103\u001b[0m  0.7932\n",
      "     21        \u001b[36m0.6137\u001b[0m       \u001b[32m0.6829\u001b[0m        \u001b[35m0.6099\u001b[0m  1.1629\n",
      "     22        \u001b[36m0.6095\u001b[0m       \u001b[32m0.6840\u001b[0m        0.6111  0.8394\n",
      "     23        \u001b[36m0.6081\u001b[0m       0.6779        0.6104  0.7875\n",
      "     24        0.6097       0.6760        0.6101  0.7587\n",
      "     25        0.6091       \u001b[32m0.6843\u001b[0m        0.6105  0.7656\n",
      "     26        \u001b[36m0.6072\u001b[0m       0.6820        0.6107  0.7538\n",
      "     27        \u001b[36m0.6069\u001b[0m       0.6777        \u001b[35m0.6095\u001b[0m  0.7461\n",
      "     28        0.6089       0.6725        0.6100  0.7482\n",
      "     29        \u001b[36m0.6064\u001b[0m       0.6817        \u001b[35m0.6076\u001b[0m  0.7581\n",
      "     30        \u001b[36m0.6063\u001b[0m       0.6807        0.6079  0.7499\n",
      "     31        \u001b[36m0.6039\u001b[0m       0.6799        0.6088  0.7717\n",
      "     32        0.6071       0.6782        0.6090  0.8257\n",
      "     33        0.6043       0.6812        0.6081  0.7591\n",
      "     34        0.6043       0.6840        0.6094  0.7547\n",
      "     35        0.6047       0.6809        0.6081  0.7363\n",
      "     36        \u001b[36m0.6028\u001b[0m       0.6792        0.6081  0.7522\n",
      "     37        \u001b[36m0.6016\u001b[0m       0.6793        0.6080  0.7416\n",
      "     38        0.6038       \u001b[32m0.6853\u001b[0m        \u001b[35m0.6075\u001b[0m  0.7449\n",
      "     39        0.6026       0.6766        0.6078  0.7514\n",
      "     40        0.6021       0.6786        \u001b[35m0.6071\u001b[0m  0.7429\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7363\u001b[0m       \u001b[32m0.5979\u001b[0m        \u001b[35m0.6626\u001b[0m  0.7429\n",
      "      2        \u001b[36m0.6836\u001b[0m       \u001b[32m0.6214\u001b[0m        \u001b[35m0.6446\u001b[0m  0.7643\n",
      "      3        \u001b[36m0.6628\u001b[0m       \u001b[32m0.6358\u001b[0m        \u001b[35m0.6367\u001b[0m  0.7566\n",
      "      4        \u001b[36m0.6558\u001b[0m       \u001b[32m0.6389\u001b[0m        \u001b[35m0.6300\u001b[0m  0.7597\n",
      "      5        \u001b[36m0.6449\u001b[0m       \u001b[32m0.6506\u001b[0m        \u001b[35m0.6254\u001b[0m  0.7447\n",
      "      6        \u001b[36m0.6414\u001b[0m       \u001b[32m0.6526\u001b[0m        \u001b[35m0.6226\u001b[0m  0.7568\n",
      "      7        \u001b[36m0.6358\u001b[0m       \u001b[32m0.6539\u001b[0m        \u001b[35m0.6213\u001b[0m  0.7429\n",
      "      8        \u001b[36m0.6335\u001b[0m       \u001b[32m0.6568\u001b[0m        \u001b[35m0.6190\u001b[0m  0.7495\n",
      "      9        \u001b[36m0.6323\u001b[0m       \u001b[32m0.6585\u001b[0m        \u001b[35m0.6171\u001b[0m  0.7485\n",
      "     10        \u001b[36m0.6258\u001b[0m       \u001b[32m0.6598\u001b[0m        0.6176  0.7524\n",
      "     11        \u001b[36m0.6219\u001b[0m       \u001b[32m0.6648\u001b[0m        \u001b[35m0.6158\u001b[0m  0.7740\n",
      "     12        \u001b[36m0.6199\u001b[0m       \u001b[32m0.6655\u001b[0m        \u001b[35m0.6152\u001b[0m  0.9099\n",
      "     13        0.6218       \u001b[32m0.6678\u001b[0m        \u001b[35m0.6145\u001b[0m  0.8077\n",
      "     14        0.6201       0.6628        0.6151  0.8408\n",
      "     15        \u001b[36m0.6170\u001b[0m       \u001b[32m0.6709\u001b[0m        \u001b[35m0.6121\u001b[0m  0.9577\n",
      "     16        \u001b[36m0.6157\u001b[0m       0.6669        0.6132  0.8927\n",
      "     17        0.6195       0.6609        0.6147  0.8456\n",
      "     18        \u001b[36m0.6125\u001b[0m       0.6696        \u001b[35m0.6118\u001b[0m  0.7558\n",
      "     19        0.6137       0.6706        0.6119  0.7619\n",
      "     20        0.6135       0.6690        0.6123  0.8154\n",
      "     21        \u001b[36m0.6098\u001b[0m       0.6689        \u001b[35m0.6117\u001b[0m  0.8306\n",
      "     22        0.6109       \u001b[32m0.6799\u001b[0m        \u001b[35m0.6103\u001b[0m  0.8831\n",
      "     23        0.6120       0.6718        0.6107  0.8229\n",
      "     24        \u001b[36m0.6091\u001b[0m       0.6689        0.6112  0.7922\n",
      "     25        \u001b[36m0.6088\u001b[0m       0.6725        0.6103  0.7907\n",
      "     26        \u001b[36m0.6082\u001b[0m       0.6712        0.6105  0.8171\n",
      "     27        0.6085       0.6783        \u001b[35m0.6088\u001b[0m  0.8175\n",
      "     28        \u001b[36m0.6074\u001b[0m       0.6777        0.6094  0.8614\n",
      "     29        \u001b[36m0.6069\u001b[0m       0.6762        0.6090  0.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     30        0.6074       0.6678        0.6100  0.8734\n",
      "     31        \u001b[36m0.6040\u001b[0m       \u001b[32m0.6803\u001b[0m        0.6091  0.8685\n",
      "     32        0.6059       0.6762        \u001b[35m0.6088\u001b[0m  0.8247\n",
      "     33        0.6057       \u001b[32m0.6825\u001b[0m        \u001b[35m0.6071\u001b[0m  0.8793\n",
      "     34        0.6056       0.6812        0.6074  0.7982\n",
      "     35        0.6052       0.6755        0.6084  0.8113\n",
      "     36        0.6043       0.6750        0.6077  0.8190\n",
      "     37        0.6042       0.6756        0.6080  0.9951\n",
      "     38        \u001b[36m0.6027\u001b[0m       0.6675        0.6081  0.8635\n",
      "     39        0.6031       0.6770        \u001b[35m0.6063\u001b[0m  0.8387\n",
      "     40        \u001b[36m0.6025\u001b[0m       0.6813        \u001b[35m0.6063\u001b[0m  0.9224\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7220\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.6578\u001b[0m  0.9042\n",
      "      2        \u001b[36m0.6877\u001b[0m       \u001b[32m0.6408\u001b[0m        \u001b[35m0.6447\u001b[0m  0.8030\n",
      "      3        \u001b[36m0.6684\u001b[0m       \u001b[32m0.6549\u001b[0m        \u001b[35m0.6348\u001b[0m  0.7630\n",
      "      4        \u001b[36m0.6585\u001b[0m       \u001b[32m0.6612\u001b[0m        \u001b[35m0.6297\u001b[0m  0.7723\n",
      "      5        \u001b[36m0.6521\u001b[0m       \u001b[32m0.6675\u001b[0m        \u001b[35m0.6259\u001b[0m  0.7663\n",
      "      6        \u001b[36m0.6447\u001b[0m       \u001b[32m0.6706\u001b[0m        \u001b[35m0.6221\u001b[0m  0.7795\n",
      "      7        \u001b[36m0.6413\u001b[0m       0.6702        \u001b[35m0.6219\u001b[0m  0.7864\n",
      "      8        \u001b[36m0.6390\u001b[0m       \u001b[32m0.6732\u001b[0m        0.6219  0.8314\n",
      "      9        \u001b[36m0.6362\u001b[0m       0.6698        \u001b[35m0.6183\u001b[0m  0.8303\n",
      "     10        \u001b[36m0.6332\u001b[0m       \u001b[32m0.6740\u001b[0m        \u001b[35m0.6169\u001b[0m  0.8293\n",
      "     11        \u001b[36m0.6306\u001b[0m       0.6699        \u001b[35m0.6161\u001b[0m  0.8144\n",
      "     12        \u001b[36m0.6299\u001b[0m       \u001b[32m0.6752\u001b[0m        \u001b[35m0.6154\u001b[0m  0.7567\n",
      "     13        \u001b[36m0.6289\u001b[0m       \u001b[32m0.6777\u001b[0m        \u001b[35m0.6151\u001b[0m  0.7637\n",
      "     14        \u001b[36m0.6276\u001b[0m       0.6739        \u001b[35m0.6147\u001b[0m  0.7745\n",
      "     15        \u001b[36m0.6265\u001b[0m       0.6738        \u001b[35m0.6135\u001b[0m  0.7551\n",
      "     16        0.6271       0.6770        0.6144  0.7434\n",
      "     17        \u001b[36m0.6208\u001b[0m       0.6773        0.6143  0.7598\n",
      "     18        0.6222       \u001b[32m0.6815\u001b[0m        0.6162  0.7507\n",
      "     19        \u001b[36m0.6196\u001b[0m       0.6783        \u001b[35m0.6116\u001b[0m  0.7466\n",
      "     20        \u001b[36m0.6191\u001b[0m       0.6773        0.6119  0.7579\n",
      "     21        0.6194       0.6802        0.6139  0.7653\n",
      "     22        0.6205       0.6792        0.6126  0.7456\n",
      "     23        \u001b[36m0.6184\u001b[0m       0.6792        0.6120  0.7489\n",
      "     24        0.6190       0.6753        \u001b[35m0.6111\u001b[0m  0.7640\n",
      "     25        0.6191       0.6806        0.6114  0.7492\n",
      "     26        \u001b[36m0.6156\u001b[0m       0.6802        \u001b[35m0.6110\u001b[0m  0.7489\n",
      "     27        \u001b[36m0.6149\u001b[0m       0.6763        \u001b[35m0.6106\u001b[0m  0.7553\n",
      "     28        \u001b[36m0.6147\u001b[0m       0.6787        0.6150  0.7536\n",
      "     29        0.6162       0.6805        0.6109  0.7852\n",
      "     30        0.6154       0.6779        \u001b[35m0.6099\u001b[0m  0.7178\n",
      "     31        0.6149       \u001b[32m0.6816\u001b[0m        0.6101  0.7173\n",
      "     32        \u001b[36m0.6130\u001b[0m       \u001b[32m0.6832\u001b[0m        0.6107  0.6888\n",
      "     33        0.6139       0.6793        \u001b[35m0.6091\u001b[0m  0.6958\n",
      "     34        \u001b[36m0.6118\u001b[0m       0.6766        0.6091  0.6965\n",
      "     35        0.6126       0.6787        \u001b[35m0.6086\u001b[0m  0.7156\n",
      "     36        0.6120       \u001b[32m0.6833\u001b[0m        0.6093  0.6926\n",
      "     37        \u001b[36m0.6116\u001b[0m       0.6796        \u001b[35m0.6086\u001b[0m  0.7041\n",
      "     38        \u001b[36m0.6111\u001b[0m       0.6822        \u001b[35m0.6085\u001b[0m  0.6997\n",
      "     39        0.6121       0.6772        \u001b[35m0.6083\u001b[0m  0.6953\n",
      "     40        0.6119       0.6799        0.6089  0.6859\n"
     ]
    }
   ],
   "source": [
    "from skorch.helper import SliceDataset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "train_slice = SliceDataset(X_scale_torch)\n",
    "y_slice = SliceDataset(y_scale_torch)\n",
    "scores = cross_validate(model, X_scale_torch, y_scale_torch, scoring='accuracy', cv=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "24bc1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy for each fold: {'fit_time': array([32.64935684, 32.56045794, 33.10055208, 30.3942039 ]), 'score_time': array([0.13864493, 0.15048194, 0.19447684, 0.137707  ]), 'test_score': array([0.68130457, 0.67582606, 0.67762369, 0.69748331])}\n",
      "Average fit_time 32.17614269256592\n",
      "Average score_time 0.1553276777267456\n",
      "Average test_score 0.6830594076356789\n"
     ]
    }
   ],
   "source": [
    "import functools as f\n",
    "print('validation accuracy for each fold: {}'.format(scores))\n",
    "#print('avg validation accuracy: {:.3f}'.format(scores.mean()))\n",
    "#loop through the dictionary\n",
    "for key,value in scores.items(): \n",
    "   #use reduce to calculate the avg\n",
    "   print(f\"Average {key}\", f.reduce(lambda x, y: x + y, scores[key]) / len(scores[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ba1aaedb",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyModule"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\n",
    "    'lr': [0.01, 0.001],\n",
    "    'module__dropoutrate': [0.2, 0.5]\n",
    "\n",
    "}\n",
    "model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9de787f0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6192\u001b[0m       \u001b[32m0.6536\u001b[0m        \u001b[35m0.6164\u001b[0m  0.7819\n",
      "      2        \u001b[36m0.5903\u001b[0m       \u001b[32m0.6765\u001b[0m        \u001b[35m0.6009\u001b[0m  0.7738\n",
      "      3        \u001b[36m0.5852\u001b[0m       0.6759        \u001b[35m0.5906\u001b[0m  0.8328\n",
      "      4        \u001b[36m0.5790\u001b[0m       \u001b[32m0.6870\u001b[0m        \u001b[35m0.5871\u001b[0m  0.7694\n",
      "      5        \u001b[36m0.5756\u001b[0m       0.6670        0.5945  0.7599\n",
      "      6        \u001b[36m0.5733\u001b[0m       0.6799        0.5873  0.7709\n",
      "      7        \u001b[36m0.5706\u001b[0m       0.6779        0.5879  0.7603\n",
      "      8        \u001b[36m0.5687\u001b[0m       \u001b[32m0.6889\u001b[0m        \u001b[35m0.5838\u001b[0m  0.7594\n",
      "      9        \u001b[36m0.5649\u001b[0m       \u001b[32m0.6926\u001b[0m        \u001b[35m0.5802\u001b[0m  0.7511\n",
      "     10        \u001b[36m0.5629\u001b[0m       0.6880        0.5808  0.7578\n",
      "     11        0.5634       0.6899        0.5818  0.7553\n",
      "     12        \u001b[36m0.5617\u001b[0m       0.6923        0.5816  0.7562\n",
      "     13        \u001b[36m0.5601\u001b[0m       0.6867        0.5829  0.7679\n",
      "     14        \u001b[36m0.5577\u001b[0m       0.6832        0.5830  0.7449\n",
      "     15        \u001b[36m0.5560\u001b[0m       0.6907        \u001b[35m0.5794\u001b[0m  0.7540\n",
      "     16        0.5564       0.6922        \u001b[35m0.5773\u001b[0m  0.7526\n",
      "     17        \u001b[36m0.5553\u001b[0m       \u001b[32m0.6942\u001b[0m        \u001b[35m0.5764\u001b[0m  0.7570\n",
      "     18        \u001b[36m0.5553\u001b[0m       0.6924        \u001b[35m0.5762\u001b[0m  0.7473\n",
      "     19        \u001b[36m0.5524\u001b[0m       0.6904        0.5783  0.7534\n",
      "     20        \u001b[36m0.5519\u001b[0m       \u001b[32m0.6981\u001b[0m        \u001b[35m0.5736\u001b[0m  0.7792\n",
      "     21        \u001b[36m0.5513\u001b[0m       0.6922        0.5748  0.7866\n",
      "     22        \u001b[36m0.5497\u001b[0m       0.6916        0.5785  0.7597\n",
      "     23        0.5502       0.6867        0.5795  0.7584\n",
      "     24        \u001b[36m0.5484\u001b[0m       0.6835        0.5812  0.7562\n",
      "     25        \u001b[36m0.5473\u001b[0m       0.6847        0.5806  0.7536\n",
      "     26        \u001b[36m0.5471\u001b[0m       0.6951        \u001b[35m0.5735\u001b[0m  0.7638\n",
      "     27        \u001b[36m0.5452\u001b[0m       0.6795        0.5841  0.7492\n",
      "     28        0.5470       \u001b[32m0.6997\u001b[0m        \u001b[35m0.5705\u001b[0m  0.7528\n",
      "     29        \u001b[36m0.5444\u001b[0m       0.6879        0.5778  0.7600\n",
      "     30        \u001b[36m0.5435\u001b[0m       0.6902        0.5764  0.7554\n",
      "     31        \u001b[36m0.5420\u001b[0m       0.6953        0.5721  0.7582\n",
      "     32        \u001b[36m0.5414\u001b[0m       0.6990        0.5733  0.7745\n",
      "     33        0.5427       0.6950        0.5710  0.7585\n",
      "     34        0.5430       0.6760        0.5899  0.7603\n",
      "     35        \u001b[36m0.5391\u001b[0m       0.6912        0.5729  0.7587\n",
      "     36        0.5409       0.6927        0.5748  0.7599\n",
      "     37        0.5415       0.6971        \u001b[35m0.5694\u001b[0m  0.7615\n",
      "     38        \u001b[36m0.5387\u001b[0m       0.6863        0.5789  0.7548\n",
      "     39        0.5406       0.6883        0.5797  0.7492\n",
      "     40        0.5402       0.6976        0.5730  0.7543\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.2; total time=  30.8s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6258\u001b[0m       \u001b[32m0.6598\u001b[0m        \u001b[35m0.6221\u001b[0m  0.7502\n",
      "      2        \u001b[36m0.5981\u001b[0m       0.6598        \u001b[35m0.6139\u001b[0m  0.7545\n",
      "      3        \u001b[36m0.5898\u001b[0m       \u001b[32m0.6669\u001b[0m        \u001b[35m0.6076\u001b[0m  0.7680\n",
      "      4        \u001b[36m0.5849\u001b[0m       \u001b[32m0.6748\u001b[0m        \u001b[35m0.6060\u001b[0m  0.8768\n",
      "      5        \u001b[36m0.5785\u001b[0m       0.6740        \u001b[35m0.6026\u001b[0m  0.7876\n",
      "      6        \u001b[36m0.5751\u001b[0m       0.6715        0.6056  0.7524\n",
      "      7        \u001b[36m0.5731\u001b[0m       0.6732        0.6058  0.7543\n",
      "      8        \u001b[36m0.5714\u001b[0m       \u001b[32m0.6835\u001b[0m        \u001b[35m0.6015\u001b[0m  0.7667\n",
      "      9        \u001b[36m0.5701\u001b[0m       0.6755        0.6019  0.7826\n",
      "     10        \u001b[36m0.5684\u001b[0m       0.6793        \u001b[35m0.5996\u001b[0m  0.7592\n",
      "     11        \u001b[36m0.5655\u001b[0m       0.6770        \u001b[35m0.5982\u001b[0m  0.7496\n",
      "     12        \u001b[36m0.5639\u001b[0m       0.6820        \u001b[35m0.5977\u001b[0m  0.7574\n",
      "     13        \u001b[36m0.5627\u001b[0m       0.6742        0.6014  0.7962\n",
      "     14        \u001b[36m0.5605\u001b[0m       0.6829        \u001b[35m0.5975\u001b[0m  0.7472\n",
      "     15        0.5615       0.6767        \u001b[35m0.5962\u001b[0m  0.7388\n",
      "     16        \u001b[36m0.5601\u001b[0m       0.6833        0.5986  0.7460\n",
      "     17        \u001b[36m0.5578\u001b[0m       \u001b[32m0.6836\u001b[0m        0.5973  0.7403\n",
      "     18        0.5598       \u001b[32m0.6842\u001b[0m        \u001b[35m0.5962\u001b[0m  0.7425\n",
      "     19        0.5579       0.6819        0.5972  0.7660\n",
      "     20        \u001b[36m0.5551\u001b[0m       0.6805        0.5973  0.7485\n",
      "     21        \u001b[36m0.5551\u001b[0m       0.6802        0.6009  0.7573\n",
      "     22        0.5553       0.6779        0.5981  0.7599\n",
      "     23        \u001b[36m0.5531\u001b[0m       \u001b[32m0.6853\u001b[0m        \u001b[35m0.5916\u001b[0m  0.7571\n",
      "     24        0.5537       0.6732        0.5979  0.7541\n",
      "     25        0.5547       0.6753        0.5978  0.7614\n",
      "     26        \u001b[36m0.5508\u001b[0m       0.6849        0.5965  0.7556\n",
      "     27        0.5529       \u001b[32m0.6863\u001b[0m        0.5969  0.7541\n",
      "     28        \u001b[36m0.5499\u001b[0m       0.6756        0.5953  0.7587\n",
      "     29        0.5504       0.6748        0.5963  0.7789\n",
      "     30        0.5511       \u001b[32m0.6880\u001b[0m        0.5958  0.7702\n",
      "     31        0.5517       0.6862        0.5940  0.7557\n",
      "     32        \u001b[36m0.5497\u001b[0m       0.6853        0.5928  0.7497\n",
      "     33        \u001b[36m0.5482\u001b[0m       0.6797        0.5970  0.7533\n",
      "     34        \u001b[36m0.5476\u001b[0m       0.6819        0.5967  0.7550\n",
      "     35        \u001b[36m0.5472\u001b[0m       0.6819        0.5959  0.7517\n",
      "     36        0.5479       0.6816        0.5954  0.7615\n",
      "     37        \u001b[36m0.5460\u001b[0m       0.6695        0.6031  0.7539\n",
      "     38        0.5469       0.6846        0.5935  0.7546\n",
      "     39        0.5463       0.6723        0.6010  0.7582\n",
      "     40        0.5469       0.6837        0.5924  0.7601\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.2; total time=  30.8s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6254\u001b[0m       \u001b[32m0.6757\u001b[0m        \u001b[35m0.6117\u001b[0m  0.7631\n",
      "      2        \u001b[36m0.5934\u001b[0m       0.6718        \u001b[35m0.6071\u001b[0m  0.7558\n",
      "      3        \u001b[36m0.5866\u001b[0m       0.6738        \u001b[35m0.6049\u001b[0m  0.7514\n",
      "      4        \u001b[36m0.5818\u001b[0m       0.6748        \u001b[35m0.6036\u001b[0m  0.7457\n",
      "      5        \u001b[36m0.5799\u001b[0m       \u001b[32m0.6835\u001b[0m        \u001b[35m0.6022\u001b[0m  0.8746\n",
      "      6        \u001b[36m0.5787\u001b[0m       0.6767        \u001b[35m0.6022\u001b[0m  0.9506\n",
      "      7        \u001b[36m0.5762\u001b[0m       0.6766        \u001b[35m0.6011\u001b[0m  0.9422\n",
      "      8        \u001b[36m0.5739\u001b[0m       0.6735        0.6025  0.9419\n",
      "      9        \u001b[36m0.5733\u001b[0m       0.6749        \u001b[35m0.6000\u001b[0m  0.7980\n",
      "     10        \u001b[36m0.5732\u001b[0m       0.6746        0.6004  0.7875\n",
      "     11        \u001b[36m0.5695\u001b[0m       0.6796        0.6000  0.7909\n",
      "     12        0.5696       0.6805        0.6011  0.7816\n",
      "     13        \u001b[36m0.5669\u001b[0m       \u001b[32m0.6853\u001b[0m        0.6002  0.7936\n",
      "     14        \u001b[36m0.5651\u001b[0m       0.6766        \u001b[35m0.5972\u001b[0m  0.7910\n",
      "     15        0.5651       0.6812        0.6001  0.7912\n",
      "     16        \u001b[36m0.5648\u001b[0m       0.6680        0.5976  0.8696\n",
      "     17        \u001b[36m0.5616\u001b[0m       0.6786        \u001b[35m0.5941\u001b[0m  0.7909\n",
      "     18        0.5637       0.6796        0.5974  0.8131\n",
      "     19        0.5635       0.6787        0.5982  0.8024\n",
      "     20        \u001b[36m0.5616\u001b[0m       0.6750        0.5965  0.8006\n",
      "     21        \u001b[36m0.5615\u001b[0m       0.6790        0.5979  0.7962\n",
      "     22        \u001b[36m0.5605\u001b[0m       0.6685        0.5955  0.7885\n",
      "     23        0.5608       0.6806        0.5951  0.8134\n",
      "     24        \u001b[36m0.5598\u001b[0m       0.6822        0.5966  0.7875\n",
      "     25        \u001b[36m0.5581\u001b[0m       0.6732        0.5952  0.7948\n",
      "     26        \u001b[36m0.5573\u001b[0m       0.6790        0.5967  0.8010\n",
      "     27        0.5574       0.6762        0.5957  0.7960\n",
      "     28        \u001b[36m0.5556\u001b[0m       0.6812        \u001b[35m0.5937\u001b[0m  0.8154\n",
      "     29        0.5559       0.6836        0.5960  0.7986\n",
      "     30        0.5564       0.6842        0.5957  0.7905\n",
      "     31        0.5570       0.6773        0.5959  0.7997\n",
      "     32        \u001b[36m0.5547\u001b[0m       0.6765        \u001b[35m0.5936\u001b[0m  0.7936\n",
      "     33        0.5560       0.6785        \u001b[35m0.5924\u001b[0m  0.7890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     34        \u001b[36m0.5533\u001b[0m       0.6835        0.5962  0.7833\n",
      "     35        0.5547       0.6835        0.5929  0.7953\n",
      "     36        0.5556       0.6815        0.5928  0.7990\n",
      "     37        \u001b[36m0.5521\u001b[0m       0.6820        0.5951  0.7769\n",
      "     38        0.5530       0.6839        0.5938  0.7786\n",
      "     39        0.5549       0.6740        0.5953  0.7836\n",
      "     40        0.5545       0.6813        0.5934  0.8708\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.2; total time=  32.6s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6352\u001b[0m       \u001b[32m0.6599\u001b[0m        \u001b[35m0.6216\u001b[0m  0.8678\n",
      "      2        \u001b[36m0.6083\u001b[0m       \u001b[32m0.6616\u001b[0m        \u001b[35m0.6175\u001b[0m  0.8280\n",
      "      3        \u001b[36m0.6011\u001b[0m       \u001b[32m0.6668\u001b[0m        \u001b[35m0.6138\u001b[0m  0.8102\n",
      "      4        \u001b[36m0.5942\u001b[0m       \u001b[32m0.6716\u001b[0m        \u001b[35m0.6102\u001b[0m  0.9634\n",
      "      5        \u001b[36m0.5897\u001b[0m       0.6692        \u001b[35m0.6073\u001b[0m  1.0690\n",
      "      6        \u001b[36m0.5879\u001b[0m       0.6662        0.6109  0.9791\n",
      "      7        \u001b[36m0.5860\u001b[0m       0.6706        \u001b[35m0.6062\u001b[0m  0.9132\n",
      "      8        \u001b[36m0.5831\u001b[0m       0.6666        0.6065  0.7859\n",
      "      9        \u001b[36m0.5823\u001b[0m       0.6716        \u001b[35m0.6042\u001b[0m  0.7935\n",
      "     10        \u001b[36m0.5803\u001b[0m       \u001b[32m0.6756\u001b[0m        \u001b[35m0.6018\u001b[0m  0.7889\n",
      "     11        \u001b[36m0.5787\u001b[0m       0.6742        \u001b[35m0.6007\u001b[0m  0.7834\n",
      "     12        \u001b[36m0.5769\u001b[0m       \u001b[32m0.6760\u001b[0m        \u001b[35m0.6005\u001b[0m  0.7921\n",
      "     13        \u001b[36m0.5764\u001b[0m       0.6753        \u001b[35m0.5992\u001b[0m  0.7891\n",
      "     14        \u001b[36m0.5756\u001b[0m       \u001b[32m0.6772\u001b[0m        0.5999  0.7817\n",
      "     15        \u001b[36m0.5720\u001b[0m       \u001b[32m0.6792\u001b[0m        \u001b[35m0.5984\u001b[0m  0.7961\n",
      "     16        \u001b[36m0.5713\u001b[0m       0.6786        0.5986  0.7985\n",
      "     17        0.5714       0.6769        0.5990  0.8105\n",
      "     18        0.5722       0.6773        0.5991  0.7864\n",
      "     19        \u001b[36m0.5702\u001b[0m       \u001b[32m0.6806\u001b[0m        \u001b[35m0.5966\u001b[0m  0.7930\n",
      "     20        \u001b[36m0.5701\u001b[0m       0.6795        \u001b[35m0.5961\u001b[0m  0.7951\n",
      "     21        \u001b[36m0.5687\u001b[0m       0.6750        0.5994  0.8550\n",
      "     22        0.5690       0.6783        0.5971  1.1680\n",
      "     23        \u001b[36m0.5677\u001b[0m       \u001b[32m0.6809\u001b[0m        \u001b[35m0.5955\u001b[0m  1.1253\n",
      "     24        \u001b[36m0.5668\u001b[0m       0.6749        0.5964  0.8983\n",
      "     25        0.5675       0.6776        0.5979  0.8063\n",
      "     26        \u001b[36m0.5660\u001b[0m       \u001b[32m0.6810\u001b[0m        \u001b[35m0.5946\u001b[0m  0.7994\n",
      "     27        \u001b[36m0.5633\u001b[0m       \u001b[32m0.6825\u001b[0m        \u001b[35m0.5944\u001b[0m  0.8023\n",
      "     28        0.5637       0.6809        0.5958  0.7895\n",
      "     29        0.5634       0.6825        0.5946  0.7831\n",
      "     30        \u001b[36m0.5606\u001b[0m       0.6776        0.5968  0.7890\n",
      "     31        0.5619       0.6809        0.5953  0.7952\n",
      "     32        0.5619       \u001b[32m0.6829\u001b[0m        0.5956  0.7890\n",
      "     33        0.5627       0.6793        0.5952  0.8269\n",
      "     34        \u001b[36m0.5606\u001b[0m       0.6802        0.5954  0.7957\n",
      "     35        \u001b[36m0.5604\u001b[0m       0.6813        0.5951  0.8066\n",
      "     36        \u001b[36m0.5601\u001b[0m       0.6813        \u001b[35m0.5937\u001b[0m  0.7492\n",
      "     37        0.5601       \u001b[32m0.6843\u001b[0m        0.5940  0.7073\n",
      "     38        \u001b[36m0.5594\u001b[0m       0.6835        \u001b[35m0.5927\u001b[0m  0.6885\n",
      "     39        \u001b[36m0.5577\u001b[0m       0.6812        0.5954  0.7425\n",
      "     40        0.5580       0.6829        0.5949  1.1693\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.2; total time=  34.0s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6580\u001b[0m       \u001b[32m0.6496\u001b[0m        \u001b[35m0.6140\u001b[0m  0.8105\n",
      "      2        \u001b[36m0.6242\u001b[0m       \u001b[32m0.6583\u001b[0m        \u001b[35m0.6069\u001b[0m  0.7580\n",
      "      3        \u001b[36m0.6132\u001b[0m       \u001b[32m0.6655\u001b[0m        \u001b[35m0.6024\u001b[0m  0.7598\n",
      "      4        \u001b[36m0.6074\u001b[0m       0.6631        \u001b[35m0.5995\u001b[0m  0.7409\n",
      "      5        \u001b[36m0.6035\u001b[0m       \u001b[32m0.6693\u001b[0m        \u001b[35m0.5979\u001b[0m  0.7474\n",
      "      6        \u001b[36m0.6014\u001b[0m       0.6680        \u001b[35m0.5964\u001b[0m  0.7530\n",
      "      7        \u001b[36m0.5982\u001b[0m       0.6619        0.5995  0.7467\n",
      "      8        \u001b[36m0.5947\u001b[0m       \u001b[32m0.6769\u001b[0m        \u001b[35m0.5939\u001b[0m  0.7463\n",
      "      9        \u001b[36m0.5912\u001b[0m       0.6732        \u001b[35m0.5913\u001b[0m  0.7490\n",
      "     10        0.5937       0.6698        0.5969  0.7714\n",
      "     11        \u001b[36m0.5896\u001b[0m       0.6760        \u001b[35m0.5896\u001b[0m  0.7461\n",
      "     12        0.5901       0.6669        0.5980  0.7498\n",
      "     13        \u001b[36m0.5857\u001b[0m       0.6756        0.5925  0.7505\n",
      "     14        \u001b[36m0.5857\u001b[0m       0.6755        \u001b[35m0.5895\u001b[0m  0.7449\n",
      "     15        \u001b[36m0.5843\u001b[0m       0.6742        \u001b[35m0.5882\u001b[0m  0.7518\n",
      "     16        0.5874       0.6753        0.5928  0.7545\n",
      "     17        \u001b[36m0.5828\u001b[0m       0.6735        0.5914  0.7505\n",
      "     18        0.5840       \u001b[32m0.6790\u001b[0m        \u001b[35m0.5851\u001b[0m  0.7542\n",
      "     19        \u001b[36m0.5820\u001b[0m       0.6759        0.5898  0.7536\n",
      "     20        \u001b[36m0.5817\u001b[0m       \u001b[32m0.6795\u001b[0m        0.5860  0.7458\n",
      "     21        0.5821       0.6772        0.5869  0.7650\n",
      "     22        \u001b[36m0.5794\u001b[0m       \u001b[32m0.6815\u001b[0m        0.5870  0.7652\n",
      "     23        \u001b[36m0.5784\u001b[0m       0.6760        0.5907  0.7565\n",
      "     24        0.5796       \u001b[32m0.6817\u001b[0m        0.5869  0.7441\n",
      "     25        0.5796       0.6817        0.5861  0.7605\n",
      "     26        \u001b[36m0.5763\u001b[0m       0.6742        0.5892  0.7535\n",
      "     27        0.5771       0.6795        0.5867  0.7433\n",
      "     28        0.5779       0.6770        0.5878  0.7382\n",
      "     29        \u001b[36m0.5760\u001b[0m       0.6796        0.5855  0.7460\n",
      "     30        \u001b[36m0.5724\u001b[0m       0.6816        0.5853  0.7473\n",
      "     31        0.5766       0.6770        0.5870  0.7611\n",
      "     32        0.5740       0.6795        0.5883  0.7483\n",
      "     33        0.5738       0.6807        \u001b[35m0.5847\u001b[0m  0.7505\n",
      "     34        0.5735       \u001b[32m0.6832\u001b[0m        \u001b[35m0.5841\u001b[0m  0.7480\n",
      "     35        \u001b[36m0.5719\u001b[0m       0.6813        0.5858  0.7532\n",
      "     36        0.5741       0.6830        \u001b[35m0.5827\u001b[0m  0.7655\n",
      "     37        0.5739       \u001b[32m0.6856\u001b[0m        \u001b[35m0.5818\u001b[0m  0.7538\n",
      "     38        0.5734       \u001b[32m0.6897\u001b[0m        \u001b[35m0.5802\u001b[0m  0.7524\n",
      "     39        0.5743       0.6782        0.5855  0.7479\n",
      "     40        \u001b[36m0.5716\u001b[0m       0.6830        0.5832  0.7532\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.5; total time=  30.5s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6607\u001b[0m       \u001b[32m0.6605\u001b[0m        \u001b[35m0.6230\u001b[0m  0.7479\n",
      "      2        \u001b[36m0.6204\u001b[0m       \u001b[32m0.6795\u001b[0m        \u001b[35m0.6131\u001b[0m  0.7729\n",
      "      3        \u001b[36m0.6135\u001b[0m       0.6765        \u001b[35m0.6110\u001b[0m  0.7489\n",
      "      4        \u001b[36m0.6059\u001b[0m       0.6740        0.6112  0.7544\n",
      "      5        \u001b[36m0.6041\u001b[0m       0.6709        \u001b[35m0.6086\u001b[0m  0.7446\n",
      "      6        \u001b[36m0.5993\u001b[0m       \u001b[32m0.6805\u001b[0m        \u001b[35m0.6071\u001b[0m  0.7452\n",
      "      7        \u001b[36m0.5978\u001b[0m       0.6730        \u001b[35m0.6047\u001b[0m  0.7402\n",
      "      8        \u001b[36m0.5943\u001b[0m       0.6723        0.6060  0.7463\n",
      "      9        \u001b[36m0.5918\u001b[0m       0.6706        0.6051  0.7439\n",
      "     10        0.5937       \u001b[32m0.6812\u001b[0m        \u001b[35m0.6017\u001b[0m  0.7647\n",
      "     11        \u001b[36m0.5907\u001b[0m       0.6756        0.6053  0.7490\n",
      "     12        0.5917       0.6799        0.6023  0.7493\n",
      "     13        \u001b[36m0.5870\u001b[0m       0.6755        \u001b[35m0.6014\u001b[0m  0.7486\n",
      "     14        0.5880       0.6809        0.6018  0.7461\n",
      "     15        0.5888       0.6725        0.6036  0.7476\n",
      "     16        \u001b[36m0.5866\u001b[0m       0.6777        0.6021  0.8773\n",
      "     17        \u001b[36m0.5856\u001b[0m       \u001b[32m0.6847\u001b[0m        \u001b[35m0.5998\u001b[0m  0.8261\n",
      "     18        0.5864       0.6755        0.6011  0.7735\n",
      "     19        \u001b[36m0.5844\u001b[0m       0.6745        \u001b[35m0.5997\u001b[0m  0.7891\n",
      "     20        \u001b[36m0.5834\u001b[0m       0.6766        \u001b[35m0.5991\u001b[0m  0.7589\n",
      "     21        \u001b[36m0.5833\u001b[0m       0.6700        0.6022  0.7626\n",
      "     22        \u001b[36m0.5830\u001b[0m       0.6746        \u001b[35m0.5986\u001b[0m  0.8117\n",
      "     23        \u001b[36m0.5818\u001b[0m       0.6796        0.6003  0.7494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24        0.5829       0.6805        \u001b[35m0.5982\u001b[0m  0.7940\n",
      "     25        \u001b[36m0.5803\u001b[0m       0.6826        0.5991  0.7708\n",
      "     26        \u001b[36m0.5801\u001b[0m       0.6779        0.5998  0.7484\n",
      "     27        \u001b[36m0.5800\u001b[0m       0.6797        \u001b[35m0.5966\u001b[0m  0.7650\n",
      "     28        0.5802       0.6812        0.5972  0.7494\n",
      "     29        0.5800       0.6809        0.5972  0.7674\n",
      "     30        \u001b[36m0.5776\u001b[0m       0.6678        0.6026  0.7798\n",
      "     31        \u001b[36m0.5769\u001b[0m       0.6757        0.5981  0.7464\n",
      "     32        0.5784       0.6790        0.5969  0.7557\n",
      "     33        0.5798       0.6796        \u001b[35m0.5965\u001b[0m  0.7584\n",
      "     34        \u001b[36m0.5760\u001b[0m       0.6833        0.5966  0.7629\n",
      "     35        \u001b[36m0.5749\u001b[0m       0.6713        0.5993  0.7537\n",
      "     36        0.5757       0.6759        0.5979  0.7833\n",
      "     37        0.5759       0.6748        0.5969  0.7848\n",
      "     38        \u001b[36m0.5744\u001b[0m       0.6803        0.5972  0.7713\n",
      "     39        \u001b[36m0.5740\u001b[0m       0.6755        0.5976  0.7945\n",
      "     40        0.5764       0.6783        0.5982  0.7928\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.5; total time=  31.0s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6614\u001b[0m       \u001b[32m0.6641\u001b[0m        \u001b[35m0.6235\u001b[0m  0.7851\n",
      "      2        \u001b[36m0.6256\u001b[0m       \u001b[32m0.6712\u001b[0m        \u001b[35m0.6163\u001b[0m  0.7500\n",
      "      3        \u001b[36m0.6144\u001b[0m       \u001b[32m0.6748\u001b[0m        \u001b[35m0.6119\u001b[0m  0.7704\n",
      "      4        \u001b[36m0.6088\u001b[0m       0.6685        0.6134  0.7659\n",
      "      5        \u001b[36m0.6052\u001b[0m       \u001b[32m0.6790\u001b[0m        \u001b[35m0.6086\u001b[0m  0.7468\n",
      "      6        \u001b[36m0.6006\u001b[0m       0.6705        0.6108  0.7615\n",
      "      7        0.6007       0.6786        \u001b[35m0.6068\u001b[0m  0.7507\n",
      "      8        \u001b[36m0.5994\u001b[0m       0.6726        \u001b[35m0.6068\u001b[0m  0.7741\n",
      "      9        \u001b[36m0.5971\u001b[0m       \u001b[32m0.6807\u001b[0m        \u001b[35m0.6050\u001b[0m  0.7473\n",
      "     10        \u001b[36m0.5968\u001b[0m       0.6805        \u001b[35m0.6048\u001b[0m  0.7551\n",
      "     11        \u001b[36m0.5966\u001b[0m       0.6706        0.6053  0.7471\n",
      "     12        \u001b[36m0.5938\u001b[0m       \u001b[32m0.6819\u001b[0m        \u001b[35m0.6037\u001b[0m  0.7520\n",
      "     13        \u001b[36m0.5924\u001b[0m       \u001b[32m0.6835\u001b[0m        \u001b[35m0.6013\u001b[0m  0.7542\n",
      "     14        \u001b[36m0.5908\u001b[0m       \u001b[32m0.6839\u001b[0m        0.6013  0.7547\n",
      "     15        \u001b[36m0.5906\u001b[0m       0.6752        0.6016  0.7534\n",
      "     16        \u001b[36m0.5906\u001b[0m       0.6810        \u001b[35m0.6011\u001b[0m  0.7699\n",
      "     17        \u001b[36m0.5860\u001b[0m       0.6763        \u001b[35m0.5995\u001b[0m  0.7568\n",
      "     18        0.5876       0.6755        0.6011  0.7554\n",
      "     19        0.5864       0.6796        \u001b[35m0.5975\u001b[0m  0.7707\n",
      "     20        0.5862       0.6690        0.6015  0.7602\n",
      "     21        0.5866       0.6713        0.6000  0.7625\n",
      "     22        0.5873       0.6809        0.5993  0.7583\n",
      "     23        \u001b[36m0.5835\u001b[0m       0.6775        0.5996  0.7548\n",
      "     24        0.5838       0.6763        0.5985  0.7523\n",
      "     25        0.5841       0.6689        0.5997  0.7423\n",
      "     26        0.5835       0.6686        0.6008  0.7513\n",
      "     27        \u001b[36m0.5823\u001b[0m       0.6802        \u001b[35m0.5964\u001b[0m  0.7514\n",
      "     28        \u001b[36m0.5822\u001b[0m       \u001b[32m0.6866\u001b[0m        0.5969  0.7506\n",
      "     29        \u001b[36m0.5801\u001b[0m       0.6785        \u001b[35m0.5960\u001b[0m  0.7607\n",
      "     30        0.5825       0.6770        0.5993  0.7543\n",
      "     31        0.5815       0.6720        0.6003  0.7525\n",
      "     32        \u001b[36m0.5793\u001b[0m       0.6716        0.6003  0.7500\n",
      "     33        0.5806       0.6816        0.5981  0.7548\n",
      "     34        \u001b[36m0.5785\u001b[0m       0.6839        0.5969  0.7545\n",
      "     35        0.5792       0.6790        0.5977  0.7553\n",
      "     36        0.5795       0.6733        0.5969  0.7570\n",
      "     37        \u001b[36m0.5783\u001b[0m       0.6817        \u001b[35m0.5959\u001b[0m  0.7743\n",
      "     38        0.5797       0.6847        \u001b[35m0.5954\u001b[0m  0.7557\n",
      "     39        0.5788       0.6787        0.5974  0.7657\n",
      "     40        0.5792       0.6736        0.5977  0.7700\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.5; total time=  30.6s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6606\u001b[0m       \u001b[32m0.6719\u001b[0m        \u001b[35m0.6186\u001b[0m  0.7448\n",
      "      2        \u001b[36m0.6264\u001b[0m       \u001b[32m0.6806\u001b[0m        \u001b[35m0.6148\u001b[0m  0.7604\n",
      "      3        \u001b[36m0.6198\u001b[0m       \u001b[32m0.6826\u001b[0m        \u001b[35m0.6118\u001b[0m  0.7467\n",
      "      4        \u001b[36m0.6129\u001b[0m       \u001b[32m0.6832\u001b[0m        \u001b[35m0.6110\u001b[0m  0.7523\n",
      "      5        \u001b[36m0.6122\u001b[0m       0.6745        \u001b[35m0.6099\u001b[0m  0.7471\n",
      "      6        \u001b[36m0.6087\u001b[0m       0.6809        \u001b[35m0.6062\u001b[0m  0.7495\n",
      "      7        \u001b[36m0.6041\u001b[0m       0.6689        0.6088  0.7577\n",
      "      8        \u001b[36m0.6033\u001b[0m       0.6735        0.6070  0.7576\n",
      "      9        \u001b[36m0.6022\u001b[0m       0.6662        0.6078  0.7880\n",
      "     10        \u001b[36m0.6000\u001b[0m       0.6710        \u001b[35m0.6042\u001b[0m  0.7482\n",
      "     11        \u001b[36m0.5997\u001b[0m       0.6699        \u001b[35m0.6039\u001b[0m  0.7482\n",
      "     12        \u001b[36m0.5984\u001b[0m       0.6796        \u001b[35m0.6038\u001b[0m  0.7583\n",
      "     13        0.5986       0.6733        0.6039  0.7512\n",
      "     14        \u001b[36m0.5964\u001b[0m       0.6796        \u001b[35m0.6025\u001b[0m  0.7462\n",
      "     15        \u001b[36m0.5947\u001b[0m       0.6762        0.6040  0.7652\n",
      "     16        0.5952       0.6785        \u001b[35m0.6016\u001b[0m  0.7493\n",
      "     17        \u001b[36m0.5922\u001b[0m       0.6825        0.6017  0.7588\n",
      "     18        0.5924       0.6819        \u001b[35m0.5999\u001b[0m  0.7489\n",
      "     19        \u001b[36m0.5920\u001b[0m       0.6806        0.6005  0.7930\n",
      "     20        0.5940       \u001b[32m0.6833\u001b[0m        \u001b[35m0.5993\u001b[0m  0.7593\n",
      "     21        \u001b[36m0.5908\u001b[0m       0.6736        0.6009  0.7668\n",
      "     22        0.5920       0.6753        0.6003  0.7573\n",
      "     23        \u001b[36m0.5891\u001b[0m       0.6827        0.5996  0.7532\n",
      "     24        0.5892       0.6803        \u001b[35m0.5991\u001b[0m  0.7473\n",
      "     25        0.5902       0.6807        0.5998  0.7455\n",
      "     26        0.5894       0.6803        0.5993  0.7478\n",
      "     27        \u001b[36m0.5883\u001b[0m       0.6805        0.5999  0.7493\n",
      "     28        \u001b[36m0.5871\u001b[0m       0.6815        \u001b[35m0.5977\u001b[0m  0.7530\n",
      "     29        0.5878       0.6780        0.5979  0.7494\n",
      "     30        \u001b[36m0.5869\u001b[0m       0.6789        0.5992  0.7553\n",
      "     31        0.5872       0.6767        0.6001  0.7496\n",
      "     32        \u001b[36m0.5850\u001b[0m       0.6813        0.5997  0.7598\n",
      "     33        0.5859       0.6767        0.5985  0.7555\n",
      "     34        0.5874       0.6810        \u001b[35m0.5971\u001b[0m  0.7628\n",
      "     35        \u001b[36m0.5826\u001b[0m       0.6796        0.5979  0.7503\n",
      "     36        0.5843       0.6787        \u001b[35m0.5967\u001b[0m  0.7564\n",
      "     37        0.5834       0.6820        \u001b[35m0.5960\u001b[0m  0.7522\n",
      "     38        0.5844       0.6807        \u001b[35m0.5953\u001b[0m  0.7701\n",
      "     39        0.5866       0.6802        0.5958  0.7580\n",
      "     40        0.5833       0.6793        0.5968  0.7461\n",
      "[CV] END ...................lr=0.01, module__dropoutrate=0.5; total time=  30.5s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6827\u001b[0m       \u001b[32m0.6337\u001b[0m        \u001b[35m0.6389\u001b[0m  0.7633\n",
      "      2        \u001b[36m0.6398\u001b[0m       \u001b[32m0.6462\u001b[0m        \u001b[35m0.6222\u001b[0m  0.7482\n",
      "      3        \u001b[36m0.6259\u001b[0m       \u001b[32m0.6491\u001b[0m        \u001b[35m0.6186\u001b[0m  0.7588\n",
      "      4        \u001b[36m0.6191\u001b[0m       \u001b[32m0.6603\u001b[0m        \u001b[35m0.6105\u001b[0m  0.7780\n",
      "      5        \u001b[36m0.6125\u001b[0m       0.6569        \u001b[35m0.6083\u001b[0m  0.7480\n",
      "      6        \u001b[36m0.6071\u001b[0m       \u001b[32m0.6623\u001b[0m        \u001b[35m0.6061\u001b[0m  0.8176\n",
      "      7        \u001b[36m0.6039\u001b[0m       0.6591        \u001b[35m0.6061\u001b[0m  0.7540\n",
      "      8        \u001b[36m0.6020\u001b[0m       \u001b[32m0.6676\u001b[0m        \u001b[35m0.6038\u001b[0m  0.7487\n",
      "      9        \u001b[36m0.6001\u001b[0m       \u001b[32m0.6702\u001b[0m        \u001b[35m0.6023\u001b[0m  0.7450\n",
      "     10        \u001b[36m0.5973\u001b[0m       0.6692        \u001b[35m0.6022\u001b[0m  0.7439\n",
      "     11        0.5983       \u001b[32m0.6759\u001b[0m        \u001b[35m0.5995\u001b[0m  0.7385\n",
      "     12        \u001b[36m0.5951\u001b[0m       0.6729        0.6007  0.7493\n",
      "     13        \u001b[36m0.5946\u001b[0m       0.6720        0.6001  0.7449\n",
      "     14        \u001b[36m0.5928\u001b[0m       0.6713        0.5999  0.7540\n",
      "     15        0.5930       0.6702        \u001b[35m0.5990\u001b[0m  0.7598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16        \u001b[36m0.5919\u001b[0m       \u001b[32m0.6789\u001b[0m        \u001b[35m0.5969\u001b[0m  0.7744\n",
      "     17        0.5921       0.6750        0.5979  0.7535\n",
      "     18        \u001b[36m0.5891\u001b[0m       0.6782        0.5972  0.7503\n",
      "     19        \u001b[36m0.5888\u001b[0m       \u001b[32m0.6812\u001b[0m        \u001b[35m0.5962\u001b[0m  0.7537\n",
      "     20        0.5890       0.6800        \u001b[35m0.5949\u001b[0m  0.7462\n",
      "     21        \u001b[36m0.5873\u001b[0m       0.6769        0.5972  0.7630\n",
      "     22        0.5875       \u001b[32m0.6816\u001b[0m        0.5952  0.7528\n",
      "     23        \u001b[36m0.5864\u001b[0m       0.6809        \u001b[35m0.5948\u001b[0m  0.7519\n",
      "     24        \u001b[36m0.5853\u001b[0m       0.6816        \u001b[35m0.5940\u001b[0m  0.7558\n",
      "     25        0.5861       \u001b[32m0.6846\u001b[0m        0.5951  0.7514\n",
      "     26        \u001b[36m0.5832\u001b[0m       0.6796        0.5947  0.7530\n",
      "     27        \u001b[36m0.5820\u001b[0m       0.6840        \u001b[35m0.5918\u001b[0m  0.7507\n",
      "     28        0.5832       0.6806        0.5935  0.7534\n",
      "     29        0.5851       0.6806        0.5933  0.7550\n",
      "     30        \u001b[36m0.5815\u001b[0m       0.6827        \u001b[35m0.5917\u001b[0m  0.7549\n",
      "     31        0.5817       0.6817        0.5921  0.7472\n",
      "     32        0.5824       \u001b[32m0.6850\u001b[0m        \u001b[35m0.5908\u001b[0m  0.7486\n",
      "     33        \u001b[36m0.5800\u001b[0m       0.6840        0.5925  0.7522\n",
      "     34        0.5815       \u001b[32m0.6864\u001b[0m        \u001b[35m0.5906\u001b[0m  0.7671\n",
      "     35        0.5803       0.6842        0.5907  0.7531\n",
      "     36        \u001b[36m0.5795\u001b[0m       \u001b[32m0.6869\u001b[0m        0.5910  0.7703\n",
      "     37        \u001b[36m0.5790\u001b[0m       0.6787        0.5936  0.7719\n",
      "     38        \u001b[36m0.5785\u001b[0m       0.6853        \u001b[35m0.5899\u001b[0m  0.7550\n",
      "     39        \u001b[36m0.5777\u001b[0m       0.6857        \u001b[35m0.5896\u001b[0m  0.7547\n",
      "     40        0.5784       0.6862        0.5900  0.7534\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.2; total time=  30.6s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6800\u001b[0m       \u001b[32m0.6258\u001b[0m        \u001b[35m0.6513\u001b[0m  0.7512\n",
      "      2        \u001b[36m0.6424\u001b[0m       \u001b[32m0.6421\u001b[0m        \u001b[35m0.6317\u001b[0m  0.7521\n",
      "      3        \u001b[36m0.6289\u001b[0m       \u001b[32m0.6586\u001b[0m        \u001b[35m0.6282\u001b[0m  0.7453\n",
      "      4        \u001b[36m0.6197\u001b[0m       \u001b[32m0.6625\u001b[0m        \u001b[35m0.6206\u001b[0m  0.7504\n",
      "      5        \u001b[36m0.6127\u001b[0m       \u001b[32m0.6752\u001b[0m        \u001b[35m0.6182\u001b[0m  0.7450\n",
      "      6        \u001b[36m0.6119\u001b[0m       0.6729        \u001b[35m0.6159\u001b[0m  0.7384\n",
      "      7        \u001b[36m0.6061\u001b[0m       \u001b[32m0.6770\u001b[0m        \u001b[35m0.6144\u001b[0m  0.7668\n",
      "      8        \u001b[36m0.6040\u001b[0m       0.6746        0.6155  0.7778\n",
      "      9        \u001b[36m0.6012\u001b[0m       0.6765        \u001b[35m0.6140\u001b[0m  0.7545\n",
      "     10        \u001b[36m0.6008\u001b[0m       \u001b[32m0.6799\u001b[0m        \u001b[35m0.6111\u001b[0m  0.7522\n",
      "     11        \u001b[36m0.5997\u001b[0m       0.6759        0.6131  0.7512\n",
      "     12        \u001b[36m0.5957\u001b[0m       0.6766        \u001b[35m0.6099\u001b[0m  0.7550\n",
      "     13        \u001b[36m0.5950\u001b[0m       0.6792        0.6099  0.7487\n",
      "     14        0.5954       0.6746        0.6103  0.7455\n",
      "     15        \u001b[36m0.5925\u001b[0m       0.6766        0.6104  0.7422\n",
      "     16        0.5930       0.6748        \u001b[35m0.6092\u001b[0m  0.7685\n",
      "     17        \u001b[36m0.5905\u001b[0m       \u001b[32m0.6813\u001b[0m        0.6094  0.7591\n",
      "     18        \u001b[36m0.5902\u001b[0m       0.6772        0.6097  0.7542\n",
      "     19        0.5904       0.6750        \u001b[35m0.6091\u001b[0m  0.7449\n",
      "     20        \u001b[36m0.5880\u001b[0m       0.6780        0.6102  0.7562\n",
      "     21        \u001b[36m0.5878\u001b[0m       0.6780        0.6097  0.7522\n",
      "     22        \u001b[36m0.5861\u001b[0m       0.6729        \u001b[35m0.6085\u001b[0m  0.7505\n",
      "     23        \u001b[36m0.5860\u001b[0m       0.6746        0.6096  0.7538\n",
      "     24        \u001b[36m0.5854\u001b[0m       0.6763        \u001b[35m0.6076\u001b[0m  0.7499\n",
      "     25        \u001b[36m0.5851\u001b[0m       0.6785        0.6088  0.7514\n",
      "     26        0.5861       0.6802        0.6085  0.7448\n",
      "     27        0.5861       0.6767        0.6088  0.7584\n",
      "     28        0.5854       0.6733        0.6084  0.7450\n",
      "     29        \u001b[36m0.5837\u001b[0m       0.6722        \u001b[35m0.6074\u001b[0m  0.7700\n",
      "     30        \u001b[36m0.5815\u001b[0m       0.6785        0.6103  0.7514\n",
      "     31        0.5816       0.6728        0.6081  0.7686\n",
      "     32        \u001b[36m0.5805\u001b[0m       0.6753        \u001b[35m0.6071\u001b[0m  0.7526\n",
      "     33        \u001b[36m0.5792\u001b[0m       0.6760        \u001b[35m0.6048\u001b[0m  0.7553\n",
      "     34        0.5800       0.6750        0.6061  0.7680\n",
      "     35        0.5795       0.6746        0.6073  0.7514\n",
      "     36        0.5816       0.6752        0.6067  0.7529\n",
      "     37        0.5793       0.6783        \u001b[35m0.6045\u001b[0m  0.7637\n",
      "     38        \u001b[36m0.5788\u001b[0m       0.6769        0.6072  0.7564\n",
      "     39        \u001b[36m0.5775\u001b[0m       0.6775        0.6070  0.7499\n",
      "     40        \u001b[36m0.5766\u001b[0m       0.6750        \u001b[35m0.6041\u001b[0m  0.7565\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.2; total time=  30.5s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6675\u001b[0m       \u001b[32m0.6295\u001b[0m        \u001b[35m0.6430\u001b[0m  0.7498\n",
      "      2        \u001b[36m0.6338\u001b[0m       \u001b[32m0.6486\u001b[0m        \u001b[35m0.6307\u001b[0m  0.8192\n",
      "      3        \u001b[36m0.6199\u001b[0m       \u001b[32m0.6598\u001b[0m        \u001b[35m0.6218\u001b[0m  0.9422\n",
      "      4        \u001b[36m0.6135\u001b[0m       \u001b[32m0.6659\u001b[0m        \u001b[35m0.6193\u001b[0m  0.8186\n",
      "      5        \u001b[36m0.6091\u001b[0m       \u001b[32m0.6736\u001b[0m        \u001b[35m0.6176\u001b[0m  0.7611\n",
      "      6        \u001b[36m0.6047\u001b[0m       0.6699        \u001b[35m0.6149\u001b[0m  0.7793\n",
      "      7        \u001b[36m0.6015\u001b[0m       0.6720        \u001b[35m0.6129\u001b[0m  0.7563\n",
      "      8        \u001b[36m0.5983\u001b[0m       0.6700        \u001b[35m0.6110\u001b[0m  0.7557\n",
      "      9        0.5983       0.6722        0.6127  0.7512\n",
      "     10        \u001b[36m0.5961\u001b[0m       \u001b[32m0.6748\u001b[0m        0.6145  0.7494\n",
      "     11        \u001b[36m0.5929\u001b[0m       0.6718        0.6135  0.7507\n",
      "     12        \u001b[36m0.5926\u001b[0m       0.6723        \u001b[35m0.6101\u001b[0m  0.7483\n",
      "     13        \u001b[36m0.5923\u001b[0m       0.6732        0.6117  0.7572\n",
      "     14        \u001b[36m0.5914\u001b[0m       0.6708        \u001b[35m0.6088\u001b[0m  0.7524\n",
      "     15        0.5921       0.6738        0.6096  0.7506\n",
      "     16        0.5917       0.6679        0.6098  0.7907\n",
      "     17        \u001b[36m0.5899\u001b[0m       0.6696        0.6100  0.7586\n",
      "     18        \u001b[36m0.5868\u001b[0m       0.6702        \u001b[35m0.6084\u001b[0m  0.7470\n",
      "     19        \u001b[36m0.5864\u001b[0m       0.6683        0.6086  0.7631\n",
      "     20        \u001b[36m0.5856\u001b[0m       0.6705        \u001b[35m0.6080\u001b[0m  0.7499\n",
      "     21        \u001b[36m0.5851\u001b[0m       0.6718        \u001b[35m0.6079\u001b[0m  0.7544\n",
      "     22        0.5854       0.6716        \u001b[35m0.6074\u001b[0m  0.7488\n",
      "     23        0.5852       0.6705        \u001b[35m0.6071\u001b[0m  0.7594\n",
      "     24        \u001b[36m0.5847\u001b[0m       0.6703        0.6076  0.7780\n",
      "     25        \u001b[36m0.5835\u001b[0m       0.6709        \u001b[35m0.6071\u001b[0m  0.7503\n",
      "     26        \u001b[36m0.5829\u001b[0m       0.6708        \u001b[35m0.6048\u001b[0m  0.7553\n",
      "     27        \u001b[36m0.5825\u001b[0m       0.6695        0.6052  0.7549\n",
      "     28        0.5826       0.6698        0.6063  0.7524\n",
      "     29        \u001b[36m0.5814\u001b[0m       \u001b[32m0.6759\u001b[0m        0.6064  0.7532\n",
      "     30        \u001b[36m0.5804\u001b[0m       0.6730        \u001b[35m0.6043\u001b[0m  0.7523\n",
      "     31        0.5810       0.6709        0.6072  0.7594\n",
      "     32        \u001b[36m0.5788\u001b[0m       0.6703        0.6049  0.7478\n",
      "     33        0.5795       0.6713        0.6061  0.7563\n",
      "     34        0.5793       0.6710        0.6062  0.7511\n",
      "     35        \u001b[36m0.5785\u001b[0m       0.6732        \u001b[35m0.6036\u001b[0m  0.7585\n",
      "     36        \u001b[36m0.5783\u001b[0m       0.6710        0.6038  0.7669\n",
      "     37        0.5787       0.6729        \u001b[35m0.6029\u001b[0m  0.7517\n",
      "     38        \u001b[36m0.5776\u001b[0m       0.6716        0.6042  0.7489\n",
      "     39        0.5787       0.6715        0.6038  0.7495\n",
      "     40        0.5787       0.6732        0.6046  0.7525\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.2; total time=  30.9s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7096\u001b[0m       \u001b[32m0.5929\u001b[0m        \u001b[35m0.6702\u001b[0m  0.7540\n",
      "      2        \u001b[36m0.6675\u001b[0m       \u001b[32m0.6238\u001b[0m        \u001b[35m0.6482\u001b[0m  0.7634\n",
      "      3        \u001b[36m0.6465\u001b[0m       \u001b[32m0.6417\u001b[0m        \u001b[35m0.6403\u001b[0m  0.7481\n",
      "      4        \u001b[36m0.6331\u001b[0m       \u001b[32m0.6505\u001b[0m        \u001b[35m0.6276\u001b[0m  0.7475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6261\u001b[0m       \u001b[32m0.6588\u001b[0m        \u001b[35m0.6240\u001b[0m  0.7691\n",
      "      6        \u001b[36m0.6200\u001b[0m       0.6558        \u001b[35m0.6218\u001b[0m  0.7820\n",
      "      7        \u001b[36m0.6150\u001b[0m       \u001b[32m0.6693\u001b[0m        \u001b[35m0.6175\u001b[0m  0.7582\n",
      "      8        \u001b[36m0.6114\u001b[0m       0.6665        \u001b[35m0.6167\u001b[0m  0.7510\n",
      "      9        \u001b[36m0.6088\u001b[0m       \u001b[32m0.6760\u001b[0m        \u001b[35m0.6167\u001b[0m  0.7561\n",
      "     10        \u001b[36m0.6070\u001b[0m       0.6685        \u001b[35m0.6150\u001b[0m  0.7577\n",
      "     11        \u001b[36m0.6017\u001b[0m       0.6753        \u001b[35m0.6143\u001b[0m  0.7569\n",
      "     12        0.6024       0.6730        \u001b[35m0.6128\u001b[0m  0.8639\n",
      "     13        0.6022       \u001b[32m0.6769\u001b[0m        \u001b[35m0.6116\u001b[0m  0.8370\n",
      "     14        \u001b[36m0.6007\u001b[0m       \u001b[32m0.6770\u001b[0m        0.6134  0.8612\n",
      "     15        \u001b[36m0.6006\u001b[0m       \u001b[32m0.6776\u001b[0m        \u001b[35m0.6115\u001b[0m  0.7700\n",
      "     16        \u001b[36m0.5968\u001b[0m       0.6767        0.6130  0.7477\n",
      "     17        \u001b[36m0.5955\u001b[0m       0.6762        0.6138  0.7454\n",
      "     18        0.5988       \u001b[32m0.6783\u001b[0m        \u001b[35m0.6099\u001b[0m  0.7540\n",
      "     19        \u001b[36m0.5937\u001b[0m       0.6757        0.6127  0.7562\n",
      "     20        0.5946       0.6752        0.6116  0.7522\n",
      "     21        0.5948       0.6777        0.6100  0.7503\n",
      "     22        \u001b[36m0.5928\u001b[0m       0.6779        \u001b[35m0.6098\u001b[0m  0.7480\n",
      "     23        0.5944       0.6738        \u001b[35m0.6085\u001b[0m  0.7454\n",
      "     24        0.5935       0.6776        0.6100  0.7571\n",
      "     25        \u001b[36m0.5900\u001b[0m       0.6708        0.6100  0.7625\n",
      "     26        0.5908       0.6749        0.6096  0.7483\n",
      "     27        0.5903       0.6777        0.6097  0.7561\n",
      "     28        0.5904       0.6766        0.6093  0.7494\n",
      "     29        \u001b[36m0.5885\u001b[0m       0.6757        0.6100  0.7506\n",
      "     30        \u001b[36m0.5871\u001b[0m       0.6770        \u001b[35m0.6077\u001b[0m  0.7610\n",
      "     31        0.5878       0.6746        0.6092  0.7523\n",
      "     32        0.5884       0.6769        0.6089  0.7738\n",
      "     33        \u001b[36m0.5868\u001b[0m       0.6757        \u001b[35m0.6071\u001b[0m  0.8606\n",
      "     34        \u001b[36m0.5861\u001b[0m       \u001b[32m0.6785\u001b[0m        0.6095  0.8732\n",
      "     35        0.5875       0.6782        \u001b[35m0.6063\u001b[0m  0.7138\n",
      "     36        \u001b[36m0.5856\u001b[0m       0.6738        0.6068  0.6859\n",
      "     37        0.5879       0.6718        0.6078  0.7378\n",
      "     38        0.5858       0.6745        \u001b[35m0.6052\u001b[0m  0.7618\n",
      "     39        0.5860       \u001b[32m0.6817\u001b[0m        0.6055  0.7332\n",
      "     40        \u001b[36m0.5855\u001b[0m       0.6760        0.6076  0.7561\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.2; total time=  30.9s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7468\u001b[0m       \u001b[32m0.6138\u001b[0m        \u001b[35m0.6674\u001b[0m  0.7609\n",
      "      2        \u001b[36m0.6871\u001b[0m       \u001b[32m0.6458\u001b[0m        \u001b[35m0.6429\u001b[0m  0.7695\n",
      "      3        \u001b[36m0.6666\u001b[0m       0.6447        \u001b[35m0.6303\u001b[0m  0.7457\n",
      "      4        \u001b[36m0.6570\u001b[0m       \u001b[32m0.6464\u001b[0m        \u001b[35m0.6254\u001b[0m  0.7651\n",
      "      5        \u001b[36m0.6475\u001b[0m       \u001b[32m0.6466\u001b[0m        \u001b[35m0.6214\u001b[0m  0.7591\n",
      "      6        \u001b[36m0.6422\u001b[0m       \u001b[32m0.6609\u001b[0m        \u001b[35m0.6193\u001b[0m  0.7619\n",
      "      7        \u001b[36m0.6369\u001b[0m       0.6538        \u001b[35m0.6161\u001b[0m  0.7455\n",
      "      8        \u001b[36m0.6335\u001b[0m       \u001b[32m0.6631\u001b[0m        \u001b[35m0.6133\u001b[0m  0.7650\n",
      "      9        \u001b[36m0.6300\u001b[0m       0.6625        \u001b[35m0.6114\u001b[0m  0.7420\n",
      "     10        \u001b[36m0.6286\u001b[0m       0.6611        \u001b[35m0.6113\u001b[0m  0.7499\n",
      "     11        \u001b[36m0.6250\u001b[0m       \u001b[32m0.6683\u001b[0m        \u001b[35m0.6103\u001b[0m  0.7496\n",
      "     12        \u001b[36m0.6219\u001b[0m       \u001b[32m0.6709\u001b[0m        0.6118  0.7586\n",
      "     13        0.6233       0.6676        \u001b[35m0.6079\u001b[0m  0.7511\n",
      "     14        \u001b[36m0.6183\u001b[0m       \u001b[32m0.6728\u001b[0m        \u001b[35m0.6066\u001b[0m  0.7627\n",
      "     15        0.6212       0.6663        \u001b[35m0.6058\u001b[0m  0.7472\n",
      "     16        \u001b[36m0.6171\u001b[0m       0.6720        0.6062  0.7470\n",
      "     17        \u001b[36m0.6161\u001b[0m       0.6708        \u001b[35m0.6051\u001b[0m  0.7581\n",
      "     18        \u001b[36m0.6157\u001b[0m       0.6680        \u001b[35m0.6047\u001b[0m  0.7577\n",
      "     19        \u001b[36m0.6144\u001b[0m       0.6685        \u001b[35m0.6043\u001b[0m  0.7517\n",
      "     20        \u001b[36m0.6130\u001b[0m       \u001b[32m0.6753\u001b[0m        0.6045  0.7533\n",
      "     21        0.6151       0.6693        \u001b[35m0.6033\u001b[0m  0.7478\n",
      "     22        \u001b[36m0.6108\u001b[0m       0.6736        \u001b[35m0.6028\u001b[0m  0.7445\n",
      "     23        0.6118       0.6718        0.6029  0.7498\n",
      "     24        \u001b[36m0.6107\u001b[0m       \u001b[32m0.6796\u001b[0m        0.6037  0.7542\n",
      "     25        \u001b[36m0.6080\u001b[0m       0.6765        \u001b[35m0.6016\u001b[0m  0.7649\n",
      "     26        \u001b[36m0.6077\u001b[0m       0.6770        \u001b[35m0.6015\u001b[0m  0.8618\n",
      "     27        0.6095       0.6795        \u001b[35m0.6010\u001b[0m  0.8656\n",
      "     28        0.6078       0.6786        0.6013  0.7747\n",
      "     29        0.6086       0.6740        \u001b[35m0.6007\u001b[0m  0.7569\n",
      "     30        \u001b[36m0.6051\u001b[0m       0.6763        \u001b[35m0.6004\u001b[0m  0.7572\n",
      "     31        0.6066       0.6722        0.6014  0.7693\n",
      "     32        \u001b[36m0.6041\u001b[0m       \u001b[32m0.6806\u001b[0m        0.6004  0.7562\n",
      "     33        0.6073       0.6799        \u001b[35m0.5994\u001b[0m  0.7544\n",
      "     34        0.6043       0.6802        0.6006  0.7545\n",
      "     35        \u001b[36m0.6040\u001b[0m       0.6783        \u001b[35m0.5982\u001b[0m  0.7645\n",
      "     36        0.6046       0.6789        0.5990  0.7552\n",
      "     37        \u001b[36m0.6017\u001b[0m       0.6756        0.6002  0.7488\n",
      "     38        0.6027       0.6783        \u001b[35m0.5976\u001b[0m  0.7543\n",
      "     39        0.6023       0.6793        0.5982  0.7575\n",
      "     40        \u001b[36m0.6007\u001b[0m       0.6802        0.5984  0.7639\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.5; total time=  30.8s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7146\u001b[0m       \u001b[32m0.5909\u001b[0m        \u001b[35m0.6550\u001b[0m  0.7532\n",
      "      2        \u001b[36m0.6842\u001b[0m       \u001b[32m0.6235\u001b[0m        \u001b[35m0.6418\u001b[0m  0.7489\n",
      "      3        \u001b[36m0.6669\u001b[0m       \u001b[32m0.6368\u001b[0m        \u001b[35m0.6348\u001b[0m  0.7574\n",
      "      4        \u001b[36m0.6569\u001b[0m       \u001b[32m0.6508\u001b[0m        \u001b[35m0.6297\u001b[0m  0.7631\n",
      "      5        \u001b[36m0.6488\u001b[0m       0.6498        \u001b[35m0.6289\u001b[0m  0.7507\n",
      "      6        \u001b[36m0.6444\u001b[0m       \u001b[32m0.6585\u001b[0m        \u001b[35m0.6247\u001b[0m  0.7585\n",
      "      7        \u001b[36m0.6410\u001b[0m       \u001b[32m0.6598\u001b[0m        \u001b[35m0.6240\u001b[0m  0.7560\n",
      "      8        \u001b[36m0.6362\u001b[0m       0.6596        \u001b[35m0.6217\u001b[0m  0.7502\n",
      "      9        \u001b[36m0.6329\u001b[0m       \u001b[32m0.6649\u001b[0m        \u001b[35m0.6196\u001b[0m  0.7510\n",
      "     10        \u001b[36m0.6301\u001b[0m       \u001b[32m0.6662\u001b[0m        \u001b[35m0.6186\u001b[0m  0.7569\n",
      "     11        \u001b[36m0.6271\u001b[0m       \u001b[32m0.6715\u001b[0m        \u001b[35m0.6173\u001b[0m  0.7548\n",
      "     12        0.6272       0.6646        0.6174  0.7553\n",
      "     13        \u001b[36m0.6234\u001b[0m       0.6698        \u001b[35m0.6166\u001b[0m  0.7503\n",
      "     14        \u001b[36m0.6224\u001b[0m       0.6710        \u001b[35m0.6152\u001b[0m  0.7552\n",
      "     15        \u001b[36m0.6221\u001b[0m       0.6686        0.6158  0.7489\n",
      "     16        \u001b[36m0.6185\u001b[0m       0.6700        \u001b[35m0.6146\u001b[0m  0.7635\n",
      "     17        0.6197       0.6658        0.6155  0.7534\n",
      "     18        \u001b[36m0.6176\u001b[0m       0.6703        \u001b[35m0.6138\u001b[0m  0.7558\n",
      "     19        \u001b[36m0.6161\u001b[0m       0.6703        \u001b[35m0.6134\u001b[0m  0.7495\n",
      "     20        \u001b[36m0.6154\u001b[0m       \u001b[32m0.6735\u001b[0m        \u001b[35m0.6127\u001b[0m  0.7517\n",
      "     21        \u001b[36m0.6151\u001b[0m       \u001b[32m0.6739\u001b[0m        \u001b[35m0.6123\u001b[0m  0.7522\n",
      "     22        0.6159       0.6730        \u001b[35m0.6122\u001b[0m  0.7573\n",
      "     23        \u001b[36m0.6137\u001b[0m       0.6718        \u001b[35m0.6120\u001b[0m  0.7612\n",
      "     24        0.6141       \u001b[32m0.6742\u001b[0m        \u001b[35m0.6120\u001b[0m  0.7596\n",
      "     25        \u001b[36m0.6117\u001b[0m       0.6725        0.6123  0.7586\n",
      "     26        0.6134       \u001b[32m0.6743\u001b[0m        \u001b[35m0.6110\u001b[0m  0.7510\n",
      "     27        0.6120       0.6743        0.6117  0.7737\n",
      "     28        \u001b[36m0.6091\u001b[0m       \u001b[32m0.6765\u001b[0m        0.6111  0.7602\n",
      "     29        0.6110       0.6765        \u001b[35m0.6103\u001b[0m  0.7651\n",
      "     30        \u001b[36m0.6075\u001b[0m       0.6762        0.6109  0.7805\n",
      "     31        0.6101       0.6732        \u001b[35m0.6102\u001b[0m  0.7614\n",
      "     32        0.6079       0.6750        \u001b[35m0.6099\u001b[0m  0.7501\n",
      "     33        0.6090       0.6740        \u001b[35m0.6096\u001b[0m  0.8061\n",
      "     34        0.6077       0.6746        \u001b[35m0.6095\u001b[0m  0.8429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     35        \u001b[36m0.6074\u001b[0m       0.6718        \u001b[35m0.6094\u001b[0m  0.8805\n",
      "     36        \u001b[36m0.6050\u001b[0m       0.6760        0.6096  0.8715\n",
      "     37        0.6066       \u001b[32m0.6775\u001b[0m        \u001b[35m0.6091\u001b[0m  0.7767\n",
      "     38        0.6057       0.6760        \u001b[35m0.6084\u001b[0m  0.8565\n",
      "     39        \u001b[36m0.6049\u001b[0m       0.6750        0.6100  0.8712\n",
      "     40        \u001b[36m0.6046\u001b[0m       \u001b[32m0.6816\u001b[0m        \u001b[35m0.6076\u001b[0m  0.7907\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.5; total time=  31.2s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7209\u001b[0m       \u001b[32m0.6126\u001b[0m        \u001b[35m0.6621\u001b[0m  0.7748\n",
      "      2        \u001b[36m0.6861\u001b[0m       \u001b[32m0.6432\u001b[0m        \u001b[35m0.6442\u001b[0m  0.7800\n",
      "      3        \u001b[36m0.6652\u001b[0m       \u001b[32m0.6515\u001b[0m        \u001b[35m0.6347\u001b[0m  0.7692\n",
      "      4        \u001b[36m0.6543\u001b[0m       \u001b[32m0.6536\u001b[0m        \u001b[35m0.6304\u001b[0m  0.7721\n",
      "      5        \u001b[36m0.6465\u001b[0m       \u001b[32m0.6585\u001b[0m        \u001b[35m0.6270\u001b[0m  0.7881\n",
      "      6        \u001b[36m0.6395\u001b[0m       \u001b[32m0.6605\u001b[0m        \u001b[35m0.6238\u001b[0m  0.7772\n",
      "      7        \u001b[36m0.6393\u001b[0m       \u001b[32m0.6625\u001b[0m        \u001b[35m0.6232\u001b[0m  0.8046\n",
      "      8        \u001b[36m0.6344\u001b[0m       \u001b[32m0.6628\u001b[0m        \u001b[35m0.6225\u001b[0m  0.7795\n",
      "      9        \u001b[36m0.6301\u001b[0m       \u001b[32m0.6652\u001b[0m        \u001b[35m0.6195\u001b[0m  0.7971\n",
      "     10        \u001b[36m0.6269\u001b[0m       0.6643        \u001b[35m0.6183\u001b[0m  0.7965\n",
      "     11        \u001b[36m0.6215\u001b[0m       \u001b[32m0.6653\u001b[0m        \u001b[35m0.6182\u001b[0m  0.7712\n",
      "     12        0.6244       \u001b[32m0.6689\u001b[0m        \u001b[35m0.6167\u001b[0m  0.8151\n",
      "     13        0.6217       \u001b[32m0.6725\u001b[0m        0.6172  0.7904\n",
      "     14        \u001b[36m0.6213\u001b[0m       0.6722        \u001b[35m0.6159\u001b[0m  0.8011\n",
      "     15        \u001b[36m0.6201\u001b[0m       0.6708        \u001b[35m0.6150\u001b[0m  0.7913\n",
      "     16        \u001b[36m0.6165\u001b[0m       0.6722        0.6168  0.7903\n",
      "     17        0.6177       0.6678        0.6152  0.8109\n",
      "     18        \u001b[36m0.6162\u001b[0m       \u001b[32m0.6750\u001b[0m        0.6158  0.7852\n",
      "     19        0.6163       0.6739        \u001b[35m0.6117\u001b[0m  0.7875\n",
      "     20        \u001b[36m0.6125\u001b[0m       \u001b[32m0.6763\u001b[0m        0.6149  0.7920\n",
      "     21        0.6143       0.6755        0.6129  0.8209\n",
      "     22        \u001b[36m0.6118\u001b[0m       0.6735        0.6139  0.7960\n",
      "     23        \u001b[36m0.6117\u001b[0m       0.6756        0.6132  0.7914\n",
      "     24        \u001b[36m0.6106\u001b[0m       \u001b[32m0.6773\u001b[0m        0.6131  0.7843\n",
      "     25        \u001b[36m0.6082\u001b[0m       0.6757        0.6124  0.7995\n",
      "     26        \u001b[36m0.6074\u001b[0m       0.6746        0.6126  0.7969\n",
      "     27        0.6096       \u001b[32m0.6807\u001b[0m        \u001b[35m0.6114\u001b[0m  0.7489\n",
      "     28        0.6110       0.6760        0.6116  0.8650\n",
      "     29        0.6096       0.6776        \u001b[35m0.6109\u001b[0m  0.8999\n",
      "     30        0.6088       \u001b[32m0.6813\u001b[0m        0.6118  0.9043\n",
      "     31        0.6106       0.6779        0.6132  0.9280\n",
      "     32        \u001b[36m0.6054\u001b[0m       0.6763        0.6117  0.9409\n",
      "     33        0.6071       0.6793        0.6135  0.9287\n",
      "     34        0.6070       0.6792        \u001b[35m0.6096\u001b[0m  1.0214\n",
      "     35        0.6073       0.6772        0.6116  1.0128\n",
      "     36        \u001b[36m0.6036\u001b[0m       0.6772        0.6104  0.9319\n",
      "     37        0.6058       0.6769        0.6136  0.9111\n",
      "     38        0.6077       0.6787        0.6114  0.8538\n",
      "     39        0.6046       0.6783        0.6121  0.7857\n",
      "     40        \u001b[36m0.6022\u001b[0m       0.6809        \u001b[35m0.6086\u001b[0m  0.7730\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.5; total time=  33.4s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7571\u001b[0m       \u001b[32m0.5926\u001b[0m        \u001b[35m0.6729\u001b[0m  0.7781\n",
      "      2        \u001b[36m0.7041\u001b[0m       \u001b[32m0.6234\u001b[0m        \u001b[35m0.6528\u001b[0m  0.7770\n",
      "      3        \u001b[36m0.6828\u001b[0m       \u001b[32m0.6461\u001b[0m        \u001b[35m0.6431\u001b[0m  0.7865\n",
      "      4        \u001b[36m0.6684\u001b[0m       \u001b[32m0.6536\u001b[0m        \u001b[35m0.6383\u001b[0m  0.7893\n",
      "      5        \u001b[36m0.6596\u001b[0m       \u001b[32m0.6575\u001b[0m        \u001b[35m0.6338\u001b[0m  0.7935\n",
      "      6        \u001b[36m0.6568\u001b[0m       \u001b[32m0.6592\u001b[0m        \u001b[35m0.6299\u001b[0m  0.7937\n",
      "      7        \u001b[36m0.6511\u001b[0m       \u001b[32m0.6631\u001b[0m        \u001b[35m0.6296\u001b[0m  0.7943\n",
      "      8        \u001b[36m0.6463\u001b[0m       \u001b[32m0.6650\u001b[0m        \u001b[35m0.6261\u001b[0m  0.8041\n",
      "      9        \u001b[36m0.6402\u001b[0m       \u001b[32m0.6695\u001b[0m        \u001b[35m0.6256\u001b[0m  0.7884\n",
      "     10        \u001b[36m0.6378\u001b[0m       \u001b[32m0.6702\u001b[0m        \u001b[35m0.6249\u001b[0m  0.8230\n",
      "     11        0.6386       0.6683        \u001b[35m0.6220\u001b[0m  0.9006\n",
      "     12        \u001b[36m0.6348\u001b[0m       0.6700        0.6223  0.9285\n",
      "     13        \u001b[36m0.6338\u001b[0m       \u001b[32m0.6708\u001b[0m        \u001b[35m0.6208\u001b[0m  0.8336\n",
      "     14        \u001b[36m0.6296\u001b[0m       0.6692        \u001b[35m0.6205\u001b[0m  0.7997\n",
      "     15        \u001b[36m0.6281\u001b[0m       \u001b[32m0.6719\u001b[0m        \u001b[35m0.6198\u001b[0m  0.7867\n",
      "     16        \u001b[36m0.6266\u001b[0m       \u001b[32m0.6725\u001b[0m        \u001b[35m0.6192\u001b[0m  0.7937\n",
      "     17        0.6278       0.6712        \u001b[35m0.6189\u001b[0m  0.7863\n",
      "     18        \u001b[36m0.6235\u001b[0m       \u001b[32m0.6739\u001b[0m        \u001b[35m0.6168\u001b[0m  0.7891\n",
      "     19        0.6255       0.6735        0.6210  0.7880\n",
      "     20        \u001b[36m0.6230\u001b[0m       0.6726        0.6169  0.8006\n",
      "     21        \u001b[36m0.6225\u001b[0m       \u001b[32m0.6760\u001b[0m        \u001b[35m0.6167\u001b[0m  0.7897\n",
      "     22        \u001b[36m0.6209\u001b[0m       0.6712        0.6176  0.7925\n",
      "     23        0.6220       0.6723        \u001b[35m0.6151\u001b[0m  0.8266\n",
      "     24        \u001b[36m0.6188\u001b[0m       0.6756        \u001b[35m0.6149\u001b[0m  0.8042\n",
      "     25        \u001b[36m0.6180\u001b[0m       0.6760        0.6152  0.7896\n",
      "     26        \u001b[36m0.6173\u001b[0m       \u001b[32m0.6796\u001b[0m        \u001b[35m0.6148\u001b[0m  0.7550\n",
      "     27        \u001b[36m0.6158\u001b[0m       0.6770        0.6149  1.0054\n",
      "     28        \u001b[36m0.6148\u001b[0m       0.6772        0.6156  0.8309\n",
      "     29        0.6166       0.6769        \u001b[35m0.6137\u001b[0m  0.7604\n",
      "     30        0.6165       0.6786        \u001b[35m0.6136\u001b[0m  0.7205\n",
      "     31        0.6149       0.6789        \u001b[35m0.6122\u001b[0m  0.7649\n",
      "     32        \u001b[36m0.6145\u001b[0m       \u001b[32m0.6799\u001b[0m        0.6127  0.7541\n",
      "     33        0.6161       0.6782        0.6133  0.7673\n",
      "     34        0.6149       \u001b[32m0.6805\u001b[0m        \u001b[35m0.6122\u001b[0m  0.7713\n",
      "     35        0.6154       0.6803        0.6137  0.7522\n",
      "     36        0.6163       \u001b[32m0.6807\u001b[0m        \u001b[35m0.6120\u001b[0m  0.7532\n",
      "     37        \u001b[36m0.6135\u001b[0m       0.6805        0.6123  0.7494\n",
      "     38        \u001b[36m0.6131\u001b[0m       \u001b[32m0.6825\u001b[0m        \u001b[35m0.6118\u001b[0m  0.7533\n",
      "     39        0.6133       0.6820        0.6118  0.7609\n",
      "     40        0.6144       0.6803        \u001b[35m0.6115\u001b[0m  0.7870\n",
      "[CV] END ..................lr=0.001, module__dropoutrate=0.5; total time=  32.2s\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(model, params, refit=False, cv=4, scoring='accuracy', verbose=2)\n",
    "gs_results = gs.fit(X_scale_torch,y_scale_torch )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "18062320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time [31.88682443 30.51245248 30.57055247 31.74798542]\n",
      "std_fit_time [1.33337528 0.20746012 0.19286978 1.00132016]\n",
      "mean_score_time [0.16935319 0.15622282 0.15137154 0.15740955]\n",
      "std_score_time [0.00573338 0.00570575 0.00165977 0.00264987]\n",
      "param_lr [0.01 0.01 0.001 0.001]\n",
      "param_module__dropoutrate [0.2 0.5 0.2 0.5]\n",
      "params [{'lr': 0.01, 'module__dropoutrate': 0.2}, {'lr': 0.01, 'module__dropoutrate': 0.5}, {'lr': 0.001, 'module__dropoutrate': 0.2}, {'lr': 0.001, 'module__dropoutrate': 0.5}]\n",
      "split0_test_score [0.68113337 0.67599726 0.67531245 0.67719569]\n",
      "split1_test_score [0.70330423 0.69149118 0.68156138 0.68130457]\n",
      "split2_test_score [0.71409005 0.69003595 0.68515665 0.68224619]\n",
      "split3_test_score [0.71391885 0.69928095 0.69910974 0.69637048]\n",
      "mean_test_score [0.70311162 0.68920134 0.68528505 0.68427923]\n",
      "std_test_score [0.01342016 0.00839473 0.00872435 0.00723458]\n",
      "rank_test_score [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "for key in gs.cv_results_.keys():\n",
    "    print(key, gs.cv_results_[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "425a244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model1.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2fa93c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_params(\n",
    "    f_params='model.pkl', f_optimizer='opt.pkl', f_history='history.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "ba4d8448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.6173\u001b[0m       \u001b[32m0.6842\u001b[0m        \u001b[35m0.5973\u001b[0m  1.0533\n",
      "      2        \u001b[36m0.5909\u001b[0m       \u001b[32m0.6884\u001b[0m        \u001b[35m0.5961\u001b[0m  1.0349\n",
      "      3        \u001b[36m0.5853\u001b[0m       \u001b[32m0.6924\u001b[0m        \u001b[35m0.5916\u001b[0m  1.0474\n",
      "      4        \u001b[36m0.5807\u001b[0m       0.6900        \u001b[35m0.5912\u001b[0m  1.0496\n",
      "      5        \u001b[36m0.5785\u001b[0m       0.6871        0.5928  1.0142\n",
      "      6        \u001b[36m0.5760\u001b[0m       0.6895        \u001b[35m0.5890\u001b[0m  1.0178\n",
      "      7        \u001b[36m0.5734\u001b[0m       0.6848        \u001b[35m0.5882\u001b[0m  1.0188\n",
      "      8        \u001b[36m0.5733\u001b[0m       0.6831        0.5899  1.0135\n",
      "      9        \u001b[36m0.5710\u001b[0m       0.6816        0.5954  1.0644\n",
      "     10        \u001b[36m0.5689\u001b[0m       0.6903        \u001b[35m0.5866\u001b[0m  1.0293\n",
      "     11        \u001b[36m0.5670\u001b[0m       0.6863        \u001b[35m0.5860\u001b[0m  1.0264\n",
      "     12        0.5673       0.6886        0.5867  1.0984\n",
      "     13        \u001b[36m0.5660\u001b[0m       0.6865        \u001b[35m0.5859\u001b[0m  1.0589\n",
      "     14        \u001b[36m0.5658\u001b[0m       0.6847        0.5887  1.0605\n",
      "     15        \u001b[36m0.5648\u001b[0m       0.6859        \u001b[35m0.5846\u001b[0m  1.0302\n",
      "     16        \u001b[36m0.5642\u001b[0m       0.6878        0.5856  1.0346\n",
      "     17        \u001b[36m0.5618\u001b[0m       0.6875        0.5858  1.0358\n",
      "     18        \u001b[36m0.5604\u001b[0m       0.6865        0.5894  1.0314\n",
      "     19        0.5621       0.6818        0.5859  1.0423\n",
      "     20        \u001b[36m0.5598\u001b[0m       0.6788        0.5900  1.0218\n",
      "     21        \u001b[36m0.5576\u001b[0m       0.6890        0.5894  1.0408\n",
      "     22        0.5589       0.6813        0.5880  1.0328\n",
      "     23        0.5579       0.6849        0.5878  1.0323\n",
      "     24        0.5581       0.6868        0.5865  1.0221\n",
      "     25        \u001b[36m0.5567\u001b[0m       \u001b[32m0.6942\u001b[0m        \u001b[35m0.5837\u001b[0m  1.0206\n",
      "     26        \u001b[36m0.5554\u001b[0m       0.6891        0.5850  1.0167\n",
      "     27        \u001b[36m0.5547\u001b[0m       0.6920        0.5843  0.9964\n",
      "     28        0.5550       0.6898        0.5862  1.0285\n",
      "     29        \u001b[36m0.5522\u001b[0m       0.6921        \u001b[35m0.5804\u001b[0m  1.2718\n",
      "     30        0.5529       0.6901        0.5868  1.3067\n",
      "     31        0.5524       0.6852        0.5891  1.2900\n",
      "     32        0.5528       0.6880        0.5844  1.2534\n",
      "     33        \u001b[36m0.5514\u001b[0m       0.6923        0.5830  1.0566\n",
      "     34        0.5524       0.6884        0.5846  1.0673\n",
      "     35        \u001b[36m0.5497\u001b[0m       0.6891        0.5818  1.0503\n",
      "     36        0.5509       0.6891        0.5854  1.0943\n",
      "     37        \u001b[36m0.5488\u001b[0m       0.6889        0.5866  1.0429\n",
      "     38        0.5490       0.6897        0.5850  1.1093\n",
      "     39        0.5502       0.6921        0.5835  1.1217\n",
      "     40        0.5500       0.6866        0.5860  1.1302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scale', StandardScaler()), ('tensor', toTensor()),\n",
       "                ('classification',\n",
       "                 <class 'skorch.classifier.NeuralNetBinaryClassifier'>[initialized](\n",
       "  module_=MyModule(\n",
       "    (layer1): Linear(in_features=23, out_features=128, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (output): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (batchnorm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  ),\n",
       "))])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from classes import toTensor\n",
    "pipeline = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "      ('tensor',toTensor()),\n",
    "      ('classification',model)\n",
    "    ])\n",
    "pipeline.fit(X_res, torch.FloatTensor(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9196a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "with open('model1.pkl', 'wb') as f:\n",
    "    joblib.dump(pipeline,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4019687d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"LIMIT_BAL\":50000,\"SEX\":2,\"EDUCATION\":3,\"MARRIAGE\":3,\"AGE\":23,\"PAY_0\":1,\"PAY_2\":2,\"PAY_3\":0,\"PAY_4\":0,\"PAY_5\":0,\"PAY_6\":0,\"BILL_AMT1\":50614,\"BILL_AMT2\":29173,\"BILL_AMT3\":28116,\"BILL_AMT4\":28771,\"BILL_AMT5\":29531,\"BILL_AMT6\":30211,\"PAY_AMT1\":0,\"PAY_AMT2\":1500,\"PAY_AMT3\":1100,\"PAY_AMT4\":1200,\"PAY_AMT5\":1300,\"PAY_AMT6\":1100}'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jinput = X_res.iloc[15].to_json()\n",
    "jinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a08d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"LIMIT_BAL\": 20000,\n",
    "  \"SEX\": 2,\n",
    "  \"EDUCATION\": 2,\n",
    "  \"MARRIAGE\": 1,\n",
    "  \"AGE\": 24,\n",
    "  \"PAY_0\": 2,\n",
    "  \"PAY_2\": -1,\n",
    "  \"PAY_3\": -1,\n",
    "  \"PAY_4\": -1,\n",
    "  \"PAY_5\": -2,\n",
    "  \"PAY_6\": -2,\n",
    "  \"BILL_AMT1\": 3913,\n",
    "  \"BILL_AMT2\": 3102,\n",
    "  \"BILL_AMT3\": 689,\n",
    "  \"BILL_AMT4\": 0,\n",
    "  \"BILL_AMT5\": 0,\n",
    "  \"BILL_AMT6\": 0,\n",
    "  \"PAY_AMT1\": 0,\n",
    "  \"PAY_AMT2\": 689,\n",
    "  \"PAY_AMT3\": 0,\n",
    "  \"PAY_AMT4\": 0,\n",
    "  \"PAY_AMT5\": 0,\n",
    "  \"PAY_AMT6\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "5c04f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "bashCommand = f\"\"\"curl -X 'POST' 'http://127.0.0.1:8000/predict' -H 'accept: application/json' -H 'Content-Type: application/json' -d {jinput}\"\"\"\n",
    "headers = {\n",
    "    \n",
    "}\n",
    "res = requests.post('http://127.0.0.1:8000/predict', data=jinput, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f0ed69aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\":\"SUCCESS\",\"prediction\":0}'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2386ca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 ms ± 538 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "res = requests.post('http://127.0.0.1:8000/predict', data=jinput, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa3858a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b356281",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9558a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f7deee00",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2',\n",
       "       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
       "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
       "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
       "       'default payment next month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3455da1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "lincolumns = (['LIMIT_BAL','AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "              'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'])\n",
    "ct = ColumnTransformer([\n",
    "        ('scalethis', StandardScaler(), lincolumns)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "ct2 = ct.fit_transform(df.iloc[:,:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "147e2ccf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.136720</td>\n",
       "      <td>-1.246020</td>\n",
       "      <td>-0.642501</td>\n",
       "      <td>-0.647399</td>\n",
       "      <td>-0.667993</td>\n",
       "      <td>-0.672497</td>\n",
       "      <td>-0.663059</td>\n",
       "      <td>-0.652724</td>\n",
       "      <td>-0.341942</td>\n",
       "      <td>-0.227086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293382</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.365981</td>\n",
       "      <td>-1.029047</td>\n",
       "      <td>-0.659219</td>\n",
       "      <td>-0.666747</td>\n",
       "      <td>-0.639254</td>\n",
       "      <td>-0.621636</td>\n",
       "      <td>-0.606229</td>\n",
       "      <td>-0.597966</td>\n",
       "      <td>-0.341942</td>\n",
       "      <td>-0.213588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180878</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.597202</td>\n",
       "      <td>-0.161156</td>\n",
       "      <td>-0.298560</td>\n",
       "      <td>-0.493899</td>\n",
       "      <td>-0.482408</td>\n",
       "      <td>-0.449730</td>\n",
       "      <td>-0.417188</td>\n",
       "      <td>-0.391630</td>\n",
       "      <td>-0.250292</td>\n",
       "      <td>-0.191887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.905498</td>\n",
       "      <td>0.164303</td>\n",
       "      <td>-0.057491</td>\n",
       "      <td>-0.013293</td>\n",
       "      <td>0.032846</td>\n",
       "      <td>-0.232373</td>\n",
       "      <td>-0.186729</td>\n",
       "      <td>-0.156579</td>\n",
       "      <td>-0.221191</td>\n",
       "      <td>-0.169361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237130</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.905498</td>\n",
       "      <td>2.334029</td>\n",
       "      <td>-0.578618</td>\n",
       "      <td>-0.611318</td>\n",
       "      <td>-0.161189</td>\n",
       "      <td>-0.346997</td>\n",
       "      <td>-0.348137</td>\n",
       "      <td>-0.331482</td>\n",
       "      <td>-0.221191</td>\n",
       "      <td>1.335034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.404759</td>\n",
       "      <td>0.381275</td>\n",
       "      <td>1.870379</td>\n",
       "      <td>2.018136</td>\n",
       "      <td>2.326690</td>\n",
       "      <td>0.695474</td>\n",
       "      <td>-0.149259</td>\n",
       "      <td>-0.384392</td>\n",
       "      <td>0.171250</td>\n",
       "      <td>0.611048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>-0.134759</td>\n",
       "      <td>0.815221</td>\n",
       "      <td>-0.672786</td>\n",
       "      <td>-0.665299</td>\n",
       "      <td>-0.627430</td>\n",
       "      <td>-0.532924</td>\n",
       "      <td>-0.577691</td>\n",
       "      <td>-0.652724</td>\n",
       "      <td>-0.231032</td>\n",
       "      <td>-0.103955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>-1.059646</td>\n",
       "      <td>0.164303</td>\n",
       "      <td>-0.647227</td>\n",
       "      <td>-0.643830</td>\n",
       "      <td>-0.638158</td>\n",
       "      <td>-0.347961</td>\n",
       "      <td>-0.324517</td>\n",
       "      <td>-0.327687</td>\n",
       "      <td>-0.341942</td>\n",
       "      <td>-0.256990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.119001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>-0.674276</td>\n",
       "      <td>0.598248</td>\n",
       "      <td>-0.717982</td>\n",
       "      <td>0.410269</td>\n",
       "      <td>0.422373</td>\n",
       "      <td>0.147844</td>\n",
       "      <td>-0.468063</td>\n",
       "      <td>0.169130</td>\n",
       "      <td>4.844316</td>\n",
       "      <td>-0.109033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>-0.905498</td>\n",
       "      <td>1.140680</td>\n",
       "      <td>-0.044739</td>\n",
       "      <td>-0.003851</td>\n",
       "      <td>0.039667</td>\n",
       "      <td>-0.104582</td>\n",
       "      <td>-0.129669</td>\n",
       "      <td>-0.395592</td>\n",
       "      <td>-0.216481</td>\n",
       "      <td>-0.178866</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -1.136720 -1.246020 -0.642501 -0.647399 -0.667993 -0.672497 -0.663059   \n",
       "1     -0.365981 -1.029047 -0.659219 -0.666747 -0.639254 -0.621636 -0.606229   \n",
       "2     -0.597202 -0.161156 -0.298560 -0.493899 -0.482408 -0.449730 -0.417188   \n",
       "3     -0.905498  0.164303 -0.057491 -0.013293  0.032846 -0.232373 -0.186729   \n",
       "4     -0.905498  2.334029 -0.578618 -0.611318 -0.161189 -0.346997 -0.348137   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "29995  0.404759  0.381275  1.870379  2.018136  2.326690  0.695474 -0.149259   \n",
       "29996 -0.134759  0.815221 -0.672786 -0.665299 -0.627430 -0.532924 -0.577691   \n",
       "29997 -1.059646  0.164303 -0.647227 -0.643830 -0.638158 -0.347961 -0.324517   \n",
       "29998 -0.674276  0.598248 -0.717982  0.410269  0.422373  0.147844 -0.468063   \n",
       "29999 -0.905498  1.140680 -0.044739 -0.003851  0.039667 -0.104582 -0.129669   \n",
       "\n",
       "             7         8         9   ...        13   14   15   16   17   18  \\\n",
       "0     -0.652724 -0.341942 -0.227086  ... -0.293382  2.0  2.0  1.0  2.0  2.0   \n",
       "1     -0.597966 -0.341942 -0.213588  ... -0.180878  2.0  2.0  2.0 -1.0  2.0   \n",
       "2     -0.391630 -0.250292 -0.191887  ... -0.012122  2.0  2.0  2.0  0.0  0.0   \n",
       "3     -0.156579 -0.221191 -0.169361  ... -0.237130  2.0  2.0  1.0  0.0  0.0   \n",
       "4     -0.331482 -0.221191  1.335034  ... -0.255187  1.0  2.0  1.0 -1.0  0.0   \n",
       "...         ...       ...       ...  ...       ...  ...  ...  ...  ...  ...   \n",
       "29995 -0.384392  0.171250  0.611048  ... -0.237130  1.0  3.0  1.0  0.0  0.0   \n",
       "29996 -0.652724 -0.231032 -0.103955  ... -0.293382  1.0  3.0  2.0 -1.0 -1.0   \n",
       "29997 -0.327687 -0.341942 -0.256990  ... -0.119001  1.0  2.0  2.0  4.0  3.0   \n",
       "29998  0.169130  4.844316 -0.109033  ... -0.191904  1.0  3.0  1.0  1.0 -1.0   \n",
       "29999 -0.395592 -0.216481 -0.178866  ... -0.237130  1.0  2.0  1.0  0.0  0.0   \n",
       "\n",
       "        19   20   21   22  \n",
       "0     -1.0 -1.0 -2.0 -2.0  \n",
       "1      0.0  0.0  0.0  2.0  \n",
       "2      0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  \n",
       "4     -1.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  \n",
       "29995  0.0  0.0  0.0  0.0  \n",
       "29996 -1.0 -1.0  0.0  0.0  \n",
       "29997  2.0 -1.0  0.0  0.0  \n",
       "29998  0.0  0.0  0.0 -1.0  \n",
       "29999  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[30000 rows x 23 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfct2 = pd.DataFrame(ct2)\n",
    "dfct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0dd6bf9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        17   18   19   20   21   22\n",
       "0      2.0  2.0 -1.0 -1.0 -2.0 -2.0\n",
       "1     -1.0  2.0  0.0  0.0  0.0  2.0\n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4     -1.0  0.0 -1.0  0.0  0.0  0.0\n",
       "...    ...  ...  ...  ...  ...  ...\n",
       "29995  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "29996 -1.0 -1.0 -1.0 -1.0  0.0  0.0\n",
       "29997  4.0  3.0  2.0 -1.0  0.0  0.0\n",
       "29998  1.0 -1.0  0.0  0.0  0.0 -1.0\n",
       "29999  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[30000 rows x 6 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_numeric = dfct2.iloc[:,:14]\n",
    "df_cat = dfct2.iloc[:,14:]\n",
    "df_cat1 = df_cat.iloc[:,0]\n",
    "df_cat2 = df_cat.iloc[:,1]\n",
    "df_cat3 = df_cat.iloc[:,2]\n",
    "df_cat4 = df_cat.iloc[:,3:]\n",
    "df_cat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c219d16a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tl/ljh9dpdd2fnbgmc62qc2b49h0000gn/T/ipykernel_79650/1427149650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_sz_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_cat1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "def emb_sz_rule(n_cat): \n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "embed = nn.Embedding(2, emb_sz_rule(2))\n",
    "\n",
    "embed(torch.tensor(df_cat1.values).to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265ce7b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def emb_sz_rule(n_cat): \n",
    "    return min(600, round(1.6 * n_cat**0.56))\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_inputs=23, num_units_d1=128, num_units_d2=128)):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.dense0 = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(num_units, num_units)\n",
    "        self.output = nn.Linear(num_units, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.embed1 = nn.Embedding(2, emb_sz_rule(2))\n",
    "        self.embed2 = nn.Embedding(7, emb_sz_rule(7))\n",
    "        self.embed3 = nn.Embedding(4, emb_sz_rule(4))\n",
    "        self.embed4 = nn.Embedding(11, emb_sz_rule(11))\n",
    "    def forward(self, X, cat1, cat2, cat3, cat4):\n",
    "        x1 = self.embed1(cat1)\n",
    "        x2 = self.embed2(cat2)\n",
    "        x3 = self.embed3(cat3)\n",
    "        x4 = self.embed4(cat4)\n",
    "        X = torch.cat((X,x1,x2,x3,x4), dim=1)\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "model = NeuralNetBinaryClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=40,\n",
    "    lr=0.001,\n",
    "\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c6775",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc279115",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(23, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_inputs=23, num_units_d1=128, num_units_d2=128)):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.dense0 = nn.Linear(20, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(num_units, num_units)\n",
    "        self.output = nn.Linear(num_units, 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "model = NeuralNetBinaryClassifier(\n",
    "    MyModule,\n",
    "    max_epochs=40,\n",
    "    lr=0.001,\n",
    "\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee122def",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Py Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fe22bf6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## train data\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_data = TrainData(torch.FloatTensor(X_train), torch.FloatTensor(y_train.to_numpy(dtype=np.float64)))\n",
    "## test data    \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "\n",
    "test_data = TestData(torch.FloatTensor(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c90383d0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0bfd8f99",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        self.layer_1 = nn.Linear(23, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e26b173",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c7c9f444",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassification(\n",
      "  (layer_1): Linear(in_features=23, out_features=64, bias=True)\n",
      "  (layer_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (layer_out): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BinaryClassification()\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ed6113d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e42c126b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.45479 | Acc: 78.439\n",
      "Epoch 002: | Loss: 0.41233 | Acc: 80.850\n",
      "Epoch 003: | Loss: 0.40462 | Acc: 81.488\n",
      "Epoch 004: | Loss: 0.39199 | Acc: 82.028\n",
      "Epoch 005: | Loss: 0.38622 | Acc: 82.199\n",
      "Epoch 006: | Loss: 0.38073 | Acc: 82.523\n",
      "Epoch 007: | Loss: 0.37937 | Acc: 82.760\n",
      "Epoch 008: | Loss: 0.37373 | Acc: 83.192\n",
      "Epoch 009: | Loss: 0.36585 | Acc: 83.111\n",
      "Epoch 010: | Loss: 0.36803 | Acc: 83.328\n",
      "Epoch 011: | Loss: 0.36370 | Acc: 83.610\n",
      "Epoch 012: | Loss: 0.35545 | Acc: 83.979\n",
      "Epoch 013: | Loss: 0.35516 | Acc: 84.031\n",
      "Epoch 014: | Loss: 0.35149 | Acc: 84.261\n",
      "Epoch 015: | Loss: 0.35078 | Acc: 84.237\n",
      "Epoch 016: | Loss: 0.34535 | Acc: 84.415\n",
      "Epoch 017: | Loss: 0.34704 | Acc: 84.495\n",
      "Epoch 018: | Loss: 0.34282 | Acc: 84.679\n",
      "Epoch 019: | Loss: 0.34274 | Acc: 84.491\n",
      "Epoch 020: | Loss: 0.33811 | Acc: 85.070\n",
      "Epoch 021: | Loss: 0.33696 | Acc: 84.927\n",
      "Epoch 022: | Loss: 0.33600 | Acc: 84.868\n",
      "Epoch 023: | Loss: 0.33675 | Acc: 85.223\n",
      "Epoch 024: | Loss: 0.33064 | Acc: 85.334\n",
      "Epoch 025: | Loss: 0.33340 | Acc: 84.714\n",
      "Epoch 026: | Loss: 0.33138 | Acc: 85.220\n",
      "Epoch 027: | Loss: 0.32413 | Acc: 85.585\n",
      "Epoch 028: | Loss: 0.32957 | Acc: 85.425\n",
      "Epoch 029: | Loss: 0.32780 | Acc: 85.213\n",
      "Epoch 030: | Loss: 0.32576 | Acc: 85.533\n",
      "Epoch 031: | Loss: 0.32255 | Acc: 85.749\n",
      "Epoch 032: | Loss: 0.32203 | Acc: 85.659\n",
      "Epoch 033: | Loss: 0.32118 | Acc: 85.648\n",
      "Epoch 034: | Loss: 0.31853 | Acc: 85.878\n",
      "Epoch 035: | Loss: 0.31647 | Acc: 85.854\n",
      "Epoch 036: | Loss: 0.31892 | Acc: 85.728\n",
      "Epoch 037: | Loss: 0.31600 | Acc: 85.885\n",
      "Epoch 038: | Loss: 0.31326 | Acc: 86.160\n",
      "Epoch 039: | Loss: 0.31420 | Acc: 85.972\n",
      "Epoch 040: | Loss: 0.31226 | Acc: 86.307\n",
      "Epoch 041: | Loss: 0.31323 | Acc: 86.073\n",
      "Epoch 042: | Loss: 0.31081 | Acc: 86.052\n",
      "Epoch 043: | Loss: 0.31061 | Acc: 86.254\n",
      "Epoch 044: | Loss: 0.30963 | Acc: 86.328\n",
      "Epoch 045: | Loss: 0.31058 | Acc: 86.348\n",
      "Epoch 046: | Loss: 0.31165 | Acc: 86.324\n",
      "Epoch 047: | Loss: 0.30692 | Acc: 86.446\n",
      "Epoch 048: | Loss: 0.30742 | Acc: 86.554\n",
      "Epoch 049: | Loss: 0.30156 | Acc: 86.568\n",
      "Epoch 050: | Loss: 0.30101 | Acc: 86.826\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "582a0c8e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "566150aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2518,  939],\n",
       "       [ 575, 5015]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62d3e6a0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.77      3457\n",
      "           1       0.84      0.90      0.87      5590\n",
      "\n",
      "    accuracy                           0.83      9047\n",
      "   macro avg       0.83      0.81      0.82      9047\n",
      "weighted avg       0.83      0.83      0.83      9047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "075f4a2b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "40754b67",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13672015,  0.81016074,  0.18582826, ..., -0.30806256,\n",
       "        -0.31413612, -0.29338206],\n",
       "       [-0.3659805 ,  0.81016074,  0.18582826, ..., -0.24422965,\n",
       "        -0.31413612, -0.18087821],\n",
       "       [-0.59720239,  0.81016074,  0.18582826, ..., -0.24422965,\n",
       "        -0.24868274, -0.01212243],\n",
       "       ...,\n",
       "       [-1.05964618, -1.23432296,  0.18582826, ..., -0.03996431,\n",
       "        -0.18322937, -0.11900109],\n",
       "       [-0.67427636, -1.23432296,  1.45111372, ..., -0.18512036,\n",
       "         3.15253642, -0.19190359],\n",
       "       [-0.90549825, -1.23432296,  0.18582826, ..., -0.24422965,\n",
       "        -0.24868274, -0.23713013]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_og = scaler.fit_transform(df.iloc[:,:23])\n",
    "X_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c36e1503",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "og_data = TestData(torch.FloatTensor(X_og))\n",
    "og_loader = DataLoader(dataset=og_data, batch_size=1)\n",
    "og_y_pred_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch in og_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        og_y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "og_y_pred_list = [a.squeeze().tolist() for a in og_y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04b35c15",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7698, 15666],\n",
       "       [  660,  5976]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df['default payment next month'].to_numpy(dtype=np.float64), og_y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3c1d531c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.33      0.49     23364\n",
      "         1.0       0.28      0.90      0.42      6636\n",
      "\n",
      "    accuracy                           0.46     30000\n",
      "   macro avg       0.60      0.62      0.45     30000\n",
      "weighted avg       0.78      0.46      0.47     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['default payment next month'].to_numpy(dtype=np.float64), og_y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "596b01dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6f04b1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2841653707.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/tl/ljh9dpdd2fnbgmc62qc2b49h0000gn/T/ipykernel_79650/2841653707.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://towardsdatascience.com/pytorch-tabular-binary-classification-a0368da5bb89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc903a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

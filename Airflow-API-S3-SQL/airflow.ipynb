{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "There are two ways in which we can process data. In isolation from client interactions and continuously as a result of client interactions. This leads us to an important ETL distinction that should be discussed. \n",
    "\n",
    "##### ETL with Batch Processing\n",
    "\n",
    "In traditional ETL pipelines, data is processed in batches from a source database to some final destination, such as a data warehouse. This batch data handling is handled intermittently usually in defined periods - these trigger are usually time or event based. \n",
    " \n",
    "* Data import or export integration\n",
    "* Periodic recalculations of data\n",
    "* Periodic operations\n",
    "\n",
    "##### ETL with Stream Processing\n",
    "\n",
    "This type of ETL pipeline deals with real-time data that can come from user interactions or continuous data streams. In these cases, it is difficult to extract and transform data in large batches. This leads to a continuous flow of extracting, cleaning, transforming, and loading that happens very much in sync. \n",
    "\n",
    "* Continuous client data\n",
    "* Scalable data loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Project\n",
    "\n",
    "This project will utilize Apache Airflow, AWS S3, PostgreSQL, and the [Spoonacular API](https://spoonacular.com/food-api). Airflow will be the framework that will define and execute the workflow in a scheduled and scalable manner. The project will take the following architecture. \n",
    "\n",
    "![Overall Architecture](https://dataincubator-course.s3.amazonaws.com/miniprojects/images/pdc_project_architecture.png)\n",
    "\n",
    "There are two type of data sets you will interact with during this project - one being user submissions and the other being an API to retrieve information as a result of these submissions. These data sets are described below.\n",
    "\n",
    "* **User Ingredient Submissions**: Real-time JSON submissions that contain a list of ingredients, preferences, and user information which will be used in the construction of a relational PostgreSQL database.\n",
    "* **Spoonacular API**: An API containing recipes that can be queried based off various filters. The request made to this API will take a list of ingredients, food specifications, and other parameters specified in the user ingredient submissions then aggregate a list of recipes that match these descriptions.\n",
    "\n",
    "Airflow will be responsible for orchestrating batch processing, API calls, loading/extracting S3 data, and populating the relational database.\n",
    "\n",
    "Let's describe the two data sources before we get deeper into S3 and Airflow architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### User Ingredient Submissions\n",
    "\n",
    "Whenever a request is made to the [simple User Ingredient API](`http://user-ingredients.herokuapp.com/`), a single json data point including user information is returned. Airflow will manage these request that are coming in and collect 20 non-duplicate values before sending them to the S3 raw bucket. This is explained further in the Airflow steps - be sure to exclude duplicate values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User.py\n",
    "Here we get the json files and dump it onto disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests as r\n",
    "import glob\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pprint as pp\n",
    "\n",
    "while True:\n",
    "    response = r.get('http://user-ingredients.herokuapp.com/')\n",
    "    json_data = response.json()\n",
    "    filename = f'user-{datetime.now()}.json'\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(json_data, json_file)\n",
    "    time.sleep(2)\n",
    "\n",
    "#print(glob.glob('user-*.json'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Spoonacular API\n",
    "\n",
    "We will primarily deal with [searching recipes through Spoonacular](https://spoonacular.com/food-api/docs#Search-Recipes-Complex) and querying based off ingredients or other parameters within the user ingredient submissions. Let's explore making some requests.\n",
    "\n",
    "You can either make direct requests or deal with [the Python SDK](https://spoonacular.com/food-api/sdk). This API also requires [authentication](https://spoonacular.com/food-api/docs#Authentication). This is submitted through the following request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "base = 'https://api.spoonacular.com/recipes/complexSearch'\n",
    "params = {\n",
    "    'apiKey': '6980d1ba951a438982534beaedb41e6f',\n",
    "    'addRecipeInformation': True,\n",
    "    'includeIngredients': ['sugar']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.spoonacular.com/recipes/complexSearch?apiKey=6980d1ba951a438982534beaedb41e6f&addRecipeInformation=True&includeIngredients=sugar'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = r.get(base, params)\n",
    "response.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### get_recipe in simple_dag.py\n",
    "This is the part of the dag that retrieves the recipes from spoonacular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Here in get_recipes we:\n",
    "1. get the list of keys from raw/\n",
    "2. get the search parameters and call the spoonacular API\n",
    "3. get the response from the API and add it to a recipe DataFrame\n",
    "4. add the user info from the key to user Dataframe\n",
    "5. upload the user dataframe and recipe dataframe to storage\n",
    "6. Delete read keys from raw/\n",
    "\"\"\"\n",
    "def get_recipes():\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    base = 'https://api.spoonacular.com/recipes/complexSearch'\n",
    "    recipeparams = {\n",
    "    'apiKey': '6980d1ba951a438982534beaedb41e6f',\n",
    "    'addRecipeInformation': True\n",
    "    }\n",
    "    columnsrecipe = [\"id\",\"title\",\"servings\",\"summary\",\"diets\",\"equipment\", \"ingredients\"]\n",
    "    dfrecipe = pd.DataFrame(columns=columnsrecipe)\n",
    "    columnsuser = [\"name\",\"address\",\"description\",\"recipes\"]\n",
    "    dfusers  = pd.DataFrame(columns=columnsuser)\n",
    "    #get the list of keys from raw/\n",
    "    keylist = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=\"raw/\")\n",
    "    jsons = []\n",
    "    for key in keylist:\n",
    "        object = s3_hook.get_key(bucket_name=BUCKET_NAME, key=key).get()['Body'].read().decode('utf-8')\n",
    "        data = json.loads(object)\n",
    "        #get the search parameters and call the spoonacular API\n",
    "        if \"includeIngredients\" in data[\"pantry\"].keys():\n",
    "            recipeparams[\"includeIngredients\"] = data[\"pantry\"][\"includeIngredients\"]\n",
    "        if \"excludeIngredients\" in data[\"pantry\"].keys():\n",
    "            recipeparams[\"excludeIngredients\"] = data[\"pantry\"][\"excludeIngredients\"]\n",
    "        if \"intolerances\" in data[\"pantry\"].keys():\n",
    "            recipeparams[\"intolerances\"] = data[\"pantry\"][\"intolerances\"]\n",
    "        response = requests.get(base, recipeparams)\n",
    "        #get the response from the API and add it to a recipe DataFrame\n",
    "        #check if recipes are returned\n",
    "        if response.json()['results']:\n",
    "            dfrecipe = pd.concat([dfrecipe, getRecipe(response.json()['results'])], ignore_index=True)\n",
    "            dfrecipe = dfrecipe.drop_duplicates(subset=['id'], keep='last')\n",
    "        #add the user info from the key to user Dataframe\n",
    "        user = data['user']\n",
    "        user['recipes'] = dfrecipe['id'].to_list()\n",
    "        dfusers = pd.concat([dfusers,pd.DataFrame.from_dict(user)])\n",
    "        dfusers = dfusers.drop_duplicates(subset=['name'], keep='last')\n",
    "    #upload the user dataframe and recipe dataframe to stage\n",
    "    userdata = bytes(json.dumps(dfusers.to_dict(orient=\"records\")).encode('UTF-8'))\n",
    "    try:\n",
    "        file = 'user-{}.json'.format(datetime.now())\n",
    "        s3_hook.load_bytes(\n",
    "            bytes_data = userdata,\n",
    "            key=''.join([\"stage/\",file]),\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            replace=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    recipedata = bytes(json.dumps(dfrecipe.to_dict(orient=\"records\")).encode('UTF-8'))\n",
    "    try:\n",
    "        file = 'recipe-{}.json'.format(datetime.now())\n",
    "        s3_hook.load_bytes(\n",
    "            bytes_data = recipedata,\n",
    "            key=''.join([\"stage/\",file]),\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            replace=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #export user info to s3\n",
    "    #Delete read keys from raw/\n",
    "    s3_hook.delete_objects(\n",
    "        bucket = BUCKET_NAME,\n",
    "        keys=keylist\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### S3 Bucket\n",
    "\n",
    "You will use a single S3 bucket to store `scripts/` as well as `raw/` and `stage/` data. The `raw` folder will store each unaltered user submission, while the `stage/` folder will store the results from the API after having the user submission handled and sent to the API. The `scripts/` folder will contain any scripts used during the creation of this pipeline.\n",
    "\n",
    "### Airflow\n",
    "\n",
    "In general, Airflow should accomplish the following:\n",
    "1. Handle user ingredient submissions and submit the unaltered data to the S3 buckets `raw` folder. User ingredient submissions will be repeated and we want to avoid recommitting these to the raw database. Each user submission should be batched together in collections of 20 by Airflow and sent in batch responses to the `raw` folder.\n",
    "2. A sensor should monitor submissions to the `raw` folder and then utilize the Spoonacular API to collect recipes based off their submissions. All this data should be maintained and transferred to the `stage` folder within the S3 bucket. \n",
    "3. Another sensor should monitor submissions to the `stage` folder and then organize this information appropriately into a PostgreSQL relational database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### PostgreSQL Database\n",
    "\n",
    "The database should be organized to separate user information and recipe information. For instance, the user table should contain a collection of recipe ids that are used to link recipes to the users given their ingredient list and preferences. The table associate with recipes should include the following information from the Spoonacular API responses:\n",
    "* `title`\n",
    "* `servings`\n",
    "* `summary`\n",
    "* `diets`\n",
    "* `ingredients`\n",
    "* `equipment`\n",
    "\n",
    "All user information should be included within the database (`name`, `address`, `description`). Also, Consider that some recipes may appear multiple times as a result of overlapping user ingredients and preferences - these repeated recipes should not be added to the database. This is in addition to the repeated json responses coming from the User Ingredient API that will be handled by Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to postgres\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Connection\n",
    "import config\n",
    "def get_connection(path):\n",
    "    \"\"\"Put the connection in cache to reuse if path does not change.\"\"\"\n",
    "  dd  engine = create_engine(path)\n",
    "    return engine.connect()\n",
    "\n",
    "path = \"postgresql://{}:{}@{}:{}/{}\".format(config.username,config.passwd,config.hostname,config.portnum,config.dbname)\n",
    "engine = get_connection(path)\n",
    "query = 'select * from information_schema.tables;'\n",
    "df = pd.read_sql(query,engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple_dag.py\n",
    "Here is the airflow dag that\n",
    "1. Handle user ingredient submissions and submit the unaltered data to the S3 buckets raw folder. User ingredient submissions will be repeated and we want to avoid recommitting these to the raw database. Each user submission should be batched together in collections of 20 by Airflow and sent in batch responses to the raw folder.\n",
    "2. A sensor should monitor submissions to the raw folder and then utilize the Spoonacular API to collect recipes based off their submissions. All this data should be maintained and transferred to the stage folder within the S3 bucket.\n",
    "3. Another sensor should monitor submissions to the stage folder and then organize this information appropriately into a PostgreSQL relational database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     34
    ]
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.amazon.aws.sensors.s3 import S3PrefixSensor\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import Connection\n",
    "import config\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'wait_for_downstream': True,\n",
    "    'max_active_runs': 1,\n",
    "    'depends_on_past': True, # determines execution based off previous DAGRun\n",
    "}\n",
    "BUCKET_NAME = \"airflowrs\"\n",
    "\n",
    "\"\"\"\n",
    "Here in getRecipe we:\n",
    "Parameters = recipe: response data\n",
    "1. make a output data frame with not nested columns\n",
    "2. make an equipment dataframe from the nested column with recipe id as reference\n",
    "3. make an indgredients dataframe from the nested column with recipe id as reference\n",
    "4. join equipment and ingredients dataframe to output dataframe\n",
    "5. return output dataframe\n",
    "\"\"\"\n",
    "def getRecipe(recipe):\n",
    "    if recipe:\n",
    "        recipelist = pd.DataFrame(recipe)\n",
    "        recipeout = recipelist[['id','title','servings','summary','diets']]\n",
    "        recipesteps = pd.DataFrame(recipelist[['id','analyzedInstructions']])\n",
    "\n",
    "        #get equipment values\n",
    "        recipesteps= recipesteps.explode('analyzedInstructions').dropna()\n",
    "        recipesteps['analyzedInstructions'] = recipesteps['analyzedInstructions'].apply(lambda x: x['steps'])\n",
    "        recipesteps= recipesteps.explode('analyzedInstructions').dropna()\n",
    "        recipesteps['equipment']= recipesteps['analyzedInstructions'].apply(lambda x: x['equipment'])\n",
    "        recipeequip = recipesteps[['id','equipment']].explode('equipment').dropna()\n",
    "        recipeequip['equipment'] = recipeequip['equipment'].apply(lambda x: x['name'])\n",
    "        recipeequip = recipeequip.groupby(\"id\").agg(lambda x: set(x)).reset_index()\n",
    "        recipeequip['equipment']= recipeequip['equipment'].apply(lambda x: list(x))\n",
    "\n",
    "        #get ingredient values\n",
    "        recipesteps['ingredients']= recipesteps['analyzedInstructions'].apply(lambda x: x['ingredients'])\n",
    "        recipeingr = recipesteps[['id','ingredients']].explode('ingredients').dropna()\n",
    "        recipeingr['ingredients']= recipeingr['ingredients'].apply(lambda x: x['name'])\n",
    "        recipeingr = recipeingr.groupby(\"id\").agg(lambda x: set(x)).reset_index()\n",
    "        recipeingr['ingredients']= recipeingr['ingredients'].apply(lambda x: list(x))\n",
    "\n",
    "        #create recipe dataframe\n",
    "        recipeout = (recipeout.join(recipeequip.set_index(\"id\"), on=\"id\")\n",
    "                            .join(recipeingr.set_index(\"id\"), on=\"id\"))\n",
    "        return recipeout\n",
    "    else:\n",
    "        return None\n",
    "\"\"\"\n",
    "Here in upload_to_bucket we:\n",
    "1. get the list of jsons from local disc\n",
    "2. add data to stack to and check if duplicate\n",
    "3. check if stack = 20 then we upload to raw/\n",
    "4. restart stack to 0 and redo data add\n",
    "5. delete uploaded raw files\n",
    "\"\"\"\n",
    "def upload_to_bucket():\n",
    "    #get list of user files in directory\n",
    "    filelist = glob.glob('user-*.json')\n",
    "    stack = []\n",
    "    stacklist = []\n",
    "    stacklistadded = []\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    sent = False\n",
    "    #uploads 20 at a time\n",
    "    #checks if 20 are available to be sent\n",
    "    while sent == False:\n",
    "        for file in filelist:\n",
    "            stacklist.append(file)\n",
    "            f = open(file)\n",
    "            data = json.load(f)\n",
    "            #check for blanks\n",
    "            if data not in stack:\n",
    "                stack.append(data)\n",
    "                stacklistadded.append(file)\n",
    "            if len(stack) == 20:\n",
    "                #send to s3 20 at a time\n",
    "                for j in stacklistadded:\n",
    "                    try:\n",
    "                        s3_hook.load_file(\n",
    "                            filename=j,\n",
    "                            key=''.join([\"raw/\",j]),\n",
    "                            bucket_name=BUCKET_NAME,\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                #delete files\n",
    "                for j in stacklist:\n",
    "                    os.remove(j)\n",
    "                #reset stacks\n",
    "                sent=True\n",
    "                stack = []\n",
    "                stacklist = []\n",
    "                stacklistadded = []\n",
    "\n",
    "\"\"\"\n",
    "Here in get_recipes we:\n",
    "1. get the list of keys from raw/\n",
    "2. get the search parameters and call the spoonacular API\n",
    "3. get the response from the API and add it to a recipe DataFrame\n",
    "4. add the user info from the key to user Dataframe\n",
    "5. upload the user dataframe and recipe dataframe to storage\n",
    "6. Delete read keys from raw/\n",
    "\"\"\"\n",
    "def get_recipes():\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    base = 'https://api.spoonacular.com/recipes/complexSearch'\n",
    "    recipeparams = {\n",
    "    'apiKey': '6980d1ba951a438982534beaedb41e6f',\n",
    "    'addRecipeInformation': True\n",
    "    }\n",
    "    columnsrecipe = [\"id\",\"title\",\"servings\",\"summary\",\"diets\",\"equipment\", \"ingredients\"]\n",
    "    dfrecipe = pd.DataFrame(columns=columnsrecipe)\n",
    "    columnsuser = [\"name\",\"address\",\"description\",\"recipes\"]\n",
    "    dfusers  = pd.DataFrame(columns=columnsuser)\n",
    "    #get the list of keys from raw/\n",
    "    keylist = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=\"raw/\")\n",
    "    jsons = []\n",
    "    for key in keylist:\n",
    "        object = s3_hook.get_key(bucket_name=BUCKET_NAME, key=key).get()['Body'].read().decode('utf-8')\n",
    "        data = json.loads(object)\n",
    "        #get the search parameters and call the spoonacular API\n",
    "        if \"includeIngredients\" in data[\"pantry\"].keys():\n",
    "            recipeparams[\"includeIngredients\"] = data[\"pantry\"][\"includeIngredients\"]\n",
    "        if \"excludeIngredients\" in data[\"pantry\"].keys():\n",
    "            recipeparams[\"excludeIngredients\"] = data[\"pantry\"][\"excludeIngredients\"]\n",
    "        if \"intolerances\" in data[\"pantry\"].keys():\n",
    "            recipeparams[\"intolerances\"] = data[\"pantry\"][\"intolerances\"]\n",
    "        response = requests.get(base, recipeparams)\n",
    "        #get the response from the API and add it to a recipe DataFrame\n",
    "        #check if recipes are returned\n",
    "        if response.json()['results']:\n",
    "            dfrecipe = pd.concat([dfrecipe, getRecipe(response.json()['results'])], ignore_index=True)\n",
    "            dfrecipe = dfrecipe.drop_duplicates(subset=['id'], keep='last')\n",
    "        #add the user info from the key to user Dataframe\n",
    "        user = data['user']\n",
    "        user['recipes'] = dfrecipe['id'].to_list()\n",
    "        dfusers = pd.concat([dfusers,pd.DataFrame.from_dict(user)])\n",
    "        dfusers = dfusers.drop_duplicates(subset=['name'], keep='last')\n",
    "    #upload the user dataframe and recipe dataframe to stage\n",
    "    userdata = bytes(json.dumps(dfusers.to_dict(orient=\"records\")).encode('UTF-8'))\n",
    "    try:\n",
    "        file = 'user-{}.json'.format(datetime.now())\n",
    "        s3_hook.load_bytes(\n",
    "            bytes_data = userdata,\n",
    "            key=''.join([\"stage/\",file]),\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            replace=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    recipedata = bytes(json.dumps(dfrecipe.to_dict(orient=\"records\")).encode('UTF-8'))\n",
    "    try:\n",
    "        file = 'recipe-{}.json'.format(datetime.now())\n",
    "        s3_hook.load_bytes(\n",
    "            bytes_data = recipedata,\n",
    "            key=''.join([\"stage/\",file]),\n",
    "            bucket_name=BUCKET_NAME,\n",
    "            replace=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        #export user info to s3\n",
    "    #Delete read keys from raw/\n",
    "    s3_hook.delete_objects(\n",
    "        bucket = BUCKET_NAME,\n",
    "        keys=keylist\n",
    "    )\n",
    "\"\"\"\n",
    "Here in to_sql we:\n",
    "1. create a connection to the postgres server\n",
    "2. get the recipes from stage/\n",
    "3. get recipe table from postgres and add stage/ recipes then write to postgres\n",
    "2. get the users from stage/\n",
    "3. get users table from postgres and add stage/ users then write to postgres\n",
    "5. close server connections and delete stage/ keys\n",
    "\"\"\"\n",
    "def to_sql():\n",
    "\n",
    "    #create a connection to the postgres server\n",
    "    path = \"postgresql://{}:{}@{}:{}/{}\".format(config.username,config.passwd,config.hostname,config.portnum,config.dbname)\n",
    "    engine = create_engine(path)\n",
    "    conn = engine.connect()\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    #get the recipes from stage/\n",
    "    recipelist = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=\"stage/recipe-\")\n",
    "    jsons = []\n",
    "    for key in recipelist:\n",
    "        object = s3_hook.get_key(bucket_name=BUCKET_NAME, key=key).get()['Body'].read().decode('utf-8')\n",
    "        data = json.loads(object)\n",
    "        jsons.extend(data)\n",
    "    recipes = pd.DataFrame(jsons).drop_duplicates(subset=['id'], keep='last')\n",
    "    print(recipes)\n",
    "    #get recipe table from postgres and add stage/ recipes then write to postgres\n",
    "    check=engine.has_table(\"recipe\")\n",
    "    if not check:\n",
    "        recipes.to_sql('recipe',conn, if_exists='replace')\n",
    "    else:\n",
    "        columnsrecipe = [\"id\",\"title\",\"servings\",\"summary\",\"diets\",\"equipment\", \"ingredients\"]\n",
    "        recipesql = pd.read_sql_table(\"recipe\",conn, columns=columnsrecipe)\n",
    "        print(recipesql)\n",
    "        out = pd.concat([recipesql,recipes]).drop_duplicates(subset=['id'], keep='last')\n",
    "        out.to_sql('recipe',conn, if_exists='replace', index=False)\n",
    "    #get the users from stage/\n",
    "    userlist = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=\"stage/user-\")\n",
    "    jsons = []\n",
    "    for key in userlist:\n",
    "        object = s3_hook.get_key(bucket_name=BUCKET_NAME, key=key).get()['Body'].read().decode('utf-8')\n",
    "        data = json.loads(object)\n",
    "        jsons.extend(data)\n",
    "    users = pd.DataFrame(jsons).drop_duplicates(subset=['name'], keep='last')\n",
    "    print(users)\n",
    "    #get users table from postgres and add stage/ users then write to postgres\n",
    "    check=engine.has_table(\"users\")\n",
    "    if not check:\n",
    "        users.to_sql('users',conn, if_exists='replace')\n",
    "    else:\n",
    "        columnsuser = [\"name\",\"address\",\"description\",\"recipes\"]\n",
    "        usersql = pd.read_sql_table(\"users\",conn,columns=columnsuser)\n",
    "        print(usersql)\n",
    "        out = pd.concat([usersql,users]).drop_duplicates(subset=['name'], keep='last')\n",
    "        print(out)\n",
    "        out.to_sql('users',conn, if_exists='replace', index=False)\n",
    "    #close server connections and delete stage/ keys\n",
    "    conn.close()\n",
    "    engine.dispose()\n",
    "    s3_hook.delete_objects(\n",
    "        bucket = BUCKET_NAME,\n",
    "        keys=userlist\n",
    "    )\n",
    "    s3_hook.delete_objects(\n",
    "        bucket = BUCKET_NAME,\n",
    "        keys=recipelist\n",
    "    )\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='simple_dag',\n",
    "    default_args=default_args,\n",
    "    start_date=datetime.now(),\n",
    "    end_date=datetime.now() + timedelta(minutes=5),\n",
    "    #schedule_interval='@once',\n",
    "    schedule_interval='*/1 * * * *', # every minute\n",
    "    catchup=True # default\n",
    ")\n",
    "\n",
    "with dag:\n",
    "    #upload to s3\n",
    "    s3_raw = PythonOperator(\n",
    "        task_id='upload_to_bucket',\n",
    "        python_callable= upload_to_bucket,\n",
    "    )\n",
    "    #check if raw/ exists\n",
    "    s3_sensor = S3PrefixSensor(\n",
    "        task_id='s3_sensor',\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        prefix =\"raw/\",\n",
    "        aws_conn_id='aws_default',\n",
    "    )\n",
    "    #get recipes from raw/ and put data into stage, delete raw/ files\n",
    "    s3_stage = PythonOperator(\n",
    "        task_id='get_recipes',\n",
    "        python_callable= get_recipes,\n",
    "    )\n",
    "    #check if stage/ exists\n",
    "    s3_sensor2 = S3PrefixSensor(\n",
    "        task_id='s3_sensor2',\n",
    "        bucket_name=BUCKET_NAME,\n",
    "        prefix =\"stage/\",\n",
    "        aws_conn_id='aws_default',\n",
    "    )\n",
    "    #send data from stage/ to Postgresql server, delete stage/ files\n",
    "    s3_sql = PythonOperator(\n",
    "        task_id='to_sql',\n",
    "        python_callable= to_sql,\n",
    "    )\n",
    "\n",
    "s3_raw >> s3_sensor  >> s3_stage >> s3_sensor2 >> s3_sql\n",
    "#s3_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2022 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
